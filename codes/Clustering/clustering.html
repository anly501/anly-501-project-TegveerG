<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.1.251">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>clustering</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1.6em;
  vertical-align: middle;
}
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>


<script src="clustering_files/libs/clipboard/clipboard.min.js"></script>
<script src="clustering_files/libs/quarto-html/quarto.js"></script>
<script src="clustering_files/libs/quarto-html/popper.min.js"></script>
<script src="clustering_files/libs/quarto-html/tippy.umd.min.js"></script>
<script src="clustering_files/libs/quarto-html/anchor.min.js"></script>
<link href="clustering_files/libs/quarto-html/tippy.css" rel="stylesheet">
<link href="clustering_files/libs/quarto-html/quarto-syntax-highlighting-dark.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="clustering_files/libs/bootstrap/bootstrap.min.js"></script>
<link href="clustering_files/libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="clustering_files/libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="dark">
<script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" integrity="sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==" crossorigin="anonymous"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js" integrity="sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==" crossorigin="anonymous"></script>
<script type="application/javascript">define('jquery', [],function() {return window.jQuery;})</script>

  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

</head>

<body class="fullcontent">

<div id="quarto-content" class="page-columns page-rows-contents page-layout-article">

<main class="content" id="quarto-document-content">



<section id="k-means-dbscan-and-hierarchical-clustering-in-python-using-record-data-rekt-api" class="level1">
<h1>K-Means, DBSCAN, and Hierarchical Clustering in Python using Record Data (REKT API)</h1>
<section id="introduction" class="level2">
<h2 class="anchored" data-anchor-id="introduction">Introduction</h2>
<p>For clustering, we shall drop the target variable (Y) of scam_type_grouped, which will leave us with our feature data (X) to cluster with. Moreover, we will need to filter out the categorical features, including scam_networks, month_of_attack, and day_of_week_of_attack, from our feature data to accurately perform clustering using only numeric record features. The feature data (X) that we will eventually be left with will contain 3 record features, including two features for log of funds (lost and returned) and one datetime extracted feature of day_of_year_of_attack.</p>
<p>I did debate with myself to include month_of_attack and day_of_week_of_attack variables for clustering, but since we already have day_of_year_of_attack, it can convey the information of both month_of_attack and day_of_week_of_attack. At the end of the analysis, we will use the target variable (Y) of scam_type_grouped to check if the clustering predictions coincided with the existing labels in the data-set, which will be our goal.</p>
</section>
<section id="theory" class="level2">
<h2 class="anchored" data-anchor-id="theory">Theory</h2>
<section id="k-means-clustering" class="level4">
<h4 class="anchored" data-anchor-id="k-means-clustering">K-Means Clustering</h4>
<p>K-Means Clustering is a centroid or distance-based algorithm with each cluster of our data points containing a centroid or cluster center. It is also a non-parametric clustering algorithm, which means that it does <strong>not</strong> make strong assumptions about the form of the mapping function that maps input variables (X) to output variables (Y). The objective of this algorithm is to minimize the sum of squared distances between data points and their respective cluster centroid. To understand better, we can break down the algorithm into 5 steps as follows: 1. Choose “k” clusters 2. Select “k” random points from the feature data (X) as centroids 3. Assign remaining datapoints to the closest cluster centroid 4. Recompute centroids of newly formed clusters 5. Repeat steps 3 and 4</p>
<p>The stopping criteria is when: 1. Centroids of newly formed clusters don’t change (convergence) 2. Data points remain in the same cluster 3. Maximum number of iterations are reached</p>
<p>To measure the “closeness” of data points with their respective cluster, we measure similarity between the data points using Euclidean Distance (Distance = <span class="math inline">\(\sqrt{(x_1 - y_1)^2+(x_2 - y_2)^2+...+(x_n - y_n)^2}\)</span>)</p>
<p>The model selection methods to use will include the elbow method to select the optimal value for “k” number of clusters. This selection technique relies on two metrics, Distortion and Inertia. Distortion is calculated as the average of the squared distances from the cluster centers of the respective clusters, using Euclidean distance. Inertia is the sum of squared distances of samples to their closest cluster center. After visualizing the distortion and inertia values for different values of “k”, we can determine the optimal number of clusters by selecting the value of k at the “elbow” of the curve, which is the point after which the distortion/inertia start decreasing in a linear fashion or when they start decreasing slowly. A good model is one with low inertia AND a low number of clusters (K). However, a tradeoff exists because as K increases, inertia decreases. I shall also employ the Silhouette score on the entire feature data (X), including categorical variables, as well as on only the numerical variables, log_funds_lost and log_funds_returned, to better visualize the clusters.</p>
<p>K-Means clustering does have its limitations, the main drawback that it is sensitive to poor initializiation of clusters, which could lead to the algorithm being stuck in a local optimum space. Moreover, the shape and size of clusters could differ with each other, resulting in larger distances among points. Yet, K-Means is interpretable, efficient, guarantees an approximation, and works well for clusters having similar properties (size, shape, and density).</p>
</section>
<section id="dbscan-density-based-spatial-clustering-of-applications-with-noise" class="level4">
<h4 class="anchored" data-anchor-id="dbscan-density-based-spatial-clustering-of-applications-with-noise">DBSCAN (Density-based Spatial Clustering of Applications with Noise)</h4>
<p>DBSCAN is a density-based clustering method that is useful when the clusters are irregular or non-spherical, for example, or when noise and outliers are present in the feature data (X). Given a set of points in some space, it groups together points that are closely packed together (points with many nearby neighbors). Outliers are marked as points that lie alone in low-density regions, whose nearest neighbors are too far away. To understand better, DBSCAN works in the following 3 steps: 1. The algorithm proceeds by arbitrarily picking up a point in the dataset (until all points have been visited). 2. If there are at least minimum number of points (a threshold) clustered together for a region to be considered dense, within a radius of <span class="math inline">\(\epsilon\)</span> (a distance measure used to locate the points in the neighborhood of any point) to the point, then we consider all these points to be part of the same cluster. These minimum number of points, known as minPts, is one parameter for DBSCAN and <span class="math inline">\(\epsilon\)</span> is another parameter. 3. The clusters are then expanded by recursively repeating the neighborhood calculation for each neighboring point.</p>
<p>If minPts = 1, then every point on its own will already be a cluster, which in infeasible. If minPts <span class="math inline">\(\leq\)</span> 2, the result will be the same as of hierarchical clustering, with the dendrogram cut at height <span class="math inline">\(\epsilon\)</span>. Therefore, minPts must be <span class="math inline">\(\geq\)</span> 3. However, larger values for minPts are usually better for data sets with noise as DBSCAN will then yield more significant clusters.</p>
<p>The model selection method to use will include the Silhouette score to select the optimal DBSCAN hyper-parameters. The Silhouette score takes values between -1 to 1, with -1 being the worst value and 1 denoting that the data point i is very compact within the cluster to which it belongs and far away from the other clusters. Values near 0 denote points are on or very close to the decision boundary between two neighboring clusters, leading to overlapping clusters.</p>
<p>DBSCAN can locate non-linearly separable clusters but K-Means cannot. Moreover, unlike K-Means, there is no need to specify the number of clusters “k” when employing DBSCAN. However, choosing a value for <span class="math inline">\(\epsilon\)</span> is more difficult than choosing a good initial value for “k” for K-Means because <span class="math inline">\(\epsilon\)</span> is less intuitive to reason.</p>
</section>
<section id="hierarchical-agglomerative-vs-divisive-clustering" class="level4">
<h4 class="anchored" data-anchor-id="hierarchical-agglomerative-vs-divisive-clustering">Hierarchical (Agglomerative vs Divisive) Clustering</h4>
<p>Hierarchical clustering technique is different from Partitional clustering, which divides the data into non-overlapping clusters such that each data point belongs to exactly one cluster. Hierarchical clustering can be thought of a set of nested clusters organized as a hierarchical tree, visualized through dendrograms. There exist two main types of hierarchical clustering: 1. Agglomerative clustering (bottom up): Start with the points as individual clusters. At each step, move up the hierarchy by merging the closest pair of clusters until only one cluster is left. 2. Divisive clustering (top down): All observations start in one, all-inclusive cluster. At each step, move down the hierarchy by splitting a cluster recursively until each cluster contains a point. Therefore, this approach is exactly opposite to Agglomerative clustering.</p>
<p>Hierarchical clustering’s process can be visualized with the help of a dendrograma, a type of tree diagram showing hierarchical relationships between different sets of data. The dendrogram can be used to decide when to stop merging the clusters or, in other words, finding the optimal number of clusters. We cut the dendrogram tree with a horizontal line at a height where the line can traverse the maximum distance up and down without intersecting the merging point.</p>
<p>Hierarchical clustering, unlike K-Means, does not require initializing the number of clusters apriori, which is a benefit. Moreover, the dendrograms generated by Hierarchical clustering can be more informative than the spherical clusters returned by K-Means. However, Hierarchical clustering can be sensitive to outliers and computationally inefficient when working with large datasets.</p>
</section>
</section>
<section id="methods" class="level2">
<h2 class="anchored" data-anchor-id="methods">Methods</h2>
<section id="data-selection" class="level3">
<h3 class="anchored" data-anchor-id="data-selection">Data Selection</h3>
<div class="cell" data-execution_count="1">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="co"># import the necessary packages</span></span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> seaborn <span class="im">as</span> sns</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> scipy.cluster.hierarchy <span class="im">as</span> sch</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> sklearn.cluster <span class="im">as</span> cluster</span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a>sns.set_theme(style<span class="op">=</span><span class="st">"whitegrid"</span>, palette<span class="op">=</span><span class="st">'Set2'</span>)</span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> warnings</span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a>warnings.filterwarnings(<span class="st">"ignore"</span>)</span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a><span class="co"># read in cleaned REKT Database and drop labels and other variables, leaving only feature data (X)</span></span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a>df <span class="op">=</span> pd.read_csv(<span class="st">"../../data/Clean Data/REKT_Database_Clean_Classification.csv"</span>, index_col<span class="op">=</span>[<span class="dv">0</span>])</span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a>df_cluster <span class="op">=</span> df.drop([<span class="st">'scam_type_grouped'</span>, <span class="st">'day_of_week_of_attack'</span>, <span class="st">'day_of_year_of_attack'</span>], axis<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a>df_cluster.head() <span class="co"># visualize first 5 rows</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display" data-execution_count="1">

<div>

<table class="dataframe table table-sm table-striped">
  <thead>
    <tr>
      <th></th>
      <th>log_funds_lost</th>
      <th>log_funds_returned</th>
      <th>scamNetworks</th>
      <th>month_of_attack</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>1</th>
      <td>15.319588</td>
      <td>0.0</td>
      <td>Ethereum</td>
      <td>8</td>
    </tr>
    <tr>
      <th>2</th>
      <td>10.603213</td>
      <td>0.0</td>
      <td>Avax</td>
      <td>5</td>
    </tr>
    <tr>
      <th>3</th>
      <td>15.420940</td>
      <td>0.0</td>
      <td>CEX</td>
      <td>4</td>
    </tr>
    <tr>
      <th>4</th>
      <td>13.504419</td>
      <td>0.0</td>
      <td>Binance</td>
      <td>7</td>
    </tr>
    <tr>
      <th>5</th>
      <td>9.798127</td>
      <td>0.0</td>
      <td>Fantom</td>
      <td>5</td>
    </tr>
  </tbody>
</table>
</div>
</div>
</div>
<div class="cell" data-execution_count="2">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a>df_cluster.shape <span class="co"># get the number of rows and columns</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display" data-execution_count="2">
<pre><code>(816, 4)</code></pre>
</div>
</div>
<div class="cell" data-execution_count="3">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(df_cluster.info()) <span class="co"># get column information</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>&lt;class 'pandas.core.frame.DataFrame'&gt;
Int64Index: 816 entries, 1 to 816
Data columns (total 4 columns):
 #   Column              Non-Null Count  Dtype  
---  ------              --------------  -----  
 0   log_funds_lost      816 non-null    float64
 1   log_funds_returned  816 non-null    float64
 2   scamNetworks        816 non-null    object 
 3   month_of_attack     816 non-null    int64  
dtypes: float64(2), int64(1), object(1)
memory usage: 31.9+ KB
None</code></pre>
</div>
</div>
<div class="cell" data-execution_count="4">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a>df_cluster.isnull().<span class="bu">sum</span>() <span class="co"># check for missing values</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display" data-execution_count="4">
<pre><code>log_funds_lost        0
log_funds_returned    0
scamNetworks          0
month_of_attack       0
dtype: int64</code></pre>
</div>
</div>
</section>
<section id="feature-selection-and-pre-processing" class="level3">
<h3 class="anchored" data-anchor-id="feature-selection-and-pre-processing">Feature selection and Pre-processing</h3>
<p>As shown in the code above, we not only dropped the target variable, scam_type_grouped, but also two datetime extracted features, day_of_week_of_attack and day_of_year_of_attack. Dr.&nbsp;Nakul’s inputs and my EDA helped me choose between the datetime variables, which led me to only choose month_of_attack as a categorical variable. Recalling from the EDA section, the month of attack conveyed that most attacks took place between August and September, with other attacks taking place roughly at the same frequency in all other months. Therefore, we can expect clusters around certain months.</p>
<p>In terms of pre-processing our data, our funds variables are already log-transformed and, if we recall from the EDA section, we obtained a fairly normal distribution for both log funds variables. The only pre-processing left to be done is on the categorical variable, scamNetworks, which represents the unique cryptocurrency token associated with the attack. We will use <code>cat.codes</code> to encode the categorical variable. Therefore, we do not need to run any scaling methods, such as sklearn’s <code>StandardScaler</code>, on our feature data.</p>
<div class="cell" data-execution_count="5">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb8"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Replace categorical values with category codes by using the cat.codes function. you can either replace them in place or create a new column. show the altered dataframe again by using head() </span></span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a>df_cluster[<span class="st">'scamNetworks'</span>].value_counts()</span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a>df_cluster[<span class="st">'scamNetworks'</span>] <span class="op">=</span> df[<span class="st">'scamNetworks'</span>].astype(<span class="st">'category'</span>).cat.codes </span>
<span id="cb8-4"><a href="#cb8-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-5"><a href="#cb8-5" aria-hidden="true" tabindex="-1"></a>df_cluster.head()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display" data-execution_count="5">

<div>

<table class="dataframe table table-sm table-striped">
  <thead>
    <tr>
      <th></th>
      <th>log_funds_lost</th>
      <th>log_funds_returned</th>
      <th>scamNetworks</th>
      <th>month_of_attack</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>1</th>
      <td>15.319588</td>
      <td>0.0</td>
      <td>8</td>
      <td>8</td>
    </tr>
    <tr>
      <th>2</th>
      <td>10.603213</td>
      <td>0.0</td>
      <td>2</td>
      <td>5</td>
    </tr>
    <tr>
      <th>3</th>
      <td>15.420940</td>
      <td>0.0</td>
      <td>4</td>
      <td>4</td>
    </tr>
    <tr>
      <th>4</th>
      <td>13.504419</td>
      <td>0.0</td>
      <td>3</td>
      <td>7</td>
    </tr>
    <tr>
      <th>5</th>
      <td>9.798127</td>
      <td>0.0</td>
      <td>9</td>
      <td>5</td>
    </tr>
  </tbody>
</table>
</div>
</div>
</div>
</section>
<section id="seperate-the-dataset-into-features-and-labels" class="level3">
<h3 class="anchored" data-anchor-id="seperate-the-dataset-into-features-and-labels">Seperate the dataset into features and labels</h3>
<div class="cell" data-execution_count="6">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb9"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Split the dataset in X and y. since this is unsupervised learning, we will not use the y labels.</span></span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.model_selection <span class="im">import</span> train_test_split</span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.preprocessing <span class="im">import</span> StandardScaler </span>
<span id="cb9-4"><a href="#cb9-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-5"><a href="#cb9-5" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> df_cluster</span>
<span id="cb9-6"><a href="#cb9-6" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> df[<span class="st">'scam_type_grouped'</span>]</span>
<span id="cb9-7"><a href="#cb9-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-8"><a href="#cb9-8" aria-hidden="true" tabindex="-1"></a><span class="co"># X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
</section>
</section>
</section>
<section id="perform-k-means-clustering" class="level1">
<h1>Perform K-means Clustering</h1>
<div class="cell" data-execution_count="7">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb10"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a><span class="co"># import relevent libraries for clustering</span></span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> statistics <span class="im">import</span> mode</span>
<span id="cb10-3"><a href="#cb10-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.cluster <span class="im">import</span> KMeans</span>
<span id="cb10-4"><a href="#cb10-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> scipy.spatial.distance <span class="im">import</span> cdist</span>
<span id="cb10-5"><a href="#cb10-5" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> accuracy_score</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<div class="cell" data-execution_count="8">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb11"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a><span class="co"># for k means clustering we will use the elbow method to find the optimal number of clusters. </span></span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a><span class="co"># we will use the inertia_ attribute to find the sum of squared distances of samples to their closest cluster center. </span></span>
<span id="cb11-3"><a href="#cb11-3" aria-hidden="true" tabindex="-1"></a><span class="co"># we will use the range of 1 to 20 clusters and plot the inertia_ values for each cluster. </span></span>
<span id="cb11-4"><a href="#cb11-4" aria-hidden="true" tabindex="-1"></a>distortions <span class="op">=</span> []</span>
<span id="cb11-5"><a href="#cb11-5" aria-hidden="true" tabindex="-1"></a>inertias <span class="op">=</span> []</span>
<span id="cb11-6"><a href="#cb11-6" aria-hidden="true" tabindex="-1"></a>k <span class="op">=</span> <span class="dv">16</span></span>
<span id="cb11-7"><a href="#cb11-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-8"><a href="#cb11-8" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> k <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">1</span>,k):</span>
<span id="cb11-9"><a href="#cb11-9" aria-hidden="true" tabindex="-1"></a>    kmeansmodel <span class="op">=</span> KMeans(n_clusters<span class="op">=</span>k, init<span class="op">=</span><span class="st">'k-means++'</span>, random_state<span class="op">=</span><span class="dv">42</span>)</span>
<span id="cb11-10"><a href="#cb11-10" aria-hidden="true" tabindex="-1"></a>    kmeansmodel.fit(X)</span>
<span id="cb11-11"><a href="#cb11-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-12"><a href="#cb11-12" aria-hidden="true" tabindex="-1"></a>    distortions.append(<span class="bu">sum</span>(np.<span class="bu">min</span>(cdist(X, kmeansmodel.cluster_centers_, <span class="st">'euclidean'</span>), axis<span class="op">=</span><span class="dv">1</span>))<span class="op">/</span> X.shape[<span class="dv">0</span>])</span>
<span id="cb11-13"><a href="#cb11-13" aria-hidden="true" tabindex="-1"></a>    inertias.append(kmeansmodel.inertia_)</span>
<span id="cb11-14"><a href="#cb11-14" aria-hidden="true" tabindex="-1"></a>    evaluation<span class="op">=</span>pd.DataFrame.from_records({<span class="st">"Cluster"</span>:np.arange(<span class="dv">1</span>,k<span class="op">+</span><span class="dv">1</span>), <span class="st">"Distortion"</span>:distortions, <span class="st">"Inertia"</span>:inertias})</span>
<span id="cb11-15"><a href="#cb11-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-16"><a href="#cb11-16" aria-hidden="true" tabindex="-1"></a>evaluation</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display" data-execution_count="8">

<div>

<table class="dataframe table table-sm table-striped">
  <thead>
    <tr>
      <th></th>
      <th>Cluster</th>
      <th>Distortion</th>
      <th>Inertia</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>1</td>
      <td>6.879449</td>
      <td>50397.468473</td>
    </tr>
    <tr>
      <th>1</th>
      <td>2</td>
      <td>5.942030</td>
      <td>35167.440519</td>
    </tr>
    <tr>
      <th>2</th>
      <td>3</td>
      <td>5.040738</td>
      <td>23742.921023</td>
    </tr>
    <tr>
      <th>3</th>
      <td>4</td>
      <td>4.213495</td>
      <td>16987.160940</td>
    </tr>
    <tr>
      <th>4</th>
      <td>5</td>
      <td>3.710141</td>
      <td>14141.979361</td>
    </tr>
    <tr>
      <th>5</th>
      <td>6</td>
      <td>3.385814</td>
      <td>12258.155012</td>
    </tr>
    <tr>
      <th>6</th>
      <td>7</td>
      <td>3.253913</td>
      <td>10613.113835</td>
    </tr>
    <tr>
      <th>7</th>
      <td>8</td>
      <td>3.001997</td>
      <td>9319.440581</td>
    </tr>
    <tr>
      <th>8</th>
      <td>9</td>
      <td>2.795708</td>
      <td>8336.158029</td>
    </tr>
    <tr>
      <th>9</th>
      <td>10</td>
      <td>2.704057</td>
      <td>7768.448107</td>
    </tr>
    <tr>
      <th>10</th>
      <td>11</td>
      <td>2.558399</td>
      <td>7094.034383</td>
    </tr>
    <tr>
      <th>11</th>
      <td>12</td>
      <td>2.446346</td>
      <td>6622.738186</td>
    </tr>
    <tr>
      <th>12</th>
      <td>13</td>
      <td>2.390318</td>
      <td>6242.239058</td>
    </tr>
    <tr>
      <th>13</th>
      <td>14</td>
      <td>2.331093</td>
      <td>5920.840441</td>
    </tr>
    <tr>
      <th>14</th>
      <td>15</td>
      <td>2.221247</td>
      <td>5528.544956</td>
    </tr>
  </tbody>
</table>
</div>
</div>
</div>
<div class="cell" data-execution_count="9">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb12"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a><span class="co"># plot distortion and inertia for kmeans, you can either plot them seperately or use fig, ax = plt.subplots(1, 2) to plot them in the same figure. Suggest the optimal number of clusters based on the plot.</span></span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true" tabindex="-1"></a>evaluation.plot.line(x<span class="op">=</span><span class="st">"Cluster"</span>, subplots<span class="op">=</span><span class="va">True</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display" data-execution_count="9">
<pre><code>array([&lt;AxesSubplot:xlabel='Cluster'&gt;, &lt;AxesSubplot:xlabel='Cluster'&gt;],
      dtype=object)</code></pre>
</div>
<div class="cell-output cell-output-display">
<p><img src="clustering_files/figure-html/cell-10-output-2.png" width="594" height="435"></p>
</div>
</div>
<div class="cell" data-execution_count="10">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb14"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Plotting clusters for best k = 4 (as per elbow method above)</span></span>
<span id="cb14-2"><a href="#cb14-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-3"><a href="#cb14-3" aria-hidden="true" tabindex="-1"></a>bestK <span class="op">=</span> KMeans(n_clusters<span class="op">=</span><span class="dv">4</span>, init<span class="op">=</span><span class="st">'k-means++'</span>, random_state<span class="op">=</span><span class="dv">42</span>)</span>
<span id="cb14-4"><a href="#cb14-4" aria-hidden="true" tabindex="-1"></a>labels4 <span class="op">=</span> bestK.fit_predict(X)</span>
<span id="cb14-5"><a href="#cb14-5" aria-hidden="true" tabindex="-1"></a>df[<span class="st">'kmeans_labels'</span>] <span class="op">=</span> labels4</span>
<span id="cb14-6"><a href="#cb14-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-7"><a href="#cb14-7" aria-hidden="true" tabindex="-1"></a>fig, ax <span class="op">=</span> plt.subplots(<span class="dv">1</span>,<span class="dv">2</span>, figsize<span class="op">=</span>(<span class="dv">10</span>,<span class="dv">5</span>))</span>
<span id="cb14-8"><a href="#cb14-8" aria-hidden="true" tabindex="-1"></a>sns.scatterplot(x<span class="op">=</span><span class="st">"log_funds_returned"</span>, y<span class="op">=</span><span class="st">"log_funds_lost"</span>, hue<span class="op">=</span><span class="st">"scam_type_grouped"</span>, data<span class="op">=</span>df, ax<span class="op">=</span>ax[<span class="dv">0</span>]).<span class="bu">set</span>(title<span class="op">=</span><span class="st">'Log Funds Clusters By Scam Type (Y)'</span>)</span>
<span id="cb14-9"><a href="#cb14-9" aria-hidden="true" tabindex="-1"></a>sns.scatterplot(x<span class="op">=</span><span class="st">"log_funds_returned"</span>, y<span class="op">=</span><span class="st">"log_funds_lost"</span>, hue<span class="op">=</span><span class="st">"kmeans_labels"</span>, data<span class="op">=</span>df, ax<span class="op">=</span>ax[<span class="dv">1</span>]).<span class="bu">set</span>(title<span class="op">=</span><span class="st">'K-Means Clustering Plot'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display" data-execution_count="10">
<pre><code>[Text(0.5, 1.0, 'K-Means Clustering Plot')]</code></pre>
</div>
<div class="cell-output cell-output-display">
<p><img src="clustering_files/figure-html/cell-11-output-2.png" width="812" height="455"></p>
</div>
</div>
<p>According to the distortion and intertia values across the 15 clusters, the initial K-Means model conveys that k=4 clusters is the optimal number of clusters to use. The scatterplots above provide a binary representation of the two continuous features, log_funds_lost and log_funds_returned, of our feature data (X) to visualize the labels generated by the K-Means model with n_clusters set to 4. Next, we shall explore how the silhouette scores of the K-Means clusters affects our conclusions about this model.</p>
<section id="hyper-parameter-tuning" class="level3">
<h3 class="anchored" data-anchor-id="hyper-parameter-tuning">Hyper-parameter tuning</h3>
<div class="cell" data-execution_count="11">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb16"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true" tabindex="-1"></a><span class="co"># THIS WILL ITERATE OVER ONE HYPER-PARAMETER (GRID SEARCH) </span></span>
<span id="cb16-2"><a href="#cb16-2" aria-hidden="true" tabindex="-1"></a><span class="co"># AND RETURN THE CLUSTER RESULT THAT OPTIMIZES THE SILHOUETTE SCORE</span></span>
<span id="cb16-3"><a href="#cb16-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> sklearn.cluster</span>
<span id="cb16-4"><a href="#cb16-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-5"><a href="#cb16-5" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> maximize_silhouette(X,algo<span class="op">=</span><span class="st">"birch"</span>,nmax<span class="op">=</span><span class="dv">20</span>,i_plot<span class="op">=</span><span class="va">False</span>):</span>
<span id="cb16-6"><a href="#cb16-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-7"><a href="#cb16-7" aria-hidden="true" tabindex="-1"></a>    <span class="co"># PARAM</span></span>
<span id="cb16-8"><a href="#cb16-8" aria-hidden="true" tabindex="-1"></a>    i_print<span class="op">=</span><span class="va">False</span></span>
<span id="cb16-9"><a href="#cb16-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-10"><a href="#cb16-10" aria-hidden="true" tabindex="-1"></a>    <span class="co">#FORCE CONTIGUOUS</span></span>
<span id="cb16-11"><a href="#cb16-11" aria-hidden="true" tabindex="-1"></a>    X<span class="op">=</span>np.ascontiguousarray(X) </span>
<span id="cb16-12"><a href="#cb16-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-13"><a href="#cb16-13" aria-hidden="true" tabindex="-1"></a>    <span class="co"># LOOP OVER HYPER-PARAM</span></span>
<span id="cb16-14"><a href="#cb16-14" aria-hidden="true" tabindex="-1"></a>    params<span class="op">=</span>[]<span class="op">;</span> sil_scores<span class="op">=</span>[]</span>
<span id="cb16-15"><a href="#cb16-15" aria-hidden="true" tabindex="-1"></a>    sil_max<span class="op">=-</span><span class="dv">10</span></span>
<span id="cb16-16"><a href="#cb16-16" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> param <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">2</span>,nmax<span class="op">+</span><span class="dv">1</span>):</span>
<span id="cb16-17"><a href="#cb16-17" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span>(algo<span class="op">==</span><span class="st">"kmeans"</span>):</span>
<span id="cb16-18"><a href="#cb16-18" aria-hidden="true" tabindex="-1"></a>            model <span class="op">=</span> sklearn.cluster.KMeans(n_clusters<span class="op">=</span>param).fit(X)</span>
<span id="cb16-19"><a href="#cb16-19" aria-hidden="true" tabindex="-1"></a>            labels<span class="op">=</span>model.predict(X)</span>
<span id="cb16-20"><a href="#cb16-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-21"><a href="#cb16-21" aria-hidden="true" tabindex="-1"></a>        <span class="cf">try</span>:</span>
<span id="cb16-22"><a href="#cb16-22" aria-hidden="true" tabindex="-1"></a>            sil_scores.append(sklearn.metrics.silhouette_score(X,labels))</span>
<span id="cb16-23"><a href="#cb16-23" aria-hidden="true" tabindex="-1"></a>            params.append(param)</span>
<span id="cb16-24"><a href="#cb16-24" aria-hidden="true" tabindex="-1"></a>        <span class="cf">except</span>:</span>
<span id="cb16-25"><a href="#cb16-25" aria-hidden="true" tabindex="-1"></a>            <span class="cf">continue</span> </span>
<span id="cb16-26"><a href="#cb16-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-27"><a href="#cb16-27" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span>(i_print): <span class="bu">print</span>(param,sil_scores[<span class="op">-</span><span class="dv">1</span>])</span>
<span id="cb16-28"><a href="#cb16-28" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb16-29"><a href="#cb16-29" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span>(sil_scores[<span class="op">-</span><span class="dv">1</span>]<span class="op">&gt;</span>sil_max):</span>
<span id="cb16-30"><a href="#cb16-30" aria-hidden="true" tabindex="-1"></a>             opt_param<span class="op">=</span>param</span>
<span id="cb16-31"><a href="#cb16-31" aria-hidden="true" tabindex="-1"></a>             sil_max<span class="op">=</span>sil_scores[<span class="op">-</span><span class="dv">1</span>]</span>
<span id="cb16-32"><a href="#cb16-32" aria-hidden="true" tabindex="-1"></a>             opt_labels<span class="op">=</span>labels</span>
<span id="cb16-33"><a href="#cb16-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-34"><a href="#cb16-34" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"OPTIMAL PARAMETER ="</span>,opt_param)</span>
<span id="cb16-35"><a href="#cb16-35" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-36"><a href="#cb16-36" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span>(i_plot):</span>
<span id="cb16-37"><a href="#cb16-37" aria-hidden="true" tabindex="-1"></a>        fig, ax <span class="op">=</span> plt.subplots()</span>
<span id="cb16-38"><a href="#cb16-38" aria-hidden="true" tabindex="-1"></a>        ax.plot(params, sil_scores, <span class="st">"-o"</span>)  </span>
<span id="cb16-39"><a href="#cb16-39" aria-hidden="true" tabindex="-1"></a>        ax.<span class="bu">set</span>(xlabel<span class="op">=</span><span class="st">'Hyper-parameter'</span>, ylabel<span class="op">=</span><span class="st">'Silhouette Score'</span>)</span>
<span id="cb16-40"><a href="#cb16-40" aria-hidden="true" tabindex="-1"></a>        plt.show()</span>
<span id="cb16-41"><a href="#cb16-41" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-42"><a href="#cb16-42" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> opt_labels</span>
<span id="cb16-43"><a href="#cb16-43" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-44"><a href="#cb16-44" aria-hidden="true" tabindex="-1"></a>k_means_opt_labels<span class="op">=</span>maximize_silhouette(X,algo<span class="op">=</span><span class="st">"kmeans"</span>,nmax<span class="op">=</span><span class="dv">15</span>, i_plot<span class="op">=</span><span class="va">True</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>OPTIMAL PARAMETER = 3</code></pre>
</div>
<div class="cell-output cell-output-display">
<p><img src="clustering_files/figure-html/cell-12-output-2.png" width="609" height="435"></p>
</div>
</div>
<p>When n_clusters=3, the silhouette score is maximized on our entire feature data (X), including both continuous features of log of funds and the two categorical features of scamNetworks and month of attack. This result differs from that of the elbow method, which conveyed that n_clusters=4 clusters was the optimal solution. The elbow method is used to find the “elbow” point, where adding additional data samples does not change cluster membership much. The Silhouette score determines whether there are large gaps between each sample and all other samples within the same cluster or across different clusters. The major difference between elbow and silhouette scores is that elbow only calculates the euclidean distance whereas silhouette takes into account variables such as variance, skewness, high-low differences, etc. Therefore, both hyper-parameter tuning methods convey different use cases and probably silhouette score can be given more weightage when considering the optimal number of clusters for K-Means.</p>
<p>Next, we will subset only the continuous features, log_funds_returned and log_funds_lost, and run K-Means on only these two features to easily visualize the silhouette score hyper-parameter tuning:</p>
<div class="cell" data-execution_count="12">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb18"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb18-1"><a href="#cb18-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> silhouette_samples, silhouette_score</span>
<span id="cb18-2"><a href="#cb18-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-3"><a href="#cb18-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb18-4"><a href="#cb18-4" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.cm <span class="im">as</span> cm</span>
<span id="cb18-5"><a href="#cb18-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-6"><a href="#cb18-6" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> X[[<span class="st">"log_funds_returned"</span>, <span class="st">"log_funds_lost"</span>]] <span class="co"># subset only the continuous features for visualization</span></span>
<span id="cb18-7"><a href="#cb18-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-8"><a href="#cb18-8" aria-hidden="true" tabindex="-1"></a>range_n_clusters <span class="op">=</span> [<span class="dv">2</span>, <span class="dv">3</span>, <span class="dv">4</span>, <span class="dv">5</span>]</span>
<span id="cb18-9"><a href="#cb18-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-10"><a href="#cb18-10" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> n_clusters <span class="kw">in</span> range_n_clusters:</span>
<span id="cb18-11"><a href="#cb18-11" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Create a subplot with 1 row and 2 columns</span></span>
<span id="cb18-12"><a href="#cb18-12" aria-hidden="true" tabindex="-1"></a>    fig, (ax1, ax2) <span class="op">=</span> plt.subplots(<span class="dv">1</span>, <span class="dv">2</span>)</span>
<span id="cb18-13"><a href="#cb18-13" aria-hidden="true" tabindex="-1"></a>    fig.set_size_inches(<span class="dv">18</span>, <span class="dv">7</span>)</span>
<span id="cb18-14"><a href="#cb18-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-15"><a href="#cb18-15" aria-hidden="true" tabindex="-1"></a>    <span class="co"># The 1st subplot is the silhouette plot</span></span>
<span id="cb18-16"><a href="#cb18-16" aria-hidden="true" tabindex="-1"></a>    <span class="co"># The silhouette coefficient can range from -1, 1 but in this example all</span></span>
<span id="cb18-17"><a href="#cb18-17" aria-hidden="true" tabindex="-1"></a>    <span class="co"># lie within [-0.1, 1]</span></span>
<span id="cb18-18"><a href="#cb18-18" aria-hidden="true" tabindex="-1"></a>    ax1.set_xlim([<span class="op">-</span><span class="fl">0.1</span>, <span class="dv">1</span>])</span>
<span id="cb18-19"><a href="#cb18-19" aria-hidden="true" tabindex="-1"></a>    <span class="co"># The (n_clusters+1)*10 is for inserting blank space between silhouette</span></span>
<span id="cb18-20"><a href="#cb18-20" aria-hidden="true" tabindex="-1"></a>    <span class="co"># plots of individual clusters, to demarcate them clearly.</span></span>
<span id="cb18-21"><a href="#cb18-21" aria-hidden="true" tabindex="-1"></a>    ax1.set_ylim([<span class="dv">0</span>, <span class="bu">len</span>(X) <span class="op">+</span> (n_clusters <span class="op">+</span> <span class="dv">1</span>) <span class="op">*</span> <span class="dv">10</span>])</span>
<span id="cb18-22"><a href="#cb18-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-23"><a href="#cb18-23" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Initialize the clusterer with n_clusters value and a random generator</span></span>
<span id="cb18-24"><a href="#cb18-24" aria-hidden="true" tabindex="-1"></a>    <span class="co"># seed of 10 for reproducibility.</span></span>
<span id="cb18-25"><a href="#cb18-25" aria-hidden="true" tabindex="-1"></a>    clusterer <span class="op">=</span> KMeans(n_clusters<span class="op">=</span>n_clusters, random_state<span class="op">=</span><span class="dv">10</span>)</span>
<span id="cb18-26"><a href="#cb18-26" aria-hidden="true" tabindex="-1"></a>    cluster_labels <span class="op">=</span> clusterer.fit_predict(X)</span>
<span id="cb18-27"><a href="#cb18-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-28"><a href="#cb18-28" aria-hidden="true" tabindex="-1"></a>    <span class="co"># The silhouette_score gives the average value for all the samples.</span></span>
<span id="cb18-29"><a href="#cb18-29" aria-hidden="true" tabindex="-1"></a>    <span class="co"># This gives a perspective into the density and separation of the formed</span></span>
<span id="cb18-30"><a href="#cb18-30" aria-hidden="true" tabindex="-1"></a>    <span class="co"># clusters</span></span>
<span id="cb18-31"><a href="#cb18-31" aria-hidden="true" tabindex="-1"></a>    silhouette_avg <span class="op">=</span> silhouette_score(X, cluster_labels)</span>
<span id="cb18-32"><a href="#cb18-32" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(</span>
<span id="cb18-33"><a href="#cb18-33" aria-hidden="true" tabindex="-1"></a>        <span class="st">"For n_clusters ="</span>,</span>
<span id="cb18-34"><a href="#cb18-34" aria-hidden="true" tabindex="-1"></a>        n_clusters,</span>
<span id="cb18-35"><a href="#cb18-35" aria-hidden="true" tabindex="-1"></a>        <span class="st">"The average silhouette_score is :"</span>,</span>
<span id="cb18-36"><a href="#cb18-36" aria-hidden="true" tabindex="-1"></a>        silhouette_avg,</span>
<span id="cb18-37"><a href="#cb18-37" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb18-38"><a href="#cb18-38" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-39"><a href="#cb18-39" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Compute the silhouette scores for each sample</span></span>
<span id="cb18-40"><a href="#cb18-40" aria-hidden="true" tabindex="-1"></a>    sample_silhouette_values <span class="op">=</span> silhouette_samples(X, cluster_labels)</span>
<span id="cb18-41"><a href="#cb18-41" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-42"><a href="#cb18-42" aria-hidden="true" tabindex="-1"></a>    y_lower <span class="op">=</span> <span class="dv">10</span></span>
<span id="cb18-43"><a href="#cb18-43" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(n_clusters):</span>
<span id="cb18-44"><a href="#cb18-44" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Aggregate the silhouette scores for samples belonging to</span></span>
<span id="cb18-45"><a href="#cb18-45" aria-hidden="true" tabindex="-1"></a>        <span class="co"># cluster i, and sort them</span></span>
<span id="cb18-46"><a href="#cb18-46" aria-hidden="true" tabindex="-1"></a>        ith_cluster_silhouette_values <span class="op">=</span> sample_silhouette_values[cluster_labels <span class="op">==</span> i]</span>
<span id="cb18-47"><a href="#cb18-47" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-48"><a href="#cb18-48" aria-hidden="true" tabindex="-1"></a>        ith_cluster_silhouette_values.sort()</span>
<span id="cb18-49"><a href="#cb18-49" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-50"><a href="#cb18-50" aria-hidden="true" tabindex="-1"></a>        size_cluster_i <span class="op">=</span> ith_cluster_silhouette_values.shape[<span class="dv">0</span>]</span>
<span id="cb18-51"><a href="#cb18-51" aria-hidden="true" tabindex="-1"></a>        y_upper <span class="op">=</span> y_lower <span class="op">+</span> size_cluster_i</span>
<span id="cb18-52"><a href="#cb18-52" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-53"><a href="#cb18-53" aria-hidden="true" tabindex="-1"></a>        color <span class="op">=</span> cm.nipy_spectral(<span class="bu">float</span>(i) <span class="op">/</span> n_clusters)</span>
<span id="cb18-54"><a href="#cb18-54" aria-hidden="true" tabindex="-1"></a>        ax1.fill_betweenx(</span>
<span id="cb18-55"><a href="#cb18-55" aria-hidden="true" tabindex="-1"></a>            np.arange(y_lower, y_upper),</span>
<span id="cb18-56"><a href="#cb18-56" aria-hidden="true" tabindex="-1"></a>            <span class="dv">0</span>,</span>
<span id="cb18-57"><a href="#cb18-57" aria-hidden="true" tabindex="-1"></a>            ith_cluster_silhouette_values,</span>
<span id="cb18-58"><a href="#cb18-58" aria-hidden="true" tabindex="-1"></a>            facecolor<span class="op">=</span>color,</span>
<span id="cb18-59"><a href="#cb18-59" aria-hidden="true" tabindex="-1"></a>            edgecolor<span class="op">=</span>color,</span>
<span id="cb18-60"><a href="#cb18-60" aria-hidden="true" tabindex="-1"></a>            alpha<span class="op">=</span><span class="fl">0.7</span>,</span>
<span id="cb18-61"><a href="#cb18-61" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb18-62"><a href="#cb18-62" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-63"><a href="#cb18-63" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Label the silhouette plots with their cluster numbers at the middle</span></span>
<span id="cb18-64"><a href="#cb18-64" aria-hidden="true" tabindex="-1"></a>        ax1.text(<span class="op">-</span><span class="fl">0.05</span>, y_lower <span class="op">+</span> <span class="fl">0.5</span> <span class="op">*</span> size_cluster_i, <span class="bu">str</span>(i))</span>
<span id="cb18-65"><a href="#cb18-65" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-66"><a href="#cb18-66" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Compute the new y_lower for next plot</span></span>
<span id="cb18-67"><a href="#cb18-67" aria-hidden="true" tabindex="-1"></a>        y_lower <span class="op">=</span> y_upper <span class="op">+</span> <span class="dv">10</span>  <span class="co"># 10 for the 0 samples</span></span>
<span id="cb18-68"><a href="#cb18-68" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-69"><a href="#cb18-69" aria-hidden="true" tabindex="-1"></a>    ax1.set_title(<span class="st">"The silhouette plot for the various clusters."</span>)</span>
<span id="cb18-70"><a href="#cb18-70" aria-hidden="true" tabindex="-1"></a>    ax1.set_xlabel(<span class="st">"The silhouette coefficient values"</span>)</span>
<span id="cb18-71"><a href="#cb18-71" aria-hidden="true" tabindex="-1"></a>    ax1.set_ylabel(<span class="st">"Cluster label"</span>)</span>
<span id="cb18-72"><a href="#cb18-72" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-73"><a href="#cb18-73" aria-hidden="true" tabindex="-1"></a>    <span class="co"># The vertical line for average silhouette score of all the values</span></span>
<span id="cb18-74"><a href="#cb18-74" aria-hidden="true" tabindex="-1"></a>    ax1.axvline(x<span class="op">=</span>silhouette_avg, color<span class="op">=</span><span class="st">"red"</span>, linestyle<span class="op">=</span><span class="st">"--"</span>)</span>
<span id="cb18-75"><a href="#cb18-75" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-76"><a href="#cb18-76" aria-hidden="true" tabindex="-1"></a>    ax1.set_yticks([])  <span class="co"># Clear the yaxis labels / ticks</span></span>
<span id="cb18-77"><a href="#cb18-77" aria-hidden="true" tabindex="-1"></a>    ax1.set_xticks([<span class="op">-</span><span class="fl">0.1</span>, <span class="dv">0</span>, <span class="fl">0.2</span>, <span class="fl">0.4</span>, <span class="fl">0.6</span>, <span class="fl">0.8</span>, <span class="dv">1</span>])</span>
<span id="cb18-78"><a href="#cb18-78" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-79"><a href="#cb18-79" aria-hidden="true" tabindex="-1"></a>    <span class="co"># 2nd Plot showing the actual clusters formed</span></span>
<span id="cb18-80"><a href="#cb18-80" aria-hidden="true" tabindex="-1"></a>    colors <span class="op">=</span> cm.nipy_spectral(cluster_labels.astype(<span class="bu">float</span>) <span class="op">/</span> n_clusters)</span>
<span id="cb18-81"><a href="#cb18-81" aria-hidden="true" tabindex="-1"></a>    ax2.scatter(</span>
<span id="cb18-82"><a href="#cb18-82" aria-hidden="true" tabindex="-1"></a>        X[<span class="st">'log_funds_returned'</span>], X[<span class="st">'log_funds_lost'</span>], marker<span class="op">=</span><span class="st">"."</span>, s<span class="op">=</span><span class="dv">30</span>, lw<span class="op">=</span><span class="dv">0</span>, alpha<span class="op">=</span><span class="fl">0.7</span>, c<span class="op">=</span>colors, edgecolor<span class="op">=</span><span class="st">"k"</span></span>
<span id="cb18-83"><a href="#cb18-83" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb18-84"><a href="#cb18-84" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-85"><a href="#cb18-85" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Labeling the clusters</span></span>
<span id="cb18-86"><a href="#cb18-86" aria-hidden="true" tabindex="-1"></a>    centers <span class="op">=</span> clusterer.cluster_centers_</span>
<span id="cb18-87"><a href="#cb18-87" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Draw white circles at cluster centers</span></span>
<span id="cb18-88"><a href="#cb18-88" aria-hidden="true" tabindex="-1"></a>    ax2.scatter(</span>
<span id="cb18-89"><a href="#cb18-89" aria-hidden="true" tabindex="-1"></a>        centers[:, <span class="dv">0</span>],</span>
<span id="cb18-90"><a href="#cb18-90" aria-hidden="true" tabindex="-1"></a>        centers[:, <span class="dv">1</span>],</span>
<span id="cb18-91"><a href="#cb18-91" aria-hidden="true" tabindex="-1"></a>        marker<span class="op">=</span><span class="st">"o"</span>,</span>
<span id="cb18-92"><a href="#cb18-92" aria-hidden="true" tabindex="-1"></a>        c<span class="op">=</span><span class="st">"white"</span>,</span>
<span id="cb18-93"><a href="#cb18-93" aria-hidden="true" tabindex="-1"></a>        alpha<span class="op">=</span><span class="dv">1</span>,</span>
<span id="cb18-94"><a href="#cb18-94" aria-hidden="true" tabindex="-1"></a>        s<span class="op">=</span><span class="dv">200</span>,</span>
<span id="cb18-95"><a href="#cb18-95" aria-hidden="true" tabindex="-1"></a>        edgecolor<span class="op">=</span><span class="st">"k"</span>,</span>
<span id="cb18-96"><a href="#cb18-96" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb18-97"><a href="#cb18-97" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-98"><a href="#cb18-98" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i, c <span class="kw">in</span> <span class="bu">enumerate</span>(centers):</span>
<span id="cb18-99"><a href="#cb18-99" aria-hidden="true" tabindex="-1"></a>        ax2.scatter(c[<span class="dv">0</span>], c[<span class="dv">1</span>], marker<span class="op">=</span><span class="st">"$</span><span class="sc">%d</span><span class="st">$"</span> <span class="op">%</span> i, alpha<span class="op">=</span><span class="dv">1</span>, s<span class="op">=</span><span class="dv">50</span>, edgecolor<span class="op">=</span><span class="st">"k"</span>)</span>
<span id="cb18-100"><a href="#cb18-100" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-101"><a href="#cb18-101" aria-hidden="true" tabindex="-1"></a>    ax2.set_title(<span class="st">"Visualization of the Clustered Continuous Features"</span>)</span>
<span id="cb18-102"><a href="#cb18-102" aria-hidden="true" tabindex="-1"></a>    ax2.set_xlabel(<span class="st">"log_funds_returned"</span>)</span>
<span id="cb18-103"><a href="#cb18-103" aria-hidden="true" tabindex="-1"></a>    ax2.set_ylabel(<span class="st">"log_funds_lost"</span>)</span>
<span id="cb18-104"><a href="#cb18-104" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-105"><a href="#cb18-105" aria-hidden="true" tabindex="-1"></a>    plt.suptitle(</span>
<span id="cb18-106"><a href="#cb18-106" aria-hidden="true" tabindex="-1"></a>        <span class="st">"Silhouette analysis for KMeans clustering on Continuous Features with n_clusters = </span><span class="sc">%d</span><span class="st">"</span></span>
<span id="cb18-107"><a href="#cb18-107" aria-hidden="true" tabindex="-1"></a>        <span class="op">%</span> n_clusters,</span>
<span id="cb18-108"><a href="#cb18-108" aria-hidden="true" tabindex="-1"></a>        fontsize<span class="op">=</span><span class="dv">14</span>,</span>
<span id="cb18-109"><a href="#cb18-109" aria-hidden="true" tabindex="-1"></a>        fontweight<span class="op">=</span><span class="st">"bold"</span>,</span>
<span id="cb18-110"><a href="#cb18-110" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb18-111"><a href="#cb18-111" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-112"><a href="#cb18-112" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>For n_clusters = 2 The average silhouette_score is : 0.7836417135554179</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>For n_clusters = 3 The average silhouette_score is : 0.5641864790603397</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>For n_clusters = 4 The average silhouette_score is : 0.5458670160810305
For n_clusters = 5 The average silhouette_score is : 0.5440487943472512</code></pre>
</div>
<div class="cell-output cell-output-display">
<p><img src="clustering_files/figure-html/cell-13-output-4.png" width="1378" height="650"></p>
</div>
<div class="cell-output cell-output-display">
<p><img src="clustering_files/figure-html/cell-13-output-5.png" width="1378" height="650"></p>
</div>
<div class="cell-output cell-output-display">
<p><img src="clustering_files/figure-html/cell-13-output-6.png" width="1378" height="650"></p>
</div>
<div class="cell-output cell-output-display">
<p><img src="clustering_files/figure-html/cell-13-output-7.png" width="1378" height="650"></p>
</div>
</div>
<p>Silhouette plots look to have an edge over elbow method as one can evaluate clusters on multiple criteria, including scores below average Silhouette score (red vertical line), wide fluctuations in the size of the plot, and non-uniform thickness. Therefore, it is highly likely that one can end up determining the most optimal number of clusters in K-means using the above plots.</p>
<p>All n_clusters, 2 to 5, have scores above the average Silhouette score (red vertical line), which means we are down to checking thickness fluctuations of the plots. When n_clusters=2, although the silhouette score is maximized, the thickness of the silhouette plots suggests that the cluster sizes are highly different or non-uniform. Our aim is to choose those n_clusters that correspond to uniform thickness of the clusters’ Silhouette plot.</p>
<p>When n_clusters=3, the silhouette score is lower than that of n_clusters=2, but the thickness of the silhouette plots is still non-uniform. However when n_clusters is equal to 4, all the plots are more or less of similar thickness and, hence, are of similar sizes as can be also verified from the labelled scatter plot on the right. Again, these plots are generated on only continuous features and are used as reference for visualization purposes.</p>
</section>
<section id="final-results-for-k-means-clustering" class="level3">
<h3 class="anchored" data-anchor-id="final-results-for-k-means-clustering">Final results for k-Means Clustering</h3>
<p>When clustering our entire feature data (X), we choose k=3 as the optimal parameter for K-Means as per the elbow method, the silhouette scores, and silhouette plots. However, when we subset our feature data for only continuous features, then k=3 is the optimal parameter as per highest silhouette score. The code below is our final result for K-Means clustering on the entire feature data (X), with a visualization of the 3 different clusters using only continuous features (since visualizing 4 dimensions is infeasible).</p>
<div class="cell" data-execution_count="13">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb22"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb22-1"><a href="#cb22-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Plotting clusters for best k = 3 (as per silhouette method)</span></span>
<span id="cb22-2"><a href="#cb22-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-3"><a href="#cb22-3" aria-hidden="true" tabindex="-1"></a>bestK <span class="op">=</span> KMeans(n_clusters<span class="op">=</span><span class="dv">3</span>, init<span class="op">=</span><span class="st">'k-means++'</span>, random_state<span class="op">=</span><span class="dv">42</span>)</span>
<span id="cb22-4"><a href="#cb22-4" aria-hidden="true" tabindex="-1"></a>kmeans_labels_final <span class="op">=</span> bestK.fit_predict(X)</span>
<span id="cb22-5"><a href="#cb22-5" aria-hidden="true" tabindex="-1"></a>df[<span class="st">'kmeans_final_labels'</span>] <span class="op">=</span> kmeans_labels_final</span>
<span id="cb22-6"><a href="#cb22-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-7"><a href="#cb22-7" aria-hidden="true" tabindex="-1"></a>fig, ax <span class="op">=</span> plt.subplots(<span class="dv">1</span>,<span class="dv">2</span>,figsize<span class="op">=</span>(<span class="dv">10</span>,<span class="dv">5</span>))</span>
<span id="cb22-8"><a href="#cb22-8" aria-hidden="true" tabindex="-1"></a>sns.scatterplot(x<span class="op">=</span><span class="st">"log_funds_returned"</span>, y<span class="op">=</span><span class="st">"log_funds_lost"</span>, hue<span class="op">=</span><span class="st">"scam_type_grouped"</span>, data<span class="op">=</span>df, ax<span class="op">=</span>ax[<span class="dv">0</span>]).<span class="bu">set</span>(title<span class="op">=</span><span class="st">'Log Funds Clusters By Scam Type (Y)'</span>)</span>
<span id="cb22-9"><a href="#cb22-9" aria-hidden="true" tabindex="-1"></a>sns.scatterplot(x<span class="op">=</span><span class="st">"log_funds_returned"</span>, y<span class="op">=</span><span class="st">"log_funds_lost"</span>, hue<span class="op">=</span><span class="st">"kmeans_final_labels"</span>, data<span class="op">=</span>df, ax<span class="op">=</span>ax[<span class="dv">1</span>]).<span class="bu">set</span>(title<span class="op">=</span><span class="st">'Final K-Means Clustering Plot'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display" data-execution_count="13">
<pre><code>[Text(0.5, 1.0, 'Final K-Means Clustering Plot')]</code></pre>
</div>
<div class="cell-output cell-output-display">
<p><img src="clustering_files/figure-html/cell-14-output-2.png" width="812" height="455"></p>
</div>
</div>
</section>
</section>
<section id="perform-dbscan" class="level1">
<h1>Perform DBSCAN</h1>
<div class="cell" data-execution_count="14">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb24"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb24-1"><a href="#cb24-1" aria-hidden="true" tabindex="-1"></a><span class="co"># perform DBSCAN clustering. use the eps and min_samples parameters to find the optimal number of clusters. plot the number of clusters vs the silhouette score. Suggest the optimal number of clusters based on the plot.</span></span>
<span id="cb24-2"><a href="#cb24-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.cluster <span class="im">import</span> DBSCAN</span>
<span id="cb24-3"><a href="#cb24-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-4"><a href="#cb24-4" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> df_cluster</span>
<span id="cb24-5"><a href="#cb24-5" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> DBSCAN(eps<span class="op">=</span><span class="fl">0.5</span>, min_samples<span class="op">=</span><span class="dv">5</span>) </span>
<span id="cb24-6"><a href="#cb24-6" aria-hidden="true" tabindex="-1"></a>model.fit(X)</span>
<span id="cb24-7"><a href="#cb24-7" aria-hidden="true" tabindex="-1"></a>y_pred <span class="op">=</span> model.fit_predict(X)</span>
<span id="cb24-8"><a href="#cb24-8" aria-hidden="true" tabindex="-1"></a>labels_DB <span class="op">=</span> model.labels_</span>
<span id="cb24-9"><a href="#cb24-9" aria-hidden="true" tabindex="-1"></a><span class="co">#print(labels_DB)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<div class="cell" data-execution_count="15">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb25"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb25-1"><a href="#cb25-1" aria-hidden="true" tabindex="-1"></a>df[<span class="st">'DBSCAN_labels'</span>] <span class="op">=</span> labels_DB</span>
<span id="cb25-2"><a href="#cb25-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-3"><a href="#cb25-3" aria-hidden="true" tabindex="-1"></a>fig, ax <span class="op">=</span> plt.subplots(<span class="dv">1</span>,<span class="dv">2</span>, figsize<span class="op">=</span>(<span class="dv">10</span>,<span class="dv">5</span>))</span>
<span id="cb25-4"><a href="#cb25-4" aria-hidden="true" tabindex="-1"></a>sns.scatterplot(x<span class="op">=</span><span class="st">"log_funds_returned"</span>, y<span class="op">=</span><span class="st">"log_funds_lost"</span>, hue<span class="op">=</span><span class="st">"scam_type_grouped"</span>, data<span class="op">=</span>df, ax<span class="op">=</span>ax[<span class="dv">0</span>]).<span class="bu">set</span>(title<span class="op">=</span><span class="st">'Log Funds Clusters By Scam Type (Y)'</span>)</span>
<span id="cb25-5"><a href="#cb25-5" aria-hidden="true" tabindex="-1"></a>sns.scatterplot(x<span class="op">=</span><span class="st">"log_funds_returned"</span>, y<span class="op">=</span><span class="st">"log_funds_lost"</span>, hue<span class="op">=</span><span class="st">"DBSCAN_labels"</span>, data<span class="op">=</span>df[df[<span class="st">'DBSCAN_labels'</span>] <span class="op">!=</span> <span class="op">-</span><span class="dv">1</span>] , ax<span class="op">=</span>ax[<span class="dv">1</span>]).<span class="bu">set</span>(title<span class="op">=</span><span class="st">'DBSCAN Clustering Plot'</span>) <span class="co"># removing label = -1 because it corresponds to noise</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display" data-execution_count="15">
<pre><code>[Text(0.5, 1.0, 'DBSCAN Clustering Plot')]</code></pre>
</div>
<div class="cell-output cell-output-display">
<p><img src="clustering_files/figure-html/cell-16-output-2.png" width="812" height="455"></p>
</div>
</div>
<div class="cell" data-execution_count="16">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb27"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb27-1"><a href="#cb27-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Number of clusters in labels, ignoring noise if present.</span></span>
<span id="cb27-2"><a href="#cb27-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> sklearn.metrics <span class="im">as</span> metrics</span>
<span id="cb27-3"><a href="#cb27-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> homogeneity_score, completeness_score, v_measure_score, adjusted_rand_score</span>
<span id="cb27-4"><a href="#cb27-4" aria-hidden="true" tabindex="-1"></a>n_clusters_ <span class="op">=</span> <span class="bu">len</span>(<span class="bu">set</span>(labels_DB)) <span class="op">-</span> (<span class="dv">1</span> <span class="cf">if</span> <span class="op">-</span><span class="dv">1</span> <span class="kw">in</span> labels_DB <span class="cf">else</span> <span class="dv">0</span>)</span>
<span id="cb27-5"><a href="#cb27-5" aria-hidden="true" tabindex="-1"></a>n_noise_ <span class="op">=</span> <span class="bu">list</span>(labels_DB).count(<span class="op">-</span><span class="dv">1</span>)</span>
<span id="cb27-6"><a href="#cb27-6" aria-hidden="true" tabindex="-1"></a>labels_true <span class="op">=</span> y</span>
<span id="cb27-7"><a href="#cb27-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-8"><a href="#cb27-8" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Estimated number of clusters: </span><span class="sc">%d</span><span class="st">"</span> <span class="op">%</span> n_clusters_)</span>
<span id="cb27-9"><a href="#cb27-9" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Estimated number of noise points: </span><span class="sc">%d</span><span class="st">"</span> <span class="op">%</span> n_noise_)</span>
<span id="cb27-10"><a href="#cb27-10" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Homogeneity: </span><span class="sc">%0.3f</span><span class="st">"</span> <span class="op">%</span> metrics.homogeneity_score(labels_true, labels_DB))</span>
<span id="cb27-11"><a href="#cb27-11" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Completeness: </span><span class="sc">%0.3f</span><span class="st">"</span> <span class="op">%</span> metrics.completeness_score(labels_true, labels_DB))</span>
<span id="cb27-12"><a href="#cb27-12" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"V-measure: </span><span class="sc">%0.3f</span><span class="st">"</span> <span class="op">%</span> metrics.v_measure_score(labels_true, labels_DB))</span>
<span id="cb27-13"><a href="#cb27-13" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Adjusted Rand Index: </span><span class="sc">%0.3f</span><span class="st">"</span> <span class="op">%</span> metrics.adjusted_rand_score(labels_true, labels_DB))</span>
<span id="cb27-14"><a href="#cb27-14" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(</span>
<span id="cb27-15"><a href="#cb27-15" aria-hidden="true" tabindex="-1"></a>    <span class="st">"Adjusted Mutual Information: </span><span class="sc">%0.3f</span><span class="st">"</span></span>
<span id="cb27-16"><a href="#cb27-16" aria-hidden="true" tabindex="-1"></a>    <span class="op">%</span> metrics.adjusted_mutual_info_score(labels_true, labels_DB)</span>
<span id="cb27-17"><a href="#cb27-17" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb27-18"><a href="#cb27-18" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Silhouette Coefficient: </span><span class="sc">%0.3f</span><span class="st">"</span> <span class="op">%</span> metrics.silhouette_score(X, labels_DB))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>Estimated number of clusters: 27
Estimated number of noise points: 510
Homogeneity: 0.228
Completeness: 0.087
V-measure: 0.126
Adjusted Rand Index: 0.110
Adjusted Mutual Information: 0.113
Silhouette Coefficient: -0.208</code></pre>
</div>
</div>
<p>The above information corresponds to the performance of the DBSCAN model we chose initially, with default sklearn hyperp-parameter values, including epsilon = 0.5 and min_samples = 5 (number of samples in a neighborhood for a point to be considered as a core point). The silhouette score is -0.208, signifying that the points are not well clustered to their own cluster (cohesion) compared to other clusters (separation). Moreover, this model generated 27 clusters for this data, which is definitely sub-optimal. The plot on the right, which employs DBSCAN’s generated labels, has noise values removed, which denotes that the initial DBSCAN model classified all log_funds_returned values &gt; 0 as noise, a clearly sub-optimal mode. Let’s see if we can increase the silhouette score and decrease number of clusters by hyper-parameter tuning on <span class="math inline">\(\epsilon\)</span> and min_samples…</p>
<section id="hyper-parameter-tuning-1" class="level3">
<h3 class="anchored" data-anchor-id="hyper-parameter-tuning-1">Hyper-parameter tuning</h3>
<div class="cell" data-execution_count="17">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb29"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb29-1"><a href="#cb29-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Defining the list of hyperparameters to try</span></span>
<span id="cb29-2"><a href="#cb29-2" aria-hidden="true" tabindex="-1"></a>eps_list<span class="op">=</span>np.arange(start<span class="op">=</span><span class="fl">0.01</span>, stop<span class="op">=</span><span class="dv">4</span>, step<span class="op">=</span><span class="fl">0.5</span>)</span>
<span id="cb29-3"><a href="#cb29-3" aria-hidden="true" tabindex="-1"></a>min_sample_list<span class="op">=</span>np.arange(start<span class="op">=</span><span class="dv">5</span>, stop<span class="op">=</span><span class="dv">10</span>, step<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb29-4"><a href="#cb29-4" aria-hidden="true" tabindex="-1"></a> </span>
<span id="cb29-5"><a href="#cb29-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Creating empty data frame to store the silhouette scores for each trials</span></span>
<span id="cb29-6"><a href="#cb29-6" aria-hidden="true" tabindex="-1"></a>silhouette_scores_data<span class="op">=</span>pd.DataFrame()</span>
<span id="cb29-7"><a href="#cb29-7" aria-hidden="true" tabindex="-1"></a> </span>
<span id="cb29-8"><a href="#cb29-8" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> eps_trial <span class="kw">in</span> eps_list:</span>
<span id="cb29-9"><a href="#cb29-9" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> min_sample_trial <span class="kw">in</span> min_sample_list:</span>
<span id="cb29-10"><a href="#cb29-10" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb29-11"><a href="#cb29-11" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Generating DBSAN clusters</span></span>
<span id="cb29-12"><a href="#cb29-12" aria-hidden="true" tabindex="-1"></a>        db <span class="op">=</span> DBSCAN(eps<span class="op">=</span>eps_trial, min_samples<span class="op">=</span>min_sample_trial)</span>
<span id="cb29-13"><a href="#cb29-13" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb29-14"><a href="#cb29-14" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span>(<span class="bu">len</span>(np.unique(db.fit_predict(X)))<span class="op">&gt;=</span><span class="dv">2</span>):</span>
<span id="cb29-15"><a href="#cb29-15" aria-hidden="true" tabindex="-1"></a>            sil_score<span class="op">=</span>silhouette_score(X, db.fit_predict(X))</span>
<span id="cb29-16"><a href="#cb29-16" aria-hidden="true" tabindex="-1"></a>        <span class="cf">else</span>:</span>
<span id="cb29-17"><a href="#cb29-17" aria-hidden="true" tabindex="-1"></a>            <span class="cf">continue</span></span>
<span id="cb29-18"><a href="#cb29-18" aria-hidden="true" tabindex="-1"></a>        trial_parameters<span class="op">=</span><span class="st">"eps:"</span> <span class="op">+</span> <span class="bu">str</span>(eps_trial.<span class="bu">round</span>(<span class="dv">1</span>)) <span class="op">+</span><span class="st">", min_sample:"</span> <span class="op">+</span> <span class="bu">str</span>(min_sample_trial)</span>
<span id="cb29-19"><a href="#cb29-19" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb29-20"><a href="#cb29-20" aria-hidden="true" tabindex="-1"></a>        silhouette_scores_data<span class="op">=</span>silhouette_scores_data.append(pd.DataFrame(data<span class="op">=</span>[[sil_score,trial_parameters]], columns<span class="op">=</span>[<span class="st">"score"</span>, <span class="st">"parameters"</span>]))</span>
<span id="cb29-21"><a href="#cb29-21" aria-hidden="true" tabindex="-1"></a> </span>
<span id="cb29-22"><a href="#cb29-22" aria-hidden="true" tabindex="-1"></a><span class="co"># Finding out the best hyperparameters with highest Score</span></span>
<span id="cb29-23"><a href="#cb29-23" aria-hidden="true" tabindex="-1"></a>silhouette_scores_data.sort_values(by<span class="op">=</span><span class="st">'score'</span>, ascending<span class="op">=</span><span class="va">False</span>).head(<span class="dv">10</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display" data-execution_count="17">

<div>

<table class="dataframe table table-sm table-striped">
  <thead>
    <tr>
      <th></th>
      <th>score</th>
      <th>parameters</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>0.484598</td>
      <td>eps:3.5, min_sample:9</td>
    </tr>
    <tr>
      <th>0</th>
      <td>0.484598</td>
      <td>eps:3.5, min_sample:8</td>
    </tr>
    <tr>
      <th>0</th>
      <td>0.480993</td>
      <td>eps:3.0, min_sample:6</td>
    </tr>
    <tr>
      <th>0</th>
      <td>0.480268</td>
      <td>eps:3.5, min_sample:5</td>
    </tr>
    <tr>
      <th>0</th>
      <td>0.480012</td>
      <td>eps:3.5, min_sample:6</td>
    </tr>
    <tr>
      <th>0</th>
      <td>0.477403</td>
      <td>eps:3.5, min_sample:7</td>
    </tr>
    <tr>
      <th>0</th>
      <td>0.470777</td>
      <td>eps:3.0, min_sample:7</td>
    </tr>
    <tr>
      <th>0</th>
      <td>0.462411</td>
      <td>eps:3.0, min_sample:5</td>
    </tr>
    <tr>
      <th>0</th>
      <td>0.458973</td>
      <td>eps:2.5, min_sample:9</td>
    </tr>
    <tr>
      <th>0</th>
      <td>0.452315</td>
      <td>eps:2.5, min_sample:6</td>
    </tr>
  </tbody>
</table>
</div>
</div>
</div>
<div class="cell" data-execution_count="18">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb30"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb30-1"><a href="#cb30-1" aria-hidden="true" tabindex="-1"></a>fig, ax <span class="op">=</span> plt.subplots()</span>
<span id="cb30-2"><a href="#cb30-2" aria-hidden="true" tabindex="-1"></a>sns.lineplot(x<span class="op">=</span><span class="st">"parameters"</span>, y<span class="op">=</span><span class="st">"score"</span>, data<span class="op">=</span>silhouette_scores_data[silhouette_scores_data[<span class="st">"score"</span>] <span class="op">&gt;</span> <span class="fl">0.45</span>].reset_index())  </span>
<span id="cb30-3"><a href="#cb30-3" aria-hidden="true" tabindex="-1"></a>ax.<span class="bu">set</span>(xlabel<span class="op">=</span><span class="st">'Hyper-parameter (Epsilon &amp; Min_Samples)'</span>, ylabel<span class="op">=</span><span class="st">'Silhouette Score'</span>)</span>
<span id="cb30-4"><a href="#cb30-4" aria-hidden="true" tabindex="-1"></a>plt.xticks(rotation<span class="op">=</span><span class="st">'vertical'</span>)</span>
<span id="cb30-5"><a href="#cb30-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Pad margins so that markers don't get clipped by the axes</span></span>
<span id="cb30-6"><a href="#cb30-6" aria-hidden="true" tabindex="-1"></a>plt.margins(<span class="fl">0.2</span>)</span>
<span id="cb30-7"><a href="#cb30-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Tweak spacing to prevent clipping of tick-labels</span></span>
<span id="cb30-8"><a href="#cb30-8" aria-hidden="true" tabindex="-1"></a>plt.subplots_adjust(bottom<span class="op">=</span><span class="fl">0.15</span>)</span>
<span id="cb30-9"><a href="#cb30-9" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<p><img src="clustering_files/figure-html/cell-19-output-1.png" width="601" height="550"></p>
</div>
</div>
<p>A larger epsilon will produce broader clusters (encompassing more data points) and a smaller epsilon will build smaller clusters. In general, we prefer smaller values because we only want a small fraction of data points within the epsilon distance from each other. Therefore, epsilon=3.5 and min_samples = 8 hyper-parameters give us the best silhouette score and, hence, the best DBSCAN model.</p>
</section>
<section id="final-results-for-dbscan" class="level3">
<h3 class="anchored" data-anchor-id="final-results-for-dbscan">Final results for DBSCAN</h3>
<div class="cell" data-execution_count="19">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb31"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb31-1"><a href="#cb31-1" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> DBSCAN(eps<span class="op">=</span><span class="fl">3.5</span>, min_samples<span class="op">=</span><span class="dv">8</span>) <span class="co"># best hyper-parameter values</span></span>
<span id="cb31-2"><a href="#cb31-2" aria-hidden="true" tabindex="-1"></a>model.fit(X)</span>
<span id="cb31-3"><a href="#cb31-3" aria-hidden="true" tabindex="-1"></a>y_pred <span class="op">=</span> model.fit_predict(X)</span>
<span id="cb31-4"><a href="#cb31-4" aria-hidden="true" tabindex="-1"></a>labels_DB <span class="op">=</span> model.labels_</span>
<span id="cb31-5"><a href="#cb31-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb31-6"><a href="#cb31-6" aria-hidden="true" tabindex="-1"></a>df[<span class="st">'DBSCAN_final_labels'</span>] <span class="op">=</span> labels_DB</span>
<span id="cb31-7"><a href="#cb31-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb31-8"><a href="#cb31-8" aria-hidden="true" tabindex="-1"></a>fig, ax <span class="op">=</span> plt.subplots(<span class="dv">1</span>,<span class="dv">2</span>, figsize<span class="op">=</span>(<span class="dv">10</span>,<span class="dv">5</span>))</span>
<span id="cb31-9"><a href="#cb31-9" aria-hidden="true" tabindex="-1"></a>sns.scatterplot(x<span class="op">=</span><span class="st">"log_funds_returned"</span>, y<span class="op">=</span><span class="st">"log_funds_lost"</span>, hue<span class="op">=</span><span class="st">"scam_type_grouped"</span>, data<span class="op">=</span>df, ax<span class="op">=</span>ax[<span class="dv">0</span>]).<span class="bu">set</span>(title<span class="op">=</span><span class="st">'Log Funds Clusters By Scam Type (Y)'</span>)</span>
<span id="cb31-10"><a href="#cb31-10" aria-hidden="true" tabindex="-1"></a>sns.scatterplot(x<span class="op">=</span><span class="st">"log_funds_returned"</span>, y<span class="op">=</span><span class="st">"log_funds_lost"</span>, hue<span class="op">=</span><span class="st">"DBSCAN_final_labels"</span>, data<span class="op">=</span>df[df[<span class="st">'DBSCAN_final_labels'</span>] <span class="op">!=</span> <span class="op">-</span><span class="dv">1</span>], ax<span class="op">=</span>ax[<span class="dv">1</span>]).<span class="bu">set</span>(title<span class="op">=</span><span class="st">'Final DBSCAN Clustering Plot'</span>) <span class="co"># removing label = -1 because it corresponds to noise</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display" data-execution_count="19">
<pre><code>[Text(0.5, 1.0, 'Final DBSCAN Clustering Plot')]</code></pre>
</div>
<div class="cell-output cell-output-display">
<p><img src="clustering_files/figure-html/cell-20-output-2.png" width="812" height="455"></p>
</div>
</div>
<div class="cell" data-execution_count="20">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb33"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb33-1"><a href="#cb33-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Number of clusters in labels, ignoring noise if present.</span></span>
<span id="cb33-2"><a href="#cb33-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> sklearn.metrics <span class="im">as</span> metrics</span>
<span id="cb33-3"><a href="#cb33-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> homogeneity_score, completeness_score, v_measure_score, adjusted_rand_score</span>
<span id="cb33-4"><a href="#cb33-4" aria-hidden="true" tabindex="-1"></a>n_clusters_ <span class="op">=</span> <span class="bu">len</span>(<span class="bu">set</span>(labels_DB)) <span class="op">-</span> (<span class="dv">1</span> <span class="cf">if</span> <span class="op">-</span><span class="dv">1</span> <span class="kw">in</span> labels_DB <span class="cf">else</span> <span class="dv">0</span>)</span>
<span id="cb33-5"><a href="#cb33-5" aria-hidden="true" tabindex="-1"></a>n_noise_ <span class="op">=</span> <span class="bu">list</span>(labels_DB).count(<span class="op">-</span><span class="dv">1</span>)</span>
<span id="cb33-6"><a href="#cb33-6" aria-hidden="true" tabindex="-1"></a>labels_true <span class="op">=</span> y</span>
<span id="cb33-7"><a href="#cb33-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-8"><a href="#cb33-8" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Estimated number of clusters: </span><span class="sc">%d</span><span class="st">"</span> <span class="op">%</span> n_clusters_)</span>
<span id="cb33-9"><a href="#cb33-9" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Estimated number of noise points: </span><span class="sc">%d</span><span class="st">"</span> <span class="op">%</span> n_noise_)</span>
<span id="cb33-10"><a href="#cb33-10" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Homogeneity: </span><span class="sc">%0.3f</span><span class="st">"</span> <span class="op">%</span> metrics.homogeneity_score(labels_true, labels_DB))</span>
<span id="cb33-11"><a href="#cb33-11" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Completeness: </span><span class="sc">%0.3f</span><span class="st">"</span> <span class="op">%</span> metrics.completeness_score(labels_true, labels_DB))</span>
<span id="cb33-12"><a href="#cb33-12" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"V-measure: </span><span class="sc">%0.3f</span><span class="st">"</span> <span class="op">%</span> metrics.v_measure_score(labels_true, labels_DB))</span>
<span id="cb33-13"><a href="#cb33-13" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Adjusted Rand Index: </span><span class="sc">%0.3f</span><span class="st">"</span> <span class="op">%</span> metrics.adjusted_rand_score(labels_true, labels_DB))</span>
<span id="cb33-14"><a href="#cb33-14" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(</span>
<span id="cb33-15"><a href="#cb33-15" aria-hidden="true" tabindex="-1"></a>    <span class="st">"Adjusted Mutual Information: </span><span class="sc">%0.3f</span><span class="st">"</span></span>
<span id="cb33-16"><a href="#cb33-16" aria-hidden="true" tabindex="-1"></a>    <span class="op">%</span> metrics.adjusted_mutual_info_score(labels_true, labels_DB)</span>
<span id="cb33-17"><a href="#cb33-17" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb33-18"><a href="#cb33-18" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Silhouette Coefficient: </span><span class="sc">%0.3f</span><span class="st">"</span> <span class="op">%</span> metrics.silhouette_score(X, labels_DB))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>Estimated number of clusters: 3
Estimated number of noise points: 43
Homogeneity: 0.081
Completeness: 0.086
V-measure: 0.083
Adjusted Rand Index: 0.010
Adjusted Mutual Information: 0.081
Silhouette Coefficient: 0.485</code></pre>
</div>
</div>
</section>
</section>
<section id="agglomerative-clustering-hierarchical-clustering" class="level1">
<h1>Agglomerative Clustering (Hierarchical clustering)</h1>
<div class="cell" data-execution_count="21">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb35"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb35-1"><a href="#cb35-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Perform Agglomerative Clustering</span></span>
<span id="cb35-2"><a href="#cb35-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> scipy.cluster.hierarchy <span class="im">import</span> dendrogram, linkage</span>
<span id="cb35-3"><a href="#cb35-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.cluster <span class="im">import</span> AgglomerativeClustering</span>
<span id="cb35-4"><a href="#cb35-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb35-5"><a href="#cb35-5" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> AgglomerativeClustering(n_clusters<span class="op">=</span><span class="dv">3</span>, affinity<span class="op">=</span><span class="st">'euclidean'</span>,linkage<span class="op">=</span><span class="st">'ward'</span>).fit(X)</span>
<span id="cb35-6"><a href="#cb35-6" aria-hidden="true" tabindex="-1"></a>labels <span class="op">=</span> model.labels_</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>### Plot the clusters for Agglomerative Clustering</p>
<div class="cell" data-execution_count="22">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb36"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb36-1"><a href="#cb36-1" aria-hidden="true" tabindex="-1"></a><span class="co"># create linkage for agglomerative clustering, and the dendrogram for the linkage. </span></span>
<span id="cb36-2"><a href="#cb36-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-3"><a href="#cb36-3" aria-hidden="true" tabindex="-1"></a>Z <span class="op">=</span> linkage(X, method<span class="op">=</span><span class="st">'ward'</span>) <span class="co"># linkage computed using euclidean distance  </span></span>
<span id="cb36-4"><a href="#cb36-4" aria-hidden="true" tabindex="-1"></a>dend <span class="op">=</span> dendrogram(Z)</span>
<span id="cb36-5"><a href="#cb36-5" aria-hidden="true" tabindex="-1"></a>plt.axhline(y<span class="op">=</span><span class="dv">115</span>, color<span class="op">=</span><span class="st">'r'</span>, linestyle<span class="op">=</span><span class="st">'--'</span>, label<span class="op">=</span><span class="st">'115'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display" data-execution_count="22">
<pre><code>&lt;matplotlib.lines.Line2D at 0x15aac9190&gt;</code></pre>
</div>
<div class="cell-output cell-output-display">
<p><img src="clustering_files/figure-html/cell-23-output-2.png" width="580" height="413"></p>
</div>
</div>
<p>From the above dendrogram, we set a threshold at value 115, which cuts through the dendrogram three times. This means we get three clusters.</p>
<section id="hyper-parameter-tuning-2" class="level3">
<h3 class="anchored" data-anchor-id="hyper-parameter-tuning-2">Hyper-parameter tuning</h3>
<div class="cell" data-execution_count="23">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb38"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb38-1"><a href="#cb38-1" aria-hidden="true" tabindex="-1"></a><span class="co"># THIS WILL ITERATE OVER ONE HYPER-PARAMETER (GRID SEARCH) </span></span>
<span id="cb38-2"><a href="#cb38-2" aria-hidden="true" tabindex="-1"></a><span class="co"># AND RETURN THE CLUSTER RESULT THAT OPTIMIZES THE SILHOUETTE SCORE</span></span>
<span id="cb38-3"><a href="#cb38-3" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> maximize_silhouette(X,algo<span class="op">=</span><span class="st">"birch"</span>,nmax<span class="op">=</span><span class="dv">20</span>,i_plot<span class="op">=</span><span class="va">False</span>):</span>
<span id="cb38-4"><a href="#cb38-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-5"><a href="#cb38-5" aria-hidden="true" tabindex="-1"></a>    <span class="co"># PARAM</span></span>
<span id="cb38-6"><a href="#cb38-6" aria-hidden="true" tabindex="-1"></a>    i_print<span class="op">=</span><span class="va">False</span></span>
<span id="cb38-7"><a href="#cb38-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-8"><a href="#cb38-8" aria-hidden="true" tabindex="-1"></a>    <span class="co">#FORCE CONTIGUOUS</span></span>
<span id="cb38-9"><a href="#cb38-9" aria-hidden="true" tabindex="-1"></a>    X<span class="op">=</span>np.ascontiguousarray(X) </span>
<span id="cb38-10"><a href="#cb38-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-11"><a href="#cb38-11" aria-hidden="true" tabindex="-1"></a>    <span class="co"># LOOP OVER HYPER-PARAM</span></span>
<span id="cb38-12"><a href="#cb38-12" aria-hidden="true" tabindex="-1"></a>    params<span class="op">=</span>[]<span class="op">;</span> sil_scores<span class="op">=</span>[]</span>
<span id="cb38-13"><a href="#cb38-13" aria-hidden="true" tabindex="-1"></a>    sil_max<span class="op">=-</span><span class="dv">10</span></span>
<span id="cb38-14"><a href="#cb38-14" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> param <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">2</span>,nmax<span class="op">+</span><span class="dv">1</span>):</span>
<span id="cb38-15"><a href="#cb38-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-16"><a href="#cb38-16" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span>(algo<span class="op">==</span><span class="st">"ag"</span>):</span>
<span id="cb38-17"><a href="#cb38-17" aria-hidden="true" tabindex="-1"></a>            model <span class="op">=</span> sklearn.cluster.AgglomerativeClustering(n_clusters<span class="op">=</span>param, affinity<span class="op">=</span><span class="st">"cosine"</span>, linkage<span class="op">=</span><span class="st">'single'</span>).fit(X)</span>
<span id="cb38-18"><a href="#cb38-18" aria-hidden="true" tabindex="-1"></a>            labels<span class="op">=</span>model.labels_</span>
<span id="cb38-19"><a href="#cb38-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-20"><a href="#cb38-20" aria-hidden="true" tabindex="-1"></a>        <span class="cf">try</span>:</span>
<span id="cb38-21"><a href="#cb38-21" aria-hidden="true" tabindex="-1"></a>            sil_scores.append(sklearn.metrics.silhouette_score(X,labels))</span>
<span id="cb38-22"><a href="#cb38-22" aria-hidden="true" tabindex="-1"></a>            params.append(param)</span>
<span id="cb38-23"><a href="#cb38-23" aria-hidden="true" tabindex="-1"></a>        <span class="cf">except</span>:</span>
<span id="cb38-24"><a href="#cb38-24" aria-hidden="true" tabindex="-1"></a>            <span class="cf">continue</span> </span>
<span id="cb38-25"><a href="#cb38-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-26"><a href="#cb38-26" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span>(i_print): <span class="bu">print</span>(param,sil_scores[<span class="op">-</span><span class="dv">1</span>])</span>
<span id="cb38-27"><a href="#cb38-27" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb38-28"><a href="#cb38-28" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span>(sil_scores[<span class="op">-</span><span class="dv">1</span>]<span class="op">&gt;</span>sil_max):</span>
<span id="cb38-29"><a href="#cb38-29" aria-hidden="true" tabindex="-1"></a>             opt_param<span class="op">=</span>param</span>
<span id="cb38-30"><a href="#cb38-30" aria-hidden="true" tabindex="-1"></a>             sil_max<span class="op">=</span>sil_scores[<span class="op">-</span><span class="dv">1</span>]</span>
<span id="cb38-31"><a href="#cb38-31" aria-hidden="true" tabindex="-1"></a>             opt_labels<span class="op">=</span>labels</span>
<span id="cb38-32"><a href="#cb38-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-33"><a href="#cb38-33" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"OPTIMAL PARAMETER ="</span>,opt_param)</span>
<span id="cb38-34"><a href="#cb38-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-35"><a href="#cb38-35" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span>(i_plot):</span>
<span id="cb38-36"><a href="#cb38-36" aria-hidden="true" tabindex="-1"></a>        fig, ax <span class="op">=</span> plt.subplots()</span>
<span id="cb38-37"><a href="#cb38-37" aria-hidden="true" tabindex="-1"></a>        ax.plot(params, sil_scores, <span class="st">"-o"</span>)  </span>
<span id="cb38-38"><a href="#cb38-38" aria-hidden="true" tabindex="-1"></a>        ax.<span class="bu">set</span>(xlabel<span class="op">=</span><span class="st">'N_Clusters'</span>, ylabel<span class="op">=</span><span class="st">'Silhouette Score'</span>)</span>
<span id="cb38-39"><a href="#cb38-39" aria-hidden="true" tabindex="-1"></a>        plt.show()</span>
<span id="cb38-40"><a href="#cb38-40" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-41"><a href="#cb38-41" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> opt_labels</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<div class="cell" data-execution_count="24">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb39"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb39-1"><a href="#cb39-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> plot(X,color_vector):</span>
<span id="cb39-2"><a href="#cb39-2" aria-hidden="true" tabindex="-1"></a>    fig, [ax1, ax2] <span class="op">=</span> plt.subplots(<span class="dv">1</span>,<span class="dv">2</span>, figsize<span class="op">=</span>(<span class="dv">10</span>,<span class="dv">5</span>))</span>
<span id="cb39-3"><a href="#cb39-3" aria-hidden="true" tabindex="-1"></a>    sns.scatterplot(df[<span class="st">"log_funds_returned"</span>], df[<span class="st">"log_funds_lost"</span>], hue<span class="op">=</span>df[<span class="st">"scam_type_grouped"</span>], ax<span class="op">=</span>ax1)</span>
<span id="cb39-4"><a href="#cb39-4" aria-hidden="true" tabindex="-1"></a>    ax1.<span class="bu">set</span>(xlabel<span class="op">=</span><span class="st">'log_funds_returned'</span>, ylabel<span class="op">=</span><span class="st">'log_funds_lost'</span>,</span>
<span id="cb39-5"><a href="#cb39-5" aria-hidden="true" tabindex="-1"></a>    title<span class="op">=</span><span class="st">'Log Funds Clusters By Scam Type (Y)'</span>)</span>
<span id="cb39-6"><a href="#cb39-6" aria-hidden="true" tabindex="-1"></a>    ax1.grid()</span>
<span id="cb39-7"><a href="#cb39-7" aria-hidden="true" tabindex="-1"></a>    scatter2 <span class="op">=</span> ax2.scatter(X[<span class="st">'log_funds_returned'</span>], X[<span class="st">'log_funds_lost'</span>],c<span class="op">=</span>color_vector, alpha<span class="op">=</span><span class="fl">0.5</span>) </span>
<span id="cb39-8"><a href="#cb39-8" aria-hidden="true" tabindex="-1"></a>    ax2.<span class="bu">set</span>(xlabel<span class="op">=</span><span class="st">'log_funds_returned'</span>, ylabel<span class="op">=</span><span class="st">'log_funds_lost'</span>,</span>
<span id="cb39-9"><a href="#cb39-9" aria-hidden="true" tabindex="-1"></a>    title<span class="op">=</span><span class="st">'Agglomerative Clustering Plot'</span>)</span>
<span id="cb39-10"><a href="#cb39-10" aria-hidden="true" tabindex="-1"></a>    legend2 <span class="op">=</span> ax2.legend(<span class="op">*</span>scatter2.legend_elements(),</span>
<span id="cb39-11"><a href="#cb39-11" aria-hidden="true" tabindex="-1"></a>                    loc<span class="op">=</span><span class="st">"lower right"</span>, title<span class="op">=</span><span class="st">"Clusters"</span>)</span>
<span id="cb39-12"><a href="#cb39-12" aria-hidden="true" tabindex="-1"></a>    ax2.add_artist(legend2)</span>
<span id="cb39-13"><a href="#cb39-13" aria-hidden="true" tabindex="-1"></a>    ax2.grid()</span>
<span id="cb39-14"><a href="#cb39-14" aria-hidden="true" tabindex="-1"></a>    plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<section id="single-linkage-agglomerative-clustering" class="level4">
<h4 class="anchored" data-anchor-id="single-linkage-agglomerative-clustering">Single-linkage Agglomerative Clustering</h4>
<div class="cell" data-execution_count="25">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb40"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb40-1"><a href="#cb40-1" aria-hidden="true" tabindex="-1"></a>opt_labels<span class="op">=</span>maximize_silhouette(X,algo<span class="op">=</span><span class="st">"ag"</span>,nmax<span class="op">=</span><span class="dv">15</span>, i_plot<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb40-2"><a href="#cb40-2" aria-hidden="true" tabindex="-1"></a>plot(X,opt_labels)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>OPTIMAL PARAMETER = 2</code></pre>
</div>
<div class="cell-output cell-output-display">
<p><img src="clustering_files/figure-html/cell-26-output-2.png" width="601" height="435"></p>
</div>
<div class="cell-output cell-output-display">
<p><img src="clustering_files/figure-html/cell-26-output-3.png" width="812" height="455"></p>
</div>
</div>
<p>Single-linkage uses the minimum of the distances between all observations of the two sets. For the above model, we also chose the affinity hyper-parameter, a metric used to compute the linkages, as cosine and found that the best number of clusters is two, as shown above in the first plot.</p>
</section>
<section id="complete-linkage-agglomerative-clustering" class="level4">
<h4 class="anchored" data-anchor-id="complete-linkage-agglomerative-clustering">Complete-linkage Agglomerative Clustering</h4>
<div class="cell" data-execution_count="26">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb42"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb42-1"><a href="#cb42-1" aria-hidden="true" tabindex="-1"></a><span class="co"># THIS WILL ITERATE OVER ONE HYPER-PARAMETER (GRID SEARCH) </span></span>
<span id="cb42-2"><a href="#cb42-2" aria-hidden="true" tabindex="-1"></a><span class="co"># AND RETURN THE CLUSTER RESULT THAT OPTIMIZES THE SILHOUETTE SCORE</span></span>
<span id="cb42-3"><a href="#cb42-3" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> maximize_silhouette(X,algo<span class="op">=</span><span class="st">"birch"</span>,nmax<span class="op">=</span><span class="dv">20</span>,i_plot<span class="op">=</span><span class="va">False</span>):</span>
<span id="cb42-4"><a href="#cb42-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-5"><a href="#cb42-5" aria-hidden="true" tabindex="-1"></a>    <span class="co"># PARAM</span></span>
<span id="cb42-6"><a href="#cb42-6" aria-hidden="true" tabindex="-1"></a>    i_print<span class="op">=</span><span class="va">False</span></span>
<span id="cb42-7"><a href="#cb42-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-8"><a href="#cb42-8" aria-hidden="true" tabindex="-1"></a>    <span class="co">#FORCE CONTIGUOUS</span></span>
<span id="cb42-9"><a href="#cb42-9" aria-hidden="true" tabindex="-1"></a>    X<span class="op">=</span>np.ascontiguousarray(X) </span>
<span id="cb42-10"><a href="#cb42-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-11"><a href="#cb42-11" aria-hidden="true" tabindex="-1"></a>    <span class="co"># LOOP OVER HYPER-PARAM</span></span>
<span id="cb42-12"><a href="#cb42-12" aria-hidden="true" tabindex="-1"></a>    params<span class="op">=</span>[]<span class="op">;</span> sil_scores<span class="op">=</span>[]</span>
<span id="cb42-13"><a href="#cb42-13" aria-hidden="true" tabindex="-1"></a>    sil_max<span class="op">=-</span><span class="dv">10</span></span>
<span id="cb42-14"><a href="#cb42-14" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> param <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">2</span>,nmax<span class="op">+</span><span class="dv">1</span>):</span>
<span id="cb42-15"><a href="#cb42-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-16"><a href="#cb42-16" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span>(algo<span class="op">==</span><span class="st">"ag"</span>):</span>
<span id="cb42-17"><a href="#cb42-17" aria-hidden="true" tabindex="-1"></a>            model <span class="op">=</span> sklearn.cluster.AgglomerativeClustering(n_clusters<span class="op">=</span>param, affinity<span class="op">=</span><span class="st">"manhattan"</span>, linkage<span class="op">=</span><span class="st">'complete'</span>).fit(X)</span>
<span id="cb42-18"><a href="#cb42-18" aria-hidden="true" tabindex="-1"></a>            labels<span class="op">=</span>model.labels_</span>
<span id="cb42-19"><a href="#cb42-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-20"><a href="#cb42-20" aria-hidden="true" tabindex="-1"></a>        <span class="cf">try</span>:</span>
<span id="cb42-21"><a href="#cb42-21" aria-hidden="true" tabindex="-1"></a>            sil_scores.append(sklearn.metrics.silhouette_score(X,labels))</span>
<span id="cb42-22"><a href="#cb42-22" aria-hidden="true" tabindex="-1"></a>            params.append(param)</span>
<span id="cb42-23"><a href="#cb42-23" aria-hidden="true" tabindex="-1"></a>        <span class="cf">except</span>:</span>
<span id="cb42-24"><a href="#cb42-24" aria-hidden="true" tabindex="-1"></a>            <span class="cf">continue</span> </span>
<span id="cb42-25"><a href="#cb42-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-26"><a href="#cb42-26" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span>(i_print): <span class="bu">print</span>(param,sil_scores[<span class="op">-</span><span class="dv">1</span>])</span>
<span id="cb42-27"><a href="#cb42-27" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb42-28"><a href="#cb42-28" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span>(sil_scores[<span class="op">-</span><span class="dv">1</span>]<span class="op">&gt;</span>sil_max):</span>
<span id="cb42-29"><a href="#cb42-29" aria-hidden="true" tabindex="-1"></a>             opt_param<span class="op">=</span>param</span>
<span id="cb42-30"><a href="#cb42-30" aria-hidden="true" tabindex="-1"></a>             sil_max<span class="op">=</span>sil_scores[<span class="op">-</span><span class="dv">1</span>]</span>
<span id="cb42-31"><a href="#cb42-31" aria-hidden="true" tabindex="-1"></a>             opt_labels<span class="op">=</span>labels</span>
<span id="cb42-32"><a href="#cb42-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-33"><a href="#cb42-33" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"OPTIMAL PARAMETER ="</span>,opt_param)</span>
<span id="cb42-34"><a href="#cb42-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-35"><a href="#cb42-35" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span>(i_plot):</span>
<span id="cb42-36"><a href="#cb42-36" aria-hidden="true" tabindex="-1"></a>        fig, ax <span class="op">=</span> plt.subplots()</span>
<span id="cb42-37"><a href="#cb42-37" aria-hidden="true" tabindex="-1"></a>        ax.plot(params, sil_scores, <span class="st">"-o"</span>)  </span>
<span id="cb42-38"><a href="#cb42-38" aria-hidden="true" tabindex="-1"></a>        ax.<span class="bu">set</span>(xlabel<span class="op">=</span><span class="st">'N_Clusters'</span>, ylabel<span class="op">=</span><span class="st">'Silhouette Score'</span>)</span>
<span id="cb42-39"><a href="#cb42-39" aria-hidden="true" tabindex="-1"></a>        plt.show()</span>
<span id="cb42-40"><a href="#cb42-40" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-41"><a href="#cb42-41" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> opt_labels</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<div class="cell" data-execution_count="27">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb43"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb43-1"><a href="#cb43-1" aria-hidden="true" tabindex="-1"></a>opt_labels<span class="op">=</span>maximize_silhouette(X,algo<span class="op">=</span><span class="st">"ag"</span>,nmax<span class="op">=</span><span class="dv">15</span>, i_plot<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb43-2"><a href="#cb43-2" aria-hidden="true" tabindex="-1"></a>plot(X,opt_labels)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>OPTIMAL PARAMETER = 3</code></pre>
</div>
<div class="cell-output cell-output-display">
<p><img src="clustering_files/figure-html/cell-28-output-2.png" width="601" height="435"></p>
</div>
<div class="cell-output cell-output-display">
<p><img src="clustering_files/figure-html/cell-28-output-3.png" width="812" height="455"></p>
</div>
</div>
<p>Complete-linkage uses the maximum distances between all observations of the two sets. For the above model, we also chose the affinity hyper-parameter, a metric used to compute the linkages, as manhattan and found that the best number of clusters is three, as shown above in the first plot. Therefore, given the plots of two different hyper-parameter tuned models, the single-linkage hierarchical clustering model performs better in clustering, as the clustering predictions better coincide with the existing labels (scam_type_grouped) in the data-set.</p>
</section>
</section>
<section id="final-results-for-agglomerative-clustering" class="level3">
<h3 class="anchored" data-anchor-id="final-results-for-agglomerative-clustering">Final results for Agglomerative Clustering</h3>
<div class="cell" data-execution_count="28">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb45"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb45-1"><a href="#cb45-1" aria-hidden="true" tabindex="-1"></a>final_model <span class="op">=</span> AgglomerativeClustering(n_clusters<span class="op">=</span><span class="dv">2</span>, linkage<span class="op">=</span><span class="st">'single'</span>).fit(X)</span>
<span id="cb45-2"><a href="#cb45-2" aria-hidden="true" tabindex="-1"></a>final_labels <span class="op">=</span> final_model.labels_</span>
<span id="cb45-3"><a href="#cb45-3" aria-hidden="true" tabindex="-1"></a>fig, [ax1, ax2] <span class="op">=</span> plt.subplots(<span class="dv">1</span>,<span class="dv">2</span>, figsize<span class="op">=</span>(<span class="dv">10</span>,<span class="dv">5</span>))</span>
<span id="cb45-4"><a href="#cb45-4" aria-hidden="true" tabindex="-1"></a>sns.scatterplot(df[<span class="st">"log_funds_returned"</span>], df[<span class="st">"log_funds_lost"</span>], hue<span class="op">=</span>df[<span class="st">"scam_type_grouped"</span>], ax<span class="op">=</span>ax1)</span>
<span id="cb45-5"><a href="#cb45-5" aria-hidden="true" tabindex="-1"></a>ax1.<span class="bu">set</span>(xlabel<span class="op">=</span><span class="st">'log_funds_returned'</span>, ylabel<span class="op">=</span><span class="st">'log_funds_lost'</span>,</span>
<span id="cb45-6"><a href="#cb45-6" aria-hidden="true" tabindex="-1"></a>title<span class="op">=</span><span class="st">'Log Funds Clusters By Scam Type (Y)'</span>)</span>
<span id="cb45-7"><a href="#cb45-7" aria-hidden="true" tabindex="-1"></a>ax1.grid()</span>
<span id="cb45-8"><a href="#cb45-8" aria-hidden="true" tabindex="-1"></a>scatter2 <span class="op">=</span> ax2.scatter(X[<span class="st">'log_funds_returned'</span>], X[<span class="st">'log_funds_lost'</span>],c<span class="op">=</span>final_labels, alpha<span class="op">=</span><span class="fl">0.5</span>) </span>
<span id="cb45-9"><a href="#cb45-9" aria-hidden="true" tabindex="-1"></a>ax2.<span class="bu">set</span>(xlabel<span class="op">=</span><span class="st">'log_funds_returned'</span>, ylabel<span class="op">=</span><span class="st">'log_funds_lost'</span>,</span>
<span id="cb45-10"><a href="#cb45-10" aria-hidden="true" tabindex="-1"></a>title<span class="op">=</span><span class="st">'Agglomerative Clustering Plot'</span>)</span>
<span id="cb45-11"><a href="#cb45-11" aria-hidden="true" tabindex="-1"></a>legend2 <span class="op">=</span> ax2.legend(<span class="op">*</span>scatter2.legend_elements(),</span>
<span id="cb45-12"><a href="#cb45-12" aria-hidden="true" tabindex="-1"></a>                loc<span class="op">=</span><span class="st">"lower right"</span>, title<span class="op">=</span><span class="st">"Clusters"</span>)</span>
<span id="cb45-13"><a href="#cb45-13" aria-hidden="true" tabindex="-1"></a>ax2.add_artist(legend2)</span>
<span id="cb45-14"><a href="#cb45-14" aria-hidden="true" tabindex="-1"></a>ax2.grid()</span>
<span id="cb45-15"><a href="#cb45-15" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<p><img src="clustering_files/figure-html/cell-29-output-1.png" width="812" height="455"></p>
</div>
</div>
</section>
</section>
<section id="results" class="level1">
<h1>Results</h1>
<p>We performed clustering using three different models, including K-Means, DBSCAN, and Agglomerative Clustering. The final result of the K-Means model was based off on the outputs of both the silhouette score and the elbow method. The elbow method conveyed that k=4 clusters would be optimum, but the silhouette score for k=4 was 0.35, lower than that of k=5 that had a silhouette score of approximately 0.53.</p>
<p>In fact, the final model for K-Means outputs the best clusters out of all other models in relation to the scam_type_grouped (Y) labels in the data set. The labels and clusters coincide the best out of all other models, as one cluster is dedicated to those observations that have a positive log_funds_returned and so are the Exploit scam type labels. Moreover, only K-Means clusters depict the behavior of correctly clustering those points that have 0 log_funds_returned but positive log_funds_lost into two clusters. If we look at the scatterplot for scam_type_grouped labels, we see that Exit Scams have lower log_funds_lost (green cluster) and Exploit scams have higher log_funds_lost (orange cluster) when log_funds_returned is 0. This characteristic is best captured by the final K-Means model, as label 2 (deep purple) clusters are similar to the Exit Scams (green cluster) and label 0 (pink) clusters are similar to the Exploit scams (orange cluster) when log_funds_returned is 0.</p>
<p>The DBSCAN model had the best improvement out of all models from its initial model to the final model, which did output three clusters, like K-Means did, but did not capture the relation of the scam_type_grouped (Y) labels. This could be because when one looks at the scatterplot of log_funds_returned vs log_funds_lost, he or she can conclude with the naked eye that there exist two clusters only, points where log_funds_returned is 0 and points where log_funds_returned and log_funds_lost are both positive. Therefore, because these clusters seem linearly separable, they can easily be clustered with a simpler algorithm, like K-Means. Moreover, our log transformed numeric features had almost no outliers present due to the transformation, but DBSCAN detected some noise points around the cluster that had positive values for log_funds_returned and log_funds_lost. Therefore, if our data was more complex, perhaps with non-globular clusters and clusters of different sizes, DBSCAN could have performed better than K-Means.</p>
<p>Finally, Agglomerative/Hierarchical clustering did a better job of clustering the feature data (X) than DBSCAN but not as good as K-Means. The initial model was powerful because it outputted three clusters and, upon hyper-parameter tuning, the final model was as good as the initial model. The reason why complete-linkage hyper-parameter model was not chosen as the final model was because it contained overlapping clusters for points that had positive log_funds_returned and log_funds_lost values. The single-linkage model, on the other hand, separated the positive log_funds_returned points from the points that had log_funds_returned = 0, leading to two distinct clusters. However, unlike K-Means, the single-linkage model could not capture cluster associations for points that had log_funds_returned = 0.</p>
</section>
<section id="conclusions" class="level1">
<h1>Conclusions</h1>
<p>To wrap up, we found that the K-Means model clustered the feature dataset (X) most aptly in relation to the target variable (Y), scam_type_grouped, of the dataset. DBSCAN performed the worst and Hierarchical clustering performed second best. The primary finding from our results was that the right clustering model can help extract the ground truths (or labels) from only the feature dataset. Therefore, although simple and easy to execute, clustering models can be highly powerful in generating accurate insights of the overall data from independent features only. It would have helped my modeling process if I had more numeric features attached to the dataset, which would have probably led to not only more clusters being formed but also clusters of different shapes and sizes/densities. For example, an additional numeric feature not related to funds would have added to the complexity of the dataset and, hence, comparing the clusters with those of the labelled (Y) clusters may have yielded different results. Moreover, we could have visualized the data in 3-D, using the three numeric features, and, as a result, added a new perspective to the dataset.</p>
</section>
<section id="references" class="level1">
<h1>References</h1>
<p>Santini, Marina. “Advantages<em>&amp;</em>Disadvantages*of** k:Means<em>and</em>Hierarchical … - Santini.” Accessed November 8, 2022. http://santini.se/teaching/ml/2016/Lect_10/10c_UnsupervisedMethods.pdf.</p>
<p>“Clustering Analysis - Computer Science | Western Michigan University.” Accessed November 10, 2022. https://cs.wmich.edu/alfuqaha/summer14/cs6530/lectures/ClusteringAnalysis.pdf.</p>
<p>Ajitesh Kumar. “Elbow Method vs Silhouette Score - Which Is Better?” Data Analytics, November 28, 2021. https://vitalflux.com/elbow-method-silhouette-score-which-better/#:~:text=The%20elbow%20method%20is%20used,cluster%20or%20across%20different%20clusters.</p>
<p>Bock, Tim. “What Is a Dendrogram?” Displayr, September 13, 2022. https://www.displayr.com/what-is-dendrogram/.</p>
<p>“Demo of DBSCAN Clustering Algorithm.” scikit. Accessed November 9, 2022. https://scikit-learn.org/stable/auto_examples/cluster/plot_dbscan.html.</p>
<p>Hashmi, Farukh. “How to Create Clusters Using DBSCAN in Python.” Thinking Neuron, November 27, 2021. https://thinkingneuron.com/how-to-create-clusters-using-dbscan-in-python/.</p>
<!-- -->

</section>

</main>
<!-- /main column -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    target: function(trigger) {
      return trigger.previousElementSibling;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    setTimeout(function() {
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  const viewSource = window.document.getElementById('quarto-view-source') ||
                     window.document.getElementById('quarto-code-tools-source');
  if (viewSource) {
    const sourceUrl = viewSource.getAttribute("data-quarto-source-url");
    viewSource.addEventListener("click", function(e) {
      if (sourceUrl) {
        // rstudio viewer pane
        if (/\bcapabilities=\b/.test(window.location)) {
          window.open(sourceUrl);
        } else {
          window.location.href = sourceUrl;
        }
      } else {
        const modal = new bootstrap.Modal(document.getElementById('quarto-embedded-source-code-modal'));
        modal.show();
      }
      return false;
    });
  }
  function toggleCodeHandler(show) {
    return function(e) {
      const detailsSrc = window.document.querySelectorAll(".cell > details > .sourceCode");
      for (let i=0; i<detailsSrc.length; i++) {
        const details = detailsSrc[i].parentElement;
        if (show) {
          details.open = true;
        } else {
          details.removeAttribute("open");
        }
      }
      const cellCodeDivs = window.document.querySelectorAll(".cell > .sourceCode");
      const fromCls = show ? "hidden" : "unhidden";
      const toCls = show ? "unhidden" : "hidden";
      for (let i=0; i<cellCodeDivs.length; i++) {
        const codeDiv = cellCodeDivs[i];
        if (codeDiv.classList.contains(fromCls)) {
          codeDiv.classList.remove(fromCls);
          codeDiv.classList.add(toCls);
        } 
      }
      return false;
    }
  }
  const hideAllCode = window.document.getElementById("quarto-hide-all-code");
  if (hideAllCode) {
    hideAllCode.addEventListener("click", toggleCodeHandler(false));
  }
  const showAllCode = window.document.getElementById("quarto-show-all-code");
  if (showAllCode) {
    showAllCode.addEventListener("click", toggleCodeHandler(true));
  }
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const cites = ref.parentNode.getAttribute('data-cites').split(' ');
    tippyHover(ref, function() {
      var popup = window.document.createElement('div');
      cites.forEach(function(cite) {
        var citeDiv = window.document.createElement('div');
        citeDiv.classList.add('hanging-indent');
        citeDiv.classList.add('csl-entry');
        var biblioDiv = window.document.getElementById('ref-' + cite);
        if (biblioDiv) {
          citeDiv.innerHTML = biblioDiv.innerHTML;
        }
        popup.appendChild(citeDiv);
      });
      return popup.innerHTML;
    });
  }
});
</script><div class="modal fade" id="quarto-embedded-source-code-modal" tabindex="-1" aria-labelledby="quarto-embedded-source-code-modal-label" aria-hidden="true"><div class="modal-dialog modal-dialog-scrollable"><div class="modal-content"><div class="modal-header"><h5 class="modal-title" id="quarto-embedded-source-code-modal-label">Source Code</h5><button class="btn-close" data-bs-dismiss="modal"></button></div><div class="modal-body"><div class="">
<div class="sourceCode" id="cb46" data-shortcodes="false"><pre class="sourceCode markdown code-with-copy"><code class="sourceCode markdown"><span id="cb46-1"><a href="#cb46-1" aria-hidden="true" tabindex="-1"></a><span class="co">---</span></span>
<span id="cb46-2"><a href="#cb46-2" aria-hidden="true" tabindex="-1"></a><span class="an">format:</span></span>
<span id="cb46-3"><a href="#cb46-3" aria-hidden="true" tabindex="-1"></a><span class="co">  html:</span></span>
<span id="cb46-4"><a href="#cb46-4" aria-hidden="true" tabindex="-1"></a><span class="co">    backgroundcolor: "#63666A"</span></span>
<span id="cb46-5"><a href="#cb46-5" aria-hidden="true" tabindex="-1"></a><span class="co">    fontcolor: white</span></span>
<span id="cb46-6"><a href="#cb46-6" aria-hidden="true" tabindex="-1"></a><span class="co">    linkcolor: black</span></span>
<span id="cb46-7"><a href="#cb46-7" aria-hidden="true" tabindex="-1"></a><span class="co">    code-fold: true</span></span>
<span id="cb46-8"><a href="#cb46-8" aria-hidden="true" tabindex="-1"></a><span class="co">    code-tools: true</span></span>
<span id="cb46-9"><a href="#cb46-9" aria-hidden="true" tabindex="-1"></a><span class="co">    df-print: paged</span></span>
<span id="cb46-10"><a href="#cb46-10" aria-hidden="true" tabindex="-1"></a><span class="co">    highlight-style: vim-dark</span></span>
<span id="cb46-11"><a href="#cb46-11" aria-hidden="true" tabindex="-1"></a><span class="an">jupyter:</span><span class="co"> python3</span></span>
<span id="cb46-12"><a href="#cb46-12" aria-hidden="true" tabindex="-1"></a><span class="co">---</span></span>
<span id="cb46-13"><a href="#cb46-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-14"><a href="#cb46-14" aria-hidden="true" tabindex="-1"></a><span class="fu"># K-Means, DBSCAN, and Hierarchical Clustering in Python using Record Data (REKT API)</span></span>
<span id="cb46-15"><a href="#cb46-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-16"><a href="#cb46-16" aria-hidden="true" tabindex="-1"></a><span class="fu">## Introduction</span></span>
<span id="cb46-17"><a href="#cb46-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-18"><a href="#cb46-18" aria-hidden="true" tabindex="-1"></a>For clustering, we shall drop the target variable (Y) of scam_type_grouped, which will leave us with our feature data (X) to cluster with. Moreover, we will need to filter out the categorical features, including scam_networks, month_of_attack, and day_of_week_of_attack, from our feature data to accurately perform clustering using only numeric record features. The feature data (X) that we will eventually be left with will contain 3 record features, including two features for log of funds (lost and returned) and one datetime extracted feature of day_of_year_of_attack. </span>
<span id="cb46-19"><a href="#cb46-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-20"><a href="#cb46-20" aria-hidden="true" tabindex="-1"></a>I did debate with myself to include month_of_attack and day_of_week_of_attack variables for clustering, but since we already have day_of_year_of_attack, it can convey the information of both month_of_attack and day_of_week_of_attack. At the end of the analysis, we will use the target variable (Y) of scam_type_grouped to check if the clustering predictions coincided with the existing labels in the data-set, which will be our goal.</span>
<span id="cb46-21"><a href="#cb46-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-22"><a href="#cb46-22" aria-hidden="true" tabindex="-1"></a><span class="fu">## Theory</span></span>
<span id="cb46-23"><a href="#cb46-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-24"><a href="#cb46-24" aria-hidden="true" tabindex="-1"></a><span class="fu">#### K-Means Clustering</span></span>
<span id="cb46-25"><a href="#cb46-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-26"><a href="#cb46-26" aria-hidden="true" tabindex="-1"></a>K-Means Clustering is a centroid or distance-based algorithm with each cluster of our data points containing a centroid or cluster center. It is also a non-parametric clustering algorithm, which means that it does **not** make strong assumptions about the form of the mapping function that maps input variables (X) to output variables (Y). The objective of this algorithm is to minimize the sum of squared distances between data points and their respective cluster centroid. To understand better, we can break down the algorithm into 5 steps as follows:</span>
<span id="cb46-27"><a href="#cb46-27" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>Choose "k" clusters</span>
<span id="cb46-28"><a href="#cb46-28" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>Select "k" random points from the feature data (X) as centroids </span>
<span id="cb46-29"><a href="#cb46-29" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>Assign remaining datapoints to the closest cluster centroid</span>
<span id="cb46-30"><a href="#cb46-30" aria-hidden="true" tabindex="-1"></a><span class="ss">4. </span>Recompute centroids of newly formed clusters </span>
<span id="cb46-31"><a href="#cb46-31" aria-hidden="true" tabindex="-1"></a><span class="ss">5. </span>Repeat steps 3 and 4</span>
<span id="cb46-32"><a href="#cb46-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-33"><a href="#cb46-33" aria-hidden="true" tabindex="-1"></a>The stopping criteria is when:</span>
<span id="cb46-34"><a href="#cb46-34" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>Centroids of newly formed clusters don't change (convergence)</span>
<span id="cb46-35"><a href="#cb46-35" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>Data points remain in the same cluster</span>
<span id="cb46-36"><a href="#cb46-36" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>Maximum number of iterations are reached</span>
<span id="cb46-37"><a href="#cb46-37" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-38"><a href="#cb46-38" aria-hidden="true" tabindex="-1"></a>To measure the "closeness" of data points with their respective cluster, we measure similarity between the data points using Euclidean Distance (Distance = $\sqrt{(x_1 - y_1)^2+(x_2 - y_2)^2+...+(x_n - y_n)^2}$)</span>
<span id="cb46-39"><a href="#cb46-39" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-40"><a href="#cb46-40" aria-hidden="true" tabindex="-1"></a>The model selection methods to use will include the elbow method to select the optimal value for "k" number of clusters. This selection technique relies on two metrics, Distortion and Inertia. Distortion is calculated as the average of the squared distances from the cluster centers of the respective clusters, using Euclidean distance. Inertia is the sum of squared distances of samples to their closest cluster center. After visualizing the distortion and inertia values for different values of "k", we can determine the optimal number of clusters by selecting the value of k at the “elbow” of the curve, which is the point after which the distortion/inertia start decreasing in a linear fashion or when they start decreasing slowly. A good model is one with low inertia AND a low number of clusters (K). However, a tradeoff exists because as K increases, inertia decreases. I shall also employ the Silhouette score on the entire feature data (X), including categorical variables, as well as on only the numerical variables, log_funds_lost and log_funds_returned, to better visualize the clusters.</span>
<span id="cb46-41"><a href="#cb46-41" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-42"><a href="#cb46-42" aria-hidden="true" tabindex="-1"></a>K-Means clustering does have its limitations, the main drawback that it is sensitive to poor initializiation of clusters, which could lead to the algorithm being stuck in a local optimum space. Moreover, the shape and size of clusters could differ with each other, resulting in larger distances among points. Yet, K-Means is interpretable, efficient, guarantees an approximation, and works well for clusters having similar properties (size, shape, and density).</span>
<span id="cb46-43"><a href="#cb46-43" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-44"><a href="#cb46-44" aria-hidden="true" tabindex="-1"></a><span class="fu">#### DBSCAN (Density-based Spatial Clustering of Applications with Noise)</span></span>
<span id="cb46-45"><a href="#cb46-45" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-46"><a href="#cb46-46" aria-hidden="true" tabindex="-1"></a>DBSCAN is a density-based clustering method that is useful when the clusters are irregular or non-spherical, for example, or when noise and outliers are present in the feature data (X). Given a set of points in some space, it groups together points that are closely packed together (points with many nearby neighbors). Outliers are marked as points that lie alone in low-density regions, whose nearest neighbors are too far away. To understand better, DBSCAN works in the following 3 steps:</span>
<span id="cb46-47"><a href="#cb46-47" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>The algorithm proceeds by arbitrarily picking up a point in the dataset (until all points have been visited).</span>
<span id="cb46-48"><a href="#cb46-48" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>If there are at least minimum number of points (a threshold) clustered together for a region to be considered dense, within a radius of $\epsilon$ (a distance measure used to locate the points in the neighborhood of any point) to the point, then we consider all these points to be part of the same cluster. These minimum number of points, known as minPts, is one parameter for DBSCAN and $\epsilon$ is another parameter.</span>
<span id="cb46-49"><a href="#cb46-49" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>The clusters are then expanded by recursively repeating the neighborhood calculation for each neighboring point.</span>
<span id="cb46-50"><a href="#cb46-50" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-51"><a href="#cb46-51" aria-hidden="true" tabindex="-1"></a>If minPts = 1, then every point on its own will already be a cluster, which in infeasible. If minPts $\leq$ 2, the result will be the same as of hierarchical clustering, with the dendrogram cut at height $\epsilon$. Therefore, minPts must be $\geq$ 3. However, larger values for minPts are usually better for data sets with noise as DBSCAN will then yield more significant clusters.</span>
<span id="cb46-52"><a href="#cb46-52" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-53"><a href="#cb46-53" aria-hidden="true" tabindex="-1"></a>The model selection method to use will include the Silhouette score to select the optimal DBSCAN hyper-parameters. The Silhouette score takes values between -1 to 1, with -1 being the worst value and 1 denoting that the data point i is very compact within the cluster to which it belongs and far away from the other clusters. Values near 0 denote points are on or very close to the decision boundary between two neighboring clusters, leading to overlapping clusters.</span>
<span id="cb46-54"><a href="#cb46-54" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-55"><a href="#cb46-55" aria-hidden="true" tabindex="-1"></a>DBSCAN can locate non-linearly separable clusters but K-Means cannot. Moreover, unlike K-Means, there is no need to specify the number of clusters "k" when employing DBSCAN. However, choosing a value for $\epsilon$ is more difficult than choosing a good initial value for "k" for K-Means because $\epsilon$ is less intuitive to reason.</span>
<span id="cb46-56"><a href="#cb46-56" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-57"><a href="#cb46-57" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Hierarchical (Agglomerative vs Divisive) Clustering</span></span>
<span id="cb46-58"><a href="#cb46-58" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-59"><a href="#cb46-59" aria-hidden="true" tabindex="-1"></a>Hierarchical clustering technique is different from Partitional clustering, which divides the data into non-overlapping clusters such that each data point belongs to exactly one cluster. Hierarchical clustering can be thought of a set of nested clusters organized as a hierarchical tree, visualized through dendrograms. There exist two main types of hierarchical clustering: </span>
<span id="cb46-60"><a href="#cb46-60" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>Agglomerative clustering (bottom up): Start with the points as individual clusters. At each step, move up the hierarchy by merging the closest pair of clusters until only one cluster is left. </span>
<span id="cb46-61"><a href="#cb46-61" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>Divisive clustering (top down): All observations start in one, all-inclusive cluster. At each step, move down the hierarchy by splitting a cluster recursively until each cluster contains a point. Therefore, this approach is exactly opposite to Agglomerative clustering.</span>
<span id="cb46-62"><a href="#cb46-62" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-63"><a href="#cb46-63" aria-hidden="true" tabindex="-1"></a>Hierarchical clustering's process can be visualized with the help of a dendrograma, a type of tree diagram showing hierarchical relationships between different sets of data. The dendrogram can be used to decide when to stop merging the clusters or, in other words, finding the optimal number of clusters. We cut the dendrogram tree with a horizontal line at a height where the line can traverse the maximum distance up and down without intersecting the merging point.</span>
<span id="cb46-64"><a href="#cb46-64" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-65"><a href="#cb46-65" aria-hidden="true" tabindex="-1"></a>Hierarchical clustering, unlike K-Means, does not require initializing the number of clusters apriori, which is a benefit. Moreover, the dendrograms generated by Hierarchical clustering can be more informative than the spherical clusters returned by K-Means. However, Hierarchical clustering can be sensitive to outliers and computationally inefficient when working with large datasets.</span>
<span id="cb46-66"><a href="#cb46-66" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-67"><a href="#cb46-67" aria-hidden="true" tabindex="-1"></a><span class="fu">## Methods</span></span>
<span id="cb46-68"><a href="#cb46-68" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-69"><a href="#cb46-69" aria-hidden="true" tabindex="-1"></a><span class="fu">### Data Selection</span></span>
<span id="cb46-70"><a href="#cb46-70" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-73"><a href="#cb46-73" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb46-74"><a href="#cb46-74" aria-hidden="true" tabindex="-1"></a><span class="co"># import the necessary packages</span></span>
<span id="cb46-75"><a href="#cb46-75" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb46-76"><a href="#cb46-76" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb46-77"><a href="#cb46-77" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb46-78"><a href="#cb46-78" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> seaborn <span class="im">as</span> sns</span>
<span id="cb46-79"><a href="#cb46-79" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> scipy.cluster.hierarchy <span class="im">as</span> sch</span>
<span id="cb46-80"><a href="#cb46-80" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> sklearn.cluster <span class="im">as</span> cluster</span>
<span id="cb46-81"><a href="#cb46-81" aria-hidden="true" tabindex="-1"></a>sns.set_theme(style<span class="op">=</span><span class="st">"whitegrid"</span>, palette<span class="op">=</span><span class="st">'Set2'</span>)</span>
<span id="cb46-82"><a href="#cb46-82" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> warnings</span>
<span id="cb46-83"><a href="#cb46-83" aria-hidden="true" tabindex="-1"></a>warnings.filterwarnings(<span class="st">"ignore"</span>)</span>
<span id="cb46-84"><a href="#cb46-84" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-85"><a href="#cb46-85" aria-hidden="true" tabindex="-1"></a><span class="co"># read in cleaned REKT Database and drop labels and other variables, leaving only feature data (X)</span></span>
<span id="cb46-86"><a href="#cb46-86" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-87"><a href="#cb46-87" aria-hidden="true" tabindex="-1"></a>df <span class="op">=</span> pd.read_csv(<span class="st">"../../data/Clean Data/REKT_Database_Clean_Classification.csv"</span>, index_col<span class="op">=</span>[<span class="dv">0</span>])</span>
<span id="cb46-88"><a href="#cb46-88" aria-hidden="true" tabindex="-1"></a>df_cluster <span class="op">=</span> df.drop([<span class="st">'scam_type_grouped'</span>, <span class="st">'day_of_week_of_attack'</span>, <span class="st">'day_of_year_of_attack'</span>], axis<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb46-89"><a href="#cb46-89" aria-hidden="true" tabindex="-1"></a>df_cluster.head() <span class="co"># visualize first 5 rows</span></span>
<span id="cb46-90"><a href="#cb46-90" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb46-91"><a href="#cb46-91" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-94"><a href="#cb46-94" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb46-95"><a href="#cb46-95" aria-hidden="true" tabindex="-1"></a>df_cluster.shape <span class="co"># get the number of rows and columns</span></span>
<span id="cb46-96"><a href="#cb46-96" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb46-97"><a href="#cb46-97" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-100"><a href="#cb46-100" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb46-101"><a href="#cb46-101" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(df_cluster.info()) <span class="co"># get column information</span></span>
<span id="cb46-102"><a href="#cb46-102" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb46-103"><a href="#cb46-103" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-106"><a href="#cb46-106" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb46-107"><a href="#cb46-107" aria-hidden="true" tabindex="-1"></a>df_cluster.isnull().<span class="bu">sum</span>() <span class="co"># check for missing values</span></span>
<span id="cb46-108"><a href="#cb46-108" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb46-109"><a href="#cb46-109" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-110"><a href="#cb46-110" aria-hidden="true" tabindex="-1"></a><span class="fu">### Feature selection and Pre-processing</span></span>
<span id="cb46-111"><a href="#cb46-111" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-112"><a href="#cb46-112" aria-hidden="true" tabindex="-1"></a>As shown in the code above, we not only dropped the target variable, scam_type_grouped, but also two datetime extracted features, day_of_week_of_attack and day_of_year_of_attack. Dr. Nakul's inputs and my EDA helped me choose between the datetime variables, which led me to only choose month_of_attack as a categorical variable. Recalling from the EDA section, the month of attack conveyed that most attacks took place between August and September, with other attacks taking place roughly at the same frequency in all other months. Therefore, we can expect clusters around certain months.</span>
<span id="cb46-113"><a href="#cb46-113" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-114"><a href="#cb46-114" aria-hidden="true" tabindex="-1"></a>In terms of pre-processing our data, our funds variables are already log-transformed and, if we recall from the EDA section, we obtained a fairly normal distribution for both log funds variables. The only pre-processing left to be done is on the categorical variable, scamNetworks, which represents the unique cryptocurrency token associated with the attack. We will use ```cat.codes``` to encode the categorical variable. Therefore, we do not need to run any scaling methods, such as sklearn's ```StandardScaler```, on our feature data.</span>
<span id="cb46-115"><a href="#cb46-115" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-118"><a href="#cb46-118" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb46-119"><a href="#cb46-119" aria-hidden="true" tabindex="-1"></a><span class="co"># Replace categorical values with category codes by using the cat.codes function. you can either replace them in place or create a new column. show the altered dataframe again by using head() </span></span>
<span id="cb46-120"><a href="#cb46-120" aria-hidden="true" tabindex="-1"></a>df_cluster[<span class="st">'scamNetworks'</span>].value_counts()</span>
<span id="cb46-121"><a href="#cb46-121" aria-hidden="true" tabindex="-1"></a>df_cluster[<span class="st">'scamNetworks'</span>] <span class="op">=</span> df[<span class="st">'scamNetworks'</span>].astype(<span class="st">'category'</span>).cat.codes </span>
<span id="cb46-122"><a href="#cb46-122" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-123"><a href="#cb46-123" aria-hidden="true" tabindex="-1"></a>df_cluster.head()</span>
<span id="cb46-124"><a href="#cb46-124" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb46-125"><a href="#cb46-125" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-126"><a href="#cb46-126" aria-hidden="true" tabindex="-1"></a><span class="fu">### Seperate the dataset into features and labels</span></span>
<span id="cb46-127"><a href="#cb46-127" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-130"><a href="#cb46-130" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb46-131"><a href="#cb46-131" aria-hidden="true" tabindex="-1"></a><span class="co"># Split the dataset in X and y. since this is unsupervised learning, we will not use the y labels.</span></span>
<span id="cb46-132"><a href="#cb46-132" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.model_selection <span class="im">import</span> train_test_split</span>
<span id="cb46-133"><a href="#cb46-133" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.preprocessing <span class="im">import</span> StandardScaler </span>
<span id="cb46-134"><a href="#cb46-134" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-135"><a href="#cb46-135" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> df_cluster</span>
<span id="cb46-136"><a href="#cb46-136" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> df[<span class="st">'scam_type_grouped'</span>]</span>
<span id="cb46-137"><a href="#cb46-137" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-138"><a href="#cb46-138" aria-hidden="true" tabindex="-1"></a><span class="co"># X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)</span></span>
<span id="cb46-139"><a href="#cb46-139" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb46-140"><a href="#cb46-140" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-141"><a href="#cb46-141" aria-hidden="true" tabindex="-1"></a><span class="fu"># Perform K-means Clustering</span></span>
<span id="cb46-142"><a href="#cb46-142" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-145"><a href="#cb46-145" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb46-146"><a href="#cb46-146" aria-hidden="true" tabindex="-1"></a><span class="co"># import relevent libraries for clustering</span></span>
<span id="cb46-147"><a href="#cb46-147" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> statistics <span class="im">import</span> mode</span>
<span id="cb46-148"><a href="#cb46-148" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.cluster <span class="im">import</span> KMeans</span>
<span id="cb46-149"><a href="#cb46-149" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> scipy.spatial.distance <span class="im">import</span> cdist</span>
<span id="cb46-150"><a href="#cb46-150" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> accuracy_score</span>
<span id="cb46-151"><a href="#cb46-151" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb46-152"><a href="#cb46-152" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-155"><a href="#cb46-155" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb46-156"><a href="#cb46-156" aria-hidden="true" tabindex="-1"></a><span class="co"># for k means clustering we will use the elbow method to find the optimal number of clusters. </span></span>
<span id="cb46-157"><a href="#cb46-157" aria-hidden="true" tabindex="-1"></a><span class="co"># we will use the inertia_ attribute to find the sum of squared distances of samples to their closest cluster center. </span></span>
<span id="cb46-158"><a href="#cb46-158" aria-hidden="true" tabindex="-1"></a><span class="co"># we will use the range of 1 to 20 clusters and plot the inertia_ values for each cluster. </span></span>
<span id="cb46-159"><a href="#cb46-159" aria-hidden="true" tabindex="-1"></a>distortions <span class="op">=</span> []</span>
<span id="cb46-160"><a href="#cb46-160" aria-hidden="true" tabindex="-1"></a>inertias <span class="op">=</span> []</span>
<span id="cb46-161"><a href="#cb46-161" aria-hidden="true" tabindex="-1"></a>k <span class="op">=</span> <span class="dv">16</span></span>
<span id="cb46-162"><a href="#cb46-162" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-163"><a href="#cb46-163" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> k <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">1</span>,k):</span>
<span id="cb46-164"><a href="#cb46-164" aria-hidden="true" tabindex="-1"></a>    kmeansmodel <span class="op">=</span> KMeans(n_clusters<span class="op">=</span>k, init<span class="op">=</span><span class="st">'k-means++'</span>, random_state<span class="op">=</span><span class="dv">42</span>)</span>
<span id="cb46-165"><a href="#cb46-165" aria-hidden="true" tabindex="-1"></a>    kmeansmodel.fit(X)</span>
<span id="cb46-166"><a href="#cb46-166" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-167"><a href="#cb46-167" aria-hidden="true" tabindex="-1"></a>    distortions.append(<span class="bu">sum</span>(np.<span class="bu">min</span>(cdist(X, kmeansmodel.cluster_centers_, <span class="st">'euclidean'</span>), axis<span class="op">=</span><span class="dv">1</span>))<span class="op">/</span> X.shape[<span class="dv">0</span>])</span>
<span id="cb46-168"><a href="#cb46-168" aria-hidden="true" tabindex="-1"></a>    inertias.append(kmeansmodel.inertia_)</span>
<span id="cb46-169"><a href="#cb46-169" aria-hidden="true" tabindex="-1"></a>    evaluation<span class="op">=</span>pd.DataFrame.from_records({<span class="st">"Cluster"</span>:np.arange(<span class="dv">1</span>,k<span class="op">+</span><span class="dv">1</span>), <span class="st">"Distortion"</span>:distortions, <span class="st">"Inertia"</span>:inertias})</span>
<span id="cb46-170"><a href="#cb46-170" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-171"><a href="#cb46-171" aria-hidden="true" tabindex="-1"></a>evaluation</span>
<span id="cb46-172"><a href="#cb46-172" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb46-173"><a href="#cb46-173" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-176"><a href="#cb46-176" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb46-177"><a href="#cb46-177" aria-hidden="true" tabindex="-1"></a><span class="co"># plot distortion and inertia for kmeans, you can either plot them seperately or use fig, ax = plt.subplots(1, 2) to plot them in the same figure. Suggest the optimal number of clusters based on the plot.</span></span>
<span id="cb46-178"><a href="#cb46-178" aria-hidden="true" tabindex="-1"></a>evaluation.plot.line(x<span class="op">=</span><span class="st">"Cluster"</span>, subplots<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb46-179"><a href="#cb46-179" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb46-180"><a href="#cb46-180" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-183"><a href="#cb46-183" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb46-184"><a href="#cb46-184" aria-hidden="true" tabindex="-1"></a><span class="co"># Plotting clusters for best k = 4 (as per elbow method above)</span></span>
<span id="cb46-185"><a href="#cb46-185" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-186"><a href="#cb46-186" aria-hidden="true" tabindex="-1"></a>bestK <span class="op">=</span> KMeans(n_clusters<span class="op">=</span><span class="dv">4</span>, init<span class="op">=</span><span class="st">'k-means++'</span>, random_state<span class="op">=</span><span class="dv">42</span>)</span>
<span id="cb46-187"><a href="#cb46-187" aria-hidden="true" tabindex="-1"></a>labels4 <span class="op">=</span> bestK.fit_predict(X)</span>
<span id="cb46-188"><a href="#cb46-188" aria-hidden="true" tabindex="-1"></a>df[<span class="st">'kmeans_labels'</span>] <span class="op">=</span> labels4</span>
<span id="cb46-189"><a href="#cb46-189" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-190"><a href="#cb46-190" aria-hidden="true" tabindex="-1"></a>fig, ax <span class="op">=</span> plt.subplots(<span class="dv">1</span>,<span class="dv">2</span>, figsize<span class="op">=</span>(<span class="dv">10</span>,<span class="dv">5</span>))</span>
<span id="cb46-191"><a href="#cb46-191" aria-hidden="true" tabindex="-1"></a>sns.scatterplot(x<span class="op">=</span><span class="st">"log_funds_returned"</span>, y<span class="op">=</span><span class="st">"log_funds_lost"</span>, hue<span class="op">=</span><span class="st">"scam_type_grouped"</span>, data<span class="op">=</span>df, ax<span class="op">=</span>ax[<span class="dv">0</span>]).<span class="bu">set</span>(title<span class="op">=</span><span class="st">'Log Funds Clusters By Scam Type (Y)'</span>)</span>
<span id="cb46-192"><a href="#cb46-192" aria-hidden="true" tabindex="-1"></a>sns.scatterplot(x<span class="op">=</span><span class="st">"log_funds_returned"</span>, y<span class="op">=</span><span class="st">"log_funds_lost"</span>, hue<span class="op">=</span><span class="st">"kmeans_labels"</span>, data<span class="op">=</span>df, ax<span class="op">=</span>ax[<span class="dv">1</span>]).<span class="bu">set</span>(title<span class="op">=</span><span class="st">'K-Means Clustering Plot'</span>)</span>
<span id="cb46-193"><a href="#cb46-193" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb46-194"><a href="#cb46-194" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-195"><a href="#cb46-195" aria-hidden="true" tabindex="-1"></a>According to the distortion and intertia values across the 15 clusters, the initial K-Means model conveys that k=4 clusters is the optimal number of clusters to use. The scatterplots above provide a binary representation of the two continuous features, log_funds_lost and log_funds_returned, of our feature data (X) to visualize the labels generated by the K-Means model with n_clusters set to 4. Next, we shall explore how the silhouette scores of the K-Means clusters affects our conclusions about this model.</span>
<span id="cb46-196"><a href="#cb46-196" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-197"><a href="#cb46-197" aria-hidden="true" tabindex="-1"></a><span class="fu">### Hyper-parameter tuning</span></span>
<span id="cb46-198"><a href="#cb46-198" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-201"><a href="#cb46-201" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb46-202"><a href="#cb46-202" aria-hidden="true" tabindex="-1"></a><span class="co"># THIS WILL ITERATE OVER ONE HYPER-PARAMETER (GRID SEARCH) </span></span>
<span id="cb46-203"><a href="#cb46-203" aria-hidden="true" tabindex="-1"></a><span class="co"># AND RETURN THE CLUSTER RESULT THAT OPTIMIZES THE SILHOUETTE SCORE</span></span>
<span id="cb46-204"><a href="#cb46-204" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> sklearn.cluster</span>
<span id="cb46-205"><a href="#cb46-205" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-206"><a href="#cb46-206" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> maximize_silhouette(X,algo<span class="op">=</span><span class="st">"birch"</span>,nmax<span class="op">=</span><span class="dv">20</span>,i_plot<span class="op">=</span><span class="va">False</span>):</span>
<span id="cb46-207"><a href="#cb46-207" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-208"><a href="#cb46-208" aria-hidden="true" tabindex="-1"></a>    <span class="co"># PARAM</span></span>
<span id="cb46-209"><a href="#cb46-209" aria-hidden="true" tabindex="-1"></a>    i_print<span class="op">=</span><span class="va">False</span></span>
<span id="cb46-210"><a href="#cb46-210" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-211"><a href="#cb46-211" aria-hidden="true" tabindex="-1"></a>    <span class="co">#FORCE CONTIGUOUS</span></span>
<span id="cb46-212"><a href="#cb46-212" aria-hidden="true" tabindex="-1"></a>    X<span class="op">=</span>np.ascontiguousarray(X) </span>
<span id="cb46-213"><a href="#cb46-213" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-214"><a href="#cb46-214" aria-hidden="true" tabindex="-1"></a>    <span class="co"># LOOP OVER HYPER-PARAM</span></span>
<span id="cb46-215"><a href="#cb46-215" aria-hidden="true" tabindex="-1"></a>    params<span class="op">=</span>[]<span class="op">;</span> sil_scores<span class="op">=</span>[]</span>
<span id="cb46-216"><a href="#cb46-216" aria-hidden="true" tabindex="-1"></a>    sil_max<span class="op">=-</span><span class="dv">10</span></span>
<span id="cb46-217"><a href="#cb46-217" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> param <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">2</span>,nmax<span class="op">+</span><span class="dv">1</span>):</span>
<span id="cb46-218"><a href="#cb46-218" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span>(algo<span class="op">==</span><span class="st">"kmeans"</span>):</span>
<span id="cb46-219"><a href="#cb46-219" aria-hidden="true" tabindex="-1"></a>            model <span class="op">=</span> sklearn.cluster.KMeans(n_clusters<span class="op">=</span>param).fit(X)</span>
<span id="cb46-220"><a href="#cb46-220" aria-hidden="true" tabindex="-1"></a>            labels<span class="op">=</span>model.predict(X)</span>
<span id="cb46-221"><a href="#cb46-221" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-222"><a href="#cb46-222" aria-hidden="true" tabindex="-1"></a>        <span class="cf">try</span>:</span>
<span id="cb46-223"><a href="#cb46-223" aria-hidden="true" tabindex="-1"></a>            sil_scores.append(sklearn.metrics.silhouette_score(X,labels))</span>
<span id="cb46-224"><a href="#cb46-224" aria-hidden="true" tabindex="-1"></a>            params.append(param)</span>
<span id="cb46-225"><a href="#cb46-225" aria-hidden="true" tabindex="-1"></a>        <span class="cf">except</span>:</span>
<span id="cb46-226"><a href="#cb46-226" aria-hidden="true" tabindex="-1"></a>            <span class="cf">continue</span> </span>
<span id="cb46-227"><a href="#cb46-227" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-228"><a href="#cb46-228" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span>(i_print): <span class="bu">print</span>(param,sil_scores[<span class="op">-</span><span class="dv">1</span>])</span>
<span id="cb46-229"><a href="#cb46-229" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb46-230"><a href="#cb46-230" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span>(sil_scores[<span class="op">-</span><span class="dv">1</span>]<span class="op">&gt;</span>sil_max):</span>
<span id="cb46-231"><a href="#cb46-231" aria-hidden="true" tabindex="-1"></a>             opt_param<span class="op">=</span>param</span>
<span id="cb46-232"><a href="#cb46-232" aria-hidden="true" tabindex="-1"></a>             sil_max<span class="op">=</span>sil_scores[<span class="op">-</span><span class="dv">1</span>]</span>
<span id="cb46-233"><a href="#cb46-233" aria-hidden="true" tabindex="-1"></a>             opt_labels<span class="op">=</span>labels</span>
<span id="cb46-234"><a href="#cb46-234" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-235"><a href="#cb46-235" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"OPTIMAL PARAMETER ="</span>,opt_param)</span>
<span id="cb46-236"><a href="#cb46-236" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-237"><a href="#cb46-237" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span>(i_plot):</span>
<span id="cb46-238"><a href="#cb46-238" aria-hidden="true" tabindex="-1"></a>        fig, ax <span class="op">=</span> plt.subplots()</span>
<span id="cb46-239"><a href="#cb46-239" aria-hidden="true" tabindex="-1"></a>        ax.plot(params, sil_scores, <span class="st">"-o"</span>)  </span>
<span id="cb46-240"><a href="#cb46-240" aria-hidden="true" tabindex="-1"></a>        ax.<span class="bu">set</span>(xlabel<span class="op">=</span><span class="st">'Hyper-parameter'</span>, ylabel<span class="op">=</span><span class="st">'Silhouette Score'</span>)</span>
<span id="cb46-241"><a href="#cb46-241" aria-hidden="true" tabindex="-1"></a>        plt.show()</span>
<span id="cb46-242"><a href="#cb46-242" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-243"><a href="#cb46-243" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> opt_labels</span>
<span id="cb46-244"><a href="#cb46-244" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-245"><a href="#cb46-245" aria-hidden="true" tabindex="-1"></a>k_means_opt_labels<span class="op">=</span>maximize_silhouette(X,algo<span class="op">=</span><span class="st">"kmeans"</span>,nmax<span class="op">=</span><span class="dv">15</span>, i_plot<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb46-246"><a href="#cb46-246" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb46-247"><a href="#cb46-247" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-248"><a href="#cb46-248" aria-hidden="true" tabindex="-1"></a>When n_clusters=3, the silhouette score is maximized on our entire feature data (X), including both continuous features of log of funds and the two categorical features of scamNetworks and month of attack. This result differs from that of the elbow method, which conveyed that n_clusters=4 clusters was the optimal solution. The elbow method is used to find the “elbow” point, where adding additional data samples does not change cluster membership much. The Silhouette score determines whether there are large gaps between each sample and all other samples within the same cluster or across different clusters. The major difference between elbow and silhouette scores is that elbow only calculates the euclidean distance whereas silhouette takes into account variables such as variance, skewness, high-low differences, etc. Therefore, both hyper-parameter tuning methods convey different use cases and probably silhouette score can be given more weightage when considering the optimal number of clusters for K-Means.</span>
<span id="cb46-249"><a href="#cb46-249" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-250"><a href="#cb46-250" aria-hidden="true" tabindex="-1"></a>Next, we will subset only the continuous features, log_funds_returned and log_funds_lost, and run K-Means on only these two features to easily visualize the silhouette score hyper-parameter tuning:</span>
<span id="cb46-251"><a href="#cb46-251" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-254"><a href="#cb46-254" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb46-255"><a href="#cb46-255" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> silhouette_samples, silhouette_score</span>
<span id="cb46-256"><a href="#cb46-256" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-257"><a href="#cb46-257" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb46-258"><a href="#cb46-258" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.cm <span class="im">as</span> cm</span>
<span id="cb46-259"><a href="#cb46-259" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-260"><a href="#cb46-260" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> X[[<span class="st">"log_funds_returned"</span>, <span class="st">"log_funds_lost"</span>]] <span class="co"># subset only the continuous features for visualization</span></span>
<span id="cb46-261"><a href="#cb46-261" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-262"><a href="#cb46-262" aria-hidden="true" tabindex="-1"></a>range_n_clusters <span class="op">=</span> [<span class="dv">2</span>, <span class="dv">3</span>, <span class="dv">4</span>, <span class="dv">5</span>]</span>
<span id="cb46-263"><a href="#cb46-263" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-264"><a href="#cb46-264" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> n_clusters <span class="kw">in</span> range_n_clusters:</span>
<span id="cb46-265"><a href="#cb46-265" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Create a subplot with 1 row and 2 columns</span></span>
<span id="cb46-266"><a href="#cb46-266" aria-hidden="true" tabindex="-1"></a>    fig, (ax1, ax2) <span class="op">=</span> plt.subplots(<span class="dv">1</span>, <span class="dv">2</span>)</span>
<span id="cb46-267"><a href="#cb46-267" aria-hidden="true" tabindex="-1"></a>    fig.set_size_inches(<span class="dv">18</span>, <span class="dv">7</span>)</span>
<span id="cb46-268"><a href="#cb46-268" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-269"><a href="#cb46-269" aria-hidden="true" tabindex="-1"></a>    <span class="co"># The 1st subplot is the silhouette plot</span></span>
<span id="cb46-270"><a href="#cb46-270" aria-hidden="true" tabindex="-1"></a>    <span class="co"># The silhouette coefficient can range from -1, 1 but in this example all</span></span>
<span id="cb46-271"><a href="#cb46-271" aria-hidden="true" tabindex="-1"></a>    <span class="co"># lie within [-0.1, 1]</span></span>
<span id="cb46-272"><a href="#cb46-272" aria-hidden="true" tabindex="-1"></a>    ax1.set_xlim([<span class="op">-</span><span class="fl">0.1</span>, <span class="dv">1</span>])</span>
<span id="cb46-273"><a href="#cb46-273" aria-hidden="true" tabindex="-1"></a>    <span class="co"># The (n_clusters+1)*10 is for inserting blank space between silhouette</span></span>
<span id="cb46-274"><a href="#cb46-274" aria-hidden="true" tabindex="-1"></a>    <span class="co"># plots of individual clusters, to demarcate them clearly.</span></span>
<span id="cb46-275"><a href="#cb46-275" aria-hidden="true" tabindex="-1"></a>    ax1.set_ylim([<span class="dv">0</span>, <span class="bu">len</span>(X) <span class="op">+</span> (n_clusters <span class="op">+</span> <span class="dv">1</span>) <span class="op">*</span> <span class="dv">10</span>])</span>
<span id="cb46-276"><a href="#cb46-276" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-277"><a href="#cb46-277" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Initialize the clusterer with n_clusters value and a random generator</span></span>
<span id="cb46-278"><a href="#cb46-278" aria-hidden="true" tabindex="-1"></a>    <span class="co"># seed of 10 for reproducibility.</span></span>
<span id="cb46-279"><a href="#cb46-279" aria-hidden="true" tabindex="-1"></a>    clusterer <span class="op">=</span> KMeans(n_clusters<span class="op">=</span>n_clusters, random_state<span class="op">=</span><span class="dv">10</span>)</span>
<span id="cb46-280"><a href="#cb46-280" aria-hidden="true" tabindex="-1"></a>    cluster_labels <span class="op">=</span> clusterer.fit_predict(X)</span>
<span id="cb46-281"><a href="#cb46-281" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-282"><a href="#cb46-282" aria-hidden="true" tabindex="-1"></a>    <span class="co"># The silhouette_score gives the average value for all the samples.</span></span>
<span id="cb46-283"><a href="#cb46-283" aria-hidden="true" tabindex="-1"></a>    <span class="co"># This gives a perspective into the density and separation of the formed</span></span>
<span id="cb46-284"><a href="#cb46-284" aria-hidden="true" tabindex="-1"></a>    <span class="co"># clusters</span></span>
<span id="cb46-285"><a href="#cb46-285" aria-hidden="true" tabindex="-1"></a>    silhouette_avg <span class="op">=</span> silhouette_score(X, cluster_labels)</span>
<span id="cb46-286"><a href="#cb46-286" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(</span>
<span id="cb46-287"><a href="#cb46-287" aria-hidden="true" tabindex="-1"></a>        <span class="st">"For n_clusters ="</span>,</span>
<span id="cb46-288"><a href="#cb46-288" aria-hidden="true" tabindex="-1"></a>        n_clusters,</span>
<span id="cb46-289"><a href="#cb46-289" aria-hidden="true" tabindex="-1"></a>        <span class="st">"The average silhouette_score is :"</span>,</span>
<span id="cb46-290"><a href="#cb46-290" aria-hidden="true" tabindex="-1"></a>        silhouette_avg,</span>
<span id="cb46-291"><a href="#cb46-291" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb46-292"><a href="#cb46-292" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-293"><a href="#cb46-293" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Compute the silhouette scores for each sample</span></span>
<span id="cb46-294"><a href="#cb46-294" aria-hidden="true" tabindex="-1"></a>    sample_silhouette_values <span class="op">=</span> silhouette_samples(X, cluster_labels)</span>
<span id="cb46-295"><a href="#cb46-295" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-296"><a href="#cb46-296" aria-hidden="true" tabindex="-1"></a>    y_lower <span class="op">=</span> <span class="dv">10</span></span>
<span id="cb46-297"><a href="#cb46-297" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(n_clusters):</span>
<span id="cb46-298"><a href="#cb46-298" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Aggregate the silhouette scores for samples belonging to</span></span>
<span id="cb46-299"><a href="#cb46-299" aria-hidden="true" tabindex="-1"></a>        <span class="co"># cluster i, and sort them</span></span>
<span id="cb46-300"><a href="#cb46-300" aria-hidden="true" tabindex="-1"></a>        ith_cluster_silhouette_values <span class="op">=</span> sample_silhouette_values[cluster_labels <span class="op">==</span> i]</span>
<span id="cb46-301"><a href="#cb46-301" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-302"><a href="#cb46-302" aria-hidden="true" tabindex="-1"></a>        ith_cluster_silhouette_values.sort()</span>
<span id="cb46-303"><a href="#cb46-303" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-304"><a href="#cb46-304" aria-hidden="true" tabindex="-1"></a>        size_cluster_i <span class="op">=</span> ith_cluster_silhouette_values.shape[<span class="dv">0</span>]</span>
<span id="cb46-305"><a href="#cb46-305" aria-hidden="true" tabindex="-1"></a>        y_upper <span class="op">=</span> y_lower <span class="op">+</span> size_cluster_i</span>
<span id="cb46-306"><a href="#cb46-306" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-307"><a href="#cb46-307" aria-hidden="true" tabindex="-1"></a>        color <span class="op">=</span> cm.nipy_spectral(<span class="bu">float</span>(i) <span class="op">/</span> n_clusters)</span>
<span id="cb46-308"><a href="#cb46-308" aria-hidden="true" tabindex="-1"></a>        ax1.fill_betweenx(</span>
<span id="cb46-309"><a href="#cb46-309" aria-hidden="true" tabindex="-1"></a>            np.arange(y_lower, y_upper),</span>
<span id="cb46-310"><a href="#cb46-310" aria-hidden="true" tabindex="-1"></a>            <span class="dv">0</span>,</span>
<span id="cb46-311"><a href="#cb46-311" aria-hidden="true" tabindex="-1"></a>            ith_cluster_silhouette_values,</span>
<span id="cb46-312"><a href="#cb46-312" aria-hidden="true" tabindex="-1"></a>            facecolor<span class="op">=</span>color,</span>
<span id="cb46-313"><a href="#cb46-313" aria-hidden="true" tabindex="-1"></a>            edgecolor<span class="op">=</span>color,</span>
<span id="cb46-314"><a href="#cb46-314" aria-hidden="true" tabindex="-1"></a>            alpha<span class="op">=</span><span class="fl">0.7</span>,</span>
<span id="cb46-315"><a href="#cb46-315" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb46-316"><a href="#cb46-316" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-317"><a href="#cb46-317" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Label the silhouette plots with their cluster numbers at the middle</span></span>
<span id="cb46-318"><a href="#cb46-318" aria-hidden="true" tabindex="-1"></a>        ax1.text(<span class="op">-</span><span class="fl">0.05</span>, y_lower <span class="op">+</span> <span class="fl">0.5</span> <span class="op">*</span> size_cluster_i, <span class="bu">str</span>(i))</span>
<span id="cb46-319"><a href="#cb46-319" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-320"><a href="#cb46-320" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Compute the new y_lower for next plot</span></span>
<span id="cb46-321"><a href="#cb46-321" aria-hidden="true" tabindex="-1"></a>        y_lower <span class="op">=</span> y_upper <span class="op">+</span> <span class="dv">10</span>  <span class="co"># 10 for the 0 samples</span></span>
<span id="cb46-322"><a href="#cb46-322" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-323"><a href="#cb46-323" aria-hidden="true" tabindex="-1"></a>    ax1.set_title(<span class="st">"The silhouette plot for the various clusters."</span>)</span>
<span id="cb46-324"><a href="#cb46-324" aria-hidden="true" tabindex="-1"></a>    ax1.set_xlabel(<span class="st">"The silhouette coefficient values"</span>)</span>
<span id="cb46-325"><a href="#cb46-325" aria-hidden="true" tabindex="-1"></a>    ax1.set_ylabel(<span class="st">"Cluster label"</span>)</span>
<span id="cb46-326"><a href="#cb46-326" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-327"><a href="#cb46-327" aria-hidden="true" tabindex="-1"></a>    <span class="co"># The vertical line for average silhouette score of all the values</span></span>
<span id="cb46-328"><a href="#cb46-328" aria-hidden="true" tabindex="-1"></a>    ax1.axvline(x<span class="op">=</span>silhouette_avg, color<span class="op">=</span><span class="st">"red"</span>, linestyle<span class="op">=</span><span class="st">"--"</span>)</span>
<span id="cb46-329"><a href="#cb46-329" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-330"><a href="#cb46-330" aria-hidden="true" tabindex="-1"></a>    ax1.set_yticks([])  <span class="co"># Clear the yaxis labels / ticks</span></span>
<span id="cb46-331"><a href="#cb46-331" aria-hidden="true" tabindex="-1"></a>    ax1.set_xticks([<span class="op">-</span><span class="fl">0.1</span>, <span class="dv">0</span>, <span class="fl">0.2</span>, <span class="fl">0.4</span>, <span class="fl">0.6</span>, <span class="fl">0.8</span>, <span class="dv">1</span>])</span>
<span id="cb46-332"><a href="#cb46-332" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-333"><a href="#cb46-333" aria-hidden="true" tabindex="-1"></a>    <span class="co"># 2nd Plot showing the actual clusters formed</span></span>
<span id="cb46-334"><a href="#cb46-334" aria-hidden="true" tabindex="-1"></a>    colors <span class="op">=</span> cm.nipy_spectral(cluster_labels.astype(<span class="bu">float</span>) <span class="op">/</span> n_clusters)</span>
<span id="cb46-335"><a href="#cb46-335" aria-hidden="true" tabindex="-1"></a>    ax2.scatter(</span>
<span id="cb46-336"><a href="#cb46-336" aria-hidden="true" tabindex="-1"></a>        X[<span class="st">'log_funds_returned'</span>], X[<span class="st">'log_funds_lost'</span>], marker<span class="op">=</span><span class="st">"."</span>, s<span class="op">=</span><span class="dv">30</span>, lw<span class="op">=</span><span class="dv">0</span>, alpha<span class="op">=</span><span class="fl">0.7</span>, c<span class="op">=</span>colors, edgecolor<span class="op">=</span><span class="st">"k"</span></span>
<span id="cb46-337"><a href="#cb46-337" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb46-338"><a href="#cb46-338" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-339"><a href="#cb46-339" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Labeling the clusters</span></span>
<span id="cb46-340"><a href="#cb46-340" aria-hidden="true" tabindex="-1"></a>    centers <span class="op">=</span> clusterer.cluster_centers_</span>
<span id="cb46-341"><a href="#cb46-341" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Draw white circles at cluster centers</span></span>
<span id="cb46-342"><a href="#cb46-342" aria-hidden="true" tabindex="-1"></a>    ax2.scatter(</span>
<span id="cb46-343"><a href="#cb46-343" aria-hidden="true" tabindex="-1"></a>        centers[:, <span class="dv">0</span>],</span>
<span id="cb46-344"><a href="#cb46-344" aria-hidden="true" tabindex="-1"></a>        centers[:, <span class="dv">1</span>],</span>
<span id="cb46-345"><a href="#cb46-345" aria-hidden="true" tabindex="-1"></a>        marker<span class="op">=</span><span class="st">"o"</span>,</span>
<span id="cb46-346"><a href="#cb46-346" aria-hidden="true" tabindex="-1"></a>        c<span class="op">=</span><span class="st">"white"</span>,</span>
<span id="cb46-347"><a href="#cb46-347" aria-hidden="true" tabindex="-1"></a>        alpha<span class="op">=</span><span class="dv">1</span>,</span>
<span id="cb46-348"><a href="#cb46-348" aria-hidden="true" tabindex="-1"></a>        s<span class="op">=</span><span class="dv">200</span>,</span>
<span id="cb46-349"><a href="#cb46-349" aria-hidden="true" tabindex="-1"></a>        edgecolor<span class="op">=</span><span class="st">"k"</span>,</span>
<span id="cb46-350"><a href="#cb46-350" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb46-351"><a href="#cb46-351" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-352"><a href="#cb46-352" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i, c <span class="kw">in</span> <span class="bu">enumerate</span>(centers):</span>
<span id="cb46-353"><a href="#cb46-353" aria-hidden="true" tabindex="-1"></a>        ax2.scatter(c[<span class="dv">0</span>], c[<span class="dv">1</span>], marker<span class="op">=</span><span class="st">"$</span><span class="sc">%d</span><span class="st">$"</span> <span class="op">%</span> i, alpha<span class="op">=</span><span class="dv">1</span>, s<span class="op">=</span><span class="dv">50</span>, edgecolor<span class="op">=</span><span class="st">"k"</span>)</span>
<span id="cb46-354"><a href="#cb46-354" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-355"><a href="#cb46-355" aria-hidden="true" tabindex="-1"></a>    ax2.set_title(<span class="st">"Visualization of the Clustered Continuous Features"</span>)</span>
<span id="cb46-356"><a href="#cb46-356" aria-hidden="true" tabindex="-1"></a>    ax2.set_xlabel(<span class="st">"log_funds_returned"</span>)</span>
<span id="cb46-357"><a href="#cb46-357" aria-hidden="true" tabindex="-1"></a>    ax2.set_ylabel(<span class="st">"log_funds_lost"</span>)</span>
<span id="cb46-358"><a href="#cb46-358" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-359"><a href="#cb46-359" aria-hidden="true" tabindex="-1"></a>    plt.suptitle(</span>
<span id="cb46-360"><a href="#cb46-360" aria-hidden="true" tabindex="-1"></a>        <span class="st">"Silhouette analysis for KMeans clustering on Continuous Features with n_clusters = </span><span class="sc">%d</span><span class="st">"</span></span>
<span id="cb46-361"><a href="#cb46-361" aria-hidden="true" tabindex="-1"></a>        <span class="op">%</span> n_clusters,</span>
<span id="cb46-362"><a href="#cb46-362" aria-hidden="true" tabindex="-1"></a>        fontsize<span class="op">=</span><span class="dv">14</span>,</span>
<span id="cb46-363"><a href="#cb46-363" aria-hidden="true" tabindex="-1"></a>        fontweight<span class="op">=</span><span class="st">"bold"</span>,</span>
<span id="cb46-364"><a href="#cb46-364" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb46-365"><a href="#cb46-365" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-366"><a href="#cb46-366" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb46-367"><a href="#cb46-367" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb46-368"><a href="#cb46-368" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-369"><a href="#cb46-369" aria-hidden="true" tabindex="-1"></a>Silhouette plots look to have an edge over elbow method as one can evaluate clusters on multiple criteria, including scores below average Silhouette score (red vertical line), wide fluctuations in the size of the plot, and non-uniform thickness. Therefore, it is highly likely that one can end up determining the most optimal number of clusters in K-means using the above plots. </span>
<span id="cb46-370"><a href="#cb46-370" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-371"><a href="#cb46-371" aria-hidden="true" tabindex="-1"></a>All n_clusters, 2 to 5, have scores above the average Silhouette score (red vertical line), which means we are down to checking thickness fluctuations of the plots. When n_clusters=2, although the silhouette score is maximized, the thickness of the silhouette plots suggests that the cluster sizes are highly different or non-uniform. Our aim is to choose those n_clusters that correspond to uniform thickness of the clusters’ Silhouette plot.</span>
<span id="cb46-372"><a href="#cb46-372" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-373"><a href="#cb46-373" aria-hidden="true" tabindex="-1"></a>When n_clusters=3, the silhouette score is lower than that of n_clusters=2, but the thickness of the silhouette plots is still non-uniform. However when n_clusters is equal to 4, all the plots are more or less of similar thickness and, hence, are of similar sizes as can be also verified from the labelled scatter plot on the right. Again, these plots are generated on only continuous features and are used as reference for visualization purposes.</span>
<span id="cb46-374"><a href="#cb46-374" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-375"><a href="#cb46-375" aria-hidden="true" tabindex="-1"></a><span class="fu">### Final results for k-Means Clustering</span></span>
<span id="cb46-376"><a href="#cb46-376" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-377"><a href="#cb46-377" aria-hidden="true" tabindex="-1"></a>When clustering our entire feature data (X), we choose k=3 as the optimal parameter for K-Means as per the elbow method, the silhouette scores, and silhouette plots. However, when we subset our feature data for only continuous features, then k=3 is the optimal parameter as per highest silhouette score. The code below is our final result for K-Means clustering on the entire feature data (X), with a visualization of the 3 different clusters using only continuous features (since visualizing 4 dimensions is infeasible).</span>
<span id="cb46-378"><a href="#cb46-378" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-381"><a href="#cb46-381" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb46-382"><a href="#cb46-382" aria-hidden="true" tabindex="-1"></a><span class="co"># Plotting clusters for best k = 3 (as per silhouette method)</span></span>
<span id="cb46-383"><a href="#cb46-383" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-384"><a href="#cb46-384" aria-hidden="true" tabindex="-1"></a>bestK <span class="op">=</span> KMeans(n_clusters<span class="op">=</span><span class="dv">3</span>, init<span class="op">=</span><span class="st">'k-means++'</span>, random_state<span class="op">=</span><span class="dv">42</span>)</span>
<span id="cb46-385"><a href="#cb46-385" aria-hidden="true" tabindex="-1"></a>kmeans_labels_final <span class="op">=</span> bestK.fit_predict(X)</span>
<span id="cb46-386"><a href="#cb46-386" aria-hidden="true" tabindex="-1"></a>df[<span class="st">'kmeans_final_labels'</span>] <span class="op">=</span> kmeans_labels_final</span>
<span id="cb46-387"><a href="#cb46-387" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-388"><a href="#cb46-388" aria-hidden="true" tabindex="-1"></a>fig, ax <span class="op">=</span> plt.subplots(<span class="dv">1</span>,<span class="dv">2</span>,figsize<span class="op">=</span>(<span class="dv">10</span>,<span class="dv">5</span>))</span>
<span id="cb46-389"><a href="#cb46-389" aria-hidden="true" tabindex="-1"></a>sns.scatterplot(x<span class="op">=</span><span class="st">"log_funds_returned"</span>, y<span class="op">=</span><span class="st">"log_funds_lost"</span>, hue<span class="op">=</span><span class="st">"scam_type_grouped"</span>, data<span class="op">=</span>df, ax<span class="op">=</span>ax[<span class="dv">0</span>]).<span class="bu">set</span>(title<span class="op">=</span><span class="st">'Log Funds Clusters By Scam Type (Y)'</span>)</span>
<span id="cb46-390"><a href="#cb46-390" aria-hidden="true" tabindex="-1"></a>sns.scatterplot(x<span class="op">=</span><span class="st">"log_funds_returned"</span>, y<span class="op">=</span><span class="st">"log_funds_lost"</span>, hue<span class="op">=</span><span class="st">"kmeans_final_labels"</span>, data<span class="op">=</span>df, ax<span class="op">=</span>ax[<span class="dv">1</span>]).<span class="bu">set</span>(title<span class="op">=</span><span class="st">'Final K-Means Clustering Plot'</span>)</span>
<span id="cb46-391"><a href="#cb46-391" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb46-392"><a href="#cb46-392" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-393"><a href="#cb46-393" aria-hidden="true" tabindex="-1"></a><span class="fu"># Perform DBSCAN </span></span>
<span id="cb46-394"><a href="#cb46-394" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-397"><a href="#cb46-397" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb46-398"><a href="#cb46-398" aria-hidden="true" tabindex="-1"></a><span class="co"># perform DBSCAN clustering. use the eps and min_samples parameters to find the optimal number of clusters. plot the number of clusters vs the silhouette score. Suggest the optimal number of clusters based on the plot.</span></span>
<span id="cb46-399"><a href="#cb46-399" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.cluster <span class="im">import</span> DBSCAN</span>
<span id="cb46-400"><a href="#cb46-400" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-401"><a href="#cb46-401" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> df_cluster</span>
<span id="cb46-402"><a href="#cb46-402" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> DBSCAN(eps<span class="op">=</span><span class="fl">0.5</span>, min_samples<span class="op">=</span><span class="dv">5</span>) </span>
<span id="cb46-403"><a href="#cb46-403" aria-hidden="true" tabindex="-1"></a>model.fit(X)</span>
<span id="cb46-404"><a href="#cb46-404" aria-hidden="true" tabindex="-1"></a>y_pred <span class="op">=</span> model.fit_predict(X)</span>
<span id="cb46-405"><a href="#cb46-405" aria-hidden="true" tabindex="-1"></a>labels_DB <span class="op">=</span> model.labels_</span>
<span id="cb46-406"><a href="#cb46-406" aria-hidden="true" tabindex="-1"></a><span class="co">#print(labels_DB)</span></span>
<span id="cb46-407"><a href="#cb46-407" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb46-408"><a href="#cb46-408" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-411"><a href="#cb46-411" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb46-412"><a href="#cb46-412" aria-hidden="true" tabindex="-1"></a>df[<span class="st">'DBSCAN_labels'</span>] <span class="op">=</span> labels_DB</span>
<span id="cb46-413"><a href="#cb46-413" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-414"><a href="#cb46-414" aria-hidden="true" tabindex="-1"></a>fig, ax <span class="op">=</span> plt.subplots(<span class="dv">1</span>,<span class="dv">2</span>, figsize<span class="op">=</span>(<span class="dv">10</span>,<span class="dv">5</span>))</span>
<span id="cb46-415"><a href="#cb46-415" aria-hidden="true" tabindex="-1"></a>sns.scatterplot(x<span class="op">=</span><span class="st">"log_funds_returned"</span>, y<span class="op">=</span><span class="st">"log_funds_lost"</span>, hue<span class="op">=</span><span class="st">"scam_type_grouped"</span>, data<span class="op">=</span>df, ax<span class="op">=</span>ax[<span class="dv">0</span>]).<span class="bu">set</span>(title<span class="op">=</span><span class="st">'Log Funds Clusters By Scam Type (Y)'</span>)</span>
<span id="cb46-416"><a href="#cb46-416" aria-hidden="true" tabindex="-1"></a>sns.scatterplot(x<span class="op">=</span><span class="st">"log_funds_returned"</span>, y<span class="op">=</span><span class="st">"log_funds_lost"</span>, hue<span class="op">=</span><span class="st">"DBSCAN_labels"</span>, data<span class="op">=</span>df[df[<span class="st">'DBSCAN_labels'</span>] <span class="op">!=</span> <span class="op">-</span><span class="dv">1</span>] , ax<span class="op">=</span>ax[<span class="dv">1</span>]).<span class="bu">set</span>(title<span class="op">=</span><span class="st">'DBSCAN Clustering Plot'</span>) <span class="co"># removing label = -1 because it corresponds to noise</span></span>
<span id="cb46-417"><a href="#cb46-417" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb46-418"><a href="#cb46-418" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-421"><a href="#cb46-421" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb46-422"><a href="#cb46-422" aria-hidden="true" tabindex="-1"></a><span class="co"># Number of clusters in labels, ignoring noise if present.</span></span>
<span id="cb46-423"><a href="#cb46-423" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> sklearn.metrics <span class="im">as</span> metrics</span>
<span id="cb46-424"><a href="#cb46-424" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> homogeneity_score, completeness_score, v_measure_score, adjusted_rand_score</span>
<span id="cb46-425"><a href="#cb46-425" aria-hidden="true" tabindex="-1"></a>n_clusters_ <span class="op">=</span> <span class="bu">len</span>(<span class="bu">set</span>(labels_DB)) <span class="op">-</span> (<span class="dv">1</span> <span class="cf">if</span> <span class="op">-</span><span class="dv">1</span> <span class="kw">in</span> labels_DB <span class="cf">else</span> <span class="dv">0</span>)</span>
<span id="cb46-426"><a href="#cb46-426" aria-hidden="true" tabindex="-1"></a>n_noise_ <span class="op">=</span> <span class="bu">list</span>(labels_DB).count(<span class="op">-</span><span class="dv">1</span>)</span>
<span id="cb46-427"><a href="#cb46-427" aria-hidden="true" tabindex="-1"></a>labels_true <span class="op">=</span> y</span>
<span id="cb46-428"><a href="#cb46-428" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-429"><a href="#cb46-429" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Estimated number of clusters: </span><span class="sc">%d</span><span class="st">"</span> <span class="op">%</span> n_clusters_)</span>
<span id="cb46-430"><a href="#cb46-430" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Estimated number of noise points: </span><span class="sc">%d</span><span class="st">"</span> <span class="op">%</span> n_noise_)</span>
<span id="cb46-431"><a href="#cb46-431" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Homogeneity: </span><span class="sc">%0.3f</span><span class="st">"</span> <span class="op">%</span> metrics.homogeneity_score(labels_true, labels_DB))</span>
<span id="cb46-432"><a href="#cb46-432" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Completeness: </span><span class="sc">%0.3f</span><span class="st">"</span> <span class="op">%</span> metrics.completeness_score(labels_true, labels_DB))</span>
<span id="cb46-433"><a href="#cb46-433" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"V-measure: </span><span class="sc">%0.3f</span><span class="st">"</span> <span class="op">%</span> metrics.v_measure_score(labels_true, labels_DB))</span>
<span id="cb46-434"><a href="#cb46-434" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Adjusted Rand Index: </span><span class="sc">%0.3f</span><span class="st">"</span> <span class="op">%</span> metrics.adjusted_rand_score(labels_true, labels_DB))</span>
<span id="cb46-435"><a href="#cb46-435" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(</span>
<span id="cb46-436"><a href="#cb46-436" aria-hidden="true" tabindex="-1"></a>    <span class="st">"Adjusted Mutual Information: </span><span class="sc">%0.3f</span><span class="st">"</span></span>
<span id="cb46-437"><a href="#cb46-437" aria-hidden="true" tabindex="-1"></a>    <span class="op">%</span> metrics.adjusted_mutual_info_score(labels_true, labels_DB)</span>
<span id="cb46-438"><a href="#cb46-438" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb46-439"><a href="#cb46-439" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Silhouette Coefficient: </span><span class="sc">%0.3f</span><span class="st">"</span> <span class="op">%</span> metrics.silhouette_score(X, labels_DB))</span>
<span id="cb46-440"><a href="#cb46-440" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb46-441"><a href="#cb46-441" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-442"><a href="#cb46-442" aria-hidden="true" tabindex="-1"></a>The above information corresponds to the performance of the DBSCAN model we chose initially, with default sklearn hyperp-parameter values, including epsilon = 0.5 and min_samples = 5 (number of samples in a neighborhood for a point to be considered as a core point). The silhouette score is -0.208, signifying that the points are not well clustered to their own cluster (cohesion) compared to other clusters (separation). Moreover, this model generated 27 clusters for this data, which is definitely sub-optimal. The plot on the right, which employs DBSCAN's generated labels, has noise values removed, which denotes that the initial DBSCAN model classified all log_funds_returned values &gt; 0 as noise, a clearly sub-optimal mode. Let's see if we can increase the silhouette score and decrease number of clusters by hyper-parameter tuning on $\epsilon$ and min_samples...</span>
<span id="cb46-443"><a href="#cb46-443" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-444"><a href="#cb46-444" aria-hidden="true" tabindex="-1"></a><span class="fu">### Hyper-parameter tuning</span></span>
<span id="cb46-445"><a href="#cb46-445" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-448"><a href="#cb46-448" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb46-449"><a href="#cb46-449" aria-hidden="true" tabindex="-1"></a><span class="co"># Defining the list of hyperparameters to try</span></span>
<span id="cb46-450"><a href="#cb46-450" aria-hidden="true" tabindex="-1"></a>eps_list<span class="op">=</span>np.arange(start<span class="op">=</span><span class="fl">0.01</span>, stop<span class="op">=</span><span class="dv">4</span>, step<span class="op">=</span><span class="fl">0.5</span>)</span>
<span id="cb46-451"><a href="#cb46-451" aria-hidden="true" tabindex="-1"></a>min_sample_list<span class="op">=</span>np.arange(start<span class="op">=</span><span class="dv">5</span>, stop<span class="op">=</span><span class="dv">10</span>, step<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb46-452"><a href="#cb46-452" aria-hidden="true" tabindex="-1"></a> </span>
<span id="cb46-453"><a href="#cb46-453" aria-hidden="true" tabindex="-1"></a><span class="co"># Creating empty data frame to store the silhouette scores for each trials</span></span>
<span id="cb46-454"><a href="#cb46-454" aria-hidden="true" tabindex="-1"></a>silhouette_scores_data<span class="op">=</span>pd.DataFrame()</span>
<span id="cb46-455"><a href="#cb46-455" aria-hidden="true" tabindex="-1"></a> </span>
<span id="cb46-456"><a href="#cb46-456" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> eps_trial <span class="kw">in</span> eps_list:</span>
<span id="cb46-457"><a href="#cb46-457" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> min_sample_trial <span class="kw">in</span> min_sample_list:</span>
<span id="cb46-458"><a href="#cb46-458" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb46-459"><a href="#cb46-459" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Generating DBSAN clusters</span></span>
<span id="cb46-460"><a href="#cb46-460" aria-hidden="true" tabindex="-1"></a>        db <span class="op">=</span> DBSCAN(eps<span class="op">=</span>eps_trial, min_samples<span class="op">=</span>min_sample_trial)</span>
<span id="cb46-461"><a href="#cb46-461" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb46-462"><a href="#cb46-462" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span>(<span class="bu">len</span>(np.unique(db.fit_predict(X)))<span class="op">&gt;=</span><span class="dv">2</span>):</span>
<span id="cb46-463"><a href="#cb46-463" aria-hidden="true" tabindex="-1"></a>            sil_score<span class="op">=</span>silhouette_score(X, db.fit_predict(X))</span>
<span id="cb46-464"><a href="#cb46-464" aria-hidden="true" tabindex="-1"></a>        <span class="cf">else</span>:</span>
<span id="cb46-465"><a href="#cb46-465" aria-hidden="true" tabindex="-1"></a>            <span class="cf">continue</span></span>
<span id="cb46-466"><a href="#cb46-466" aria-hidden="true" tabindex="-1"></a>        trial_parameters<span class="op">=</span><span class="st">"eps:"</span> <span class="op">+</span> <span class="bu">str</span>(eps_trial.<span class="bu">round</span>(<span class="dv">1</span>)) <span class="op">+</span><span class="st">", min_sample:"</span> <span class="op">+</span> <span class="bu">str</span>(min_sample_trial)</span>
<span id="cb46-467"><a href="#cb46-467" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb46-468"><a href="#cb46-468" aria-hidden="true" tabindex="-1"></a>        silhouette_scores_data<span class="op">=</span>silhouette_scores_data.append(pd.DataFrame(data<span class="op">=</span>[[sil_score,trial_parameters]], columns<span class="op">=</span>[<span class="st">"score"</span>, <span class="st">"parameters"</span>]))</span>
<span id="cb46-469"><a href="#cb46-469" aria-hidden="true" tabindex="-1"></a> </span>
<span id="cb46-470"><a href="#cb46-470" aria-hidden="true" tabindex="-1"></a><span class="co"># Finding out the best hyperparameters with highest Score</span></span>
<span id="cb46-471"><a href="#cb46-471" aria-hidden="true" tabindex="-1"></a>silhouette_scores_data.sort_values(by<span class="op">=</span><span class="st">'score'</span>, ascending<span class="op">=</span><span class="va">False</span>).head(<span class="dv">10</span>)</span>
<span id="cb46-472"><a href="#cb46-472" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb46-473"><a href="#cb46-473" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-476"><a href="#cb46-476" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb46-477"><a href="#cb46-477" aria-hidden="true" tabindex="-1"></a>fig, ax <span class="op">=</span> plt.subplots()</span>
<span id="cb46-478"><a href="#cb46-478" aria-hidden="true" tabindex="-1"></a>sns.lineplot(x<span class="op">=</span><span class="st">"parameters"</span>, y<span class="op">=</span><span class="st">"score"</span>, data<span class="op">=</span>silhouette_scores_data[silhouette_scores_data[<span class="st">"score"</span>] <span class="op">&gt;</span> <span class="fl">0.45</span>].reset_index())  </span>
<span id="cb46-479"><a href="#cb46-479" aria-hidden="true" tabindex="-1"></a>ax.<span class="bu">set</span>(xlabel<span class="op">=</span><span class="st">'Hyper-parameter (Epsilon &amp; Min_Samples)'</span>, ylabel<span class="op">=</span><span class="st">'Silhouette Score'</span>)</span>
<span id="cb46-480"><a href="#cb46-480" aria-hidden="true" tabindex="-1"></a>plt.xticks(rotation<span class="op">=</span><span class="st">'vertical'</span>)</span>
<span id="cb46-481"><a href="#cb46-481" aria-hidden="true" tabindex="-1"></a><span class="co"># Pad margins so that markers don't get clipped by the axes</span></span>
<span id="cb46-482"><a href="#cb46-482" aria-hidden="true" tabindex="-1"></a>plt.margins(<span class="fl">0.2</span>)</span>
<span id="cb46-483"><a href="#cb46-483" aria-hidden="true" tabindex="-1"></a><span class="co"># Tweak spacing to prevent clipping of tick-labels</span></span>
<span id="cb46-484"><a href="#cb46-484" aria-hidden="true" tabindex="-1"></a>plt.subplots_adjust(bottom<span class="op">=</span><span class="fl">0.15</span>)</span>
<span id="cb46-485"><a href="#cb46-485" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb46-486"><a href="#cb46-486" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb46-487"><a href="#cb46-487" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-488"><a href="#cb46-488" aria-hidden="true" tabindex="-1"></a>A larger epsilon will produce broader clusters (encompassing more data points) and a smaller epsilon will build smaller clusters. In general, we prefer smaller values because we only want a small fraction of data points within the epsilon distance from each other. Therefore, epsilon=3.5 and min_samples = 8 hyper-parameters give us the best silhouette score and, hence, the best DBSCAN model.</span>
<span id="cb46-489"><a href="#cb46-489" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-490"><a href="#cb46-490" aria-hidden="true" tabindex="-1"></a><span class="fu">### Final results for DBSCAN</span></span>
<span id="cb46-491"><a href="#cb46-491" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-494"><a href="#cb46-494" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb46-495"><a href="#cb46-495" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> DBSCAN(eps<span class="op">=</span><span class="fl">3.5</span>, min_samples<span class="op">=</span><span class="dv">8</span>) <span class="co"># best hyper-parameter values</span></span>
<span id="cb46-496"><a href="#cb46-496" aria-hidden="true" tabindex="-1"></a>model.fit(X)</span>
<span id="cb46-497"><a href="#cb46-497" aria-hidden="true" tabindex="-1"></a>y_pred <span class="op">=</span> model.fit_predict(X)</span>
<span id="cb46-498"><a href="#cb46-498" aria-hidden="true" tabindex="-1"></a>labels_DB <span class="op">=</span> model.labels_</span>
<span id="cb46-499"><a href="#cb46-499" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-500"><a href="#cb46-500" aria-hidden="true" tabindex="-1"></a>df[<span class="st">'DBSCAN_final_labels'</span>] <span class="op">=</span> labels_DB</span>
<span id="cb46-501"><a href="#cb46-501" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-502"><a href="#cb46-502" aria-hidden="true" tabindex="-1"></a>fig, ax <span class="op">=</span> plt.subplots(<span class="dv">1</span>,<span class="dv">2</span>, figsize<span class="op">=</span>(<span class="dv">10</span>,<span class="dv">5</span>))</span>
<span id="cb46-503"><a href="#cb46-503" aria-hidden="true" tabindex="-1"></a>sns.scatterplot(x<span class="op">=</span><span class="st">"log_funds_returned"</span>, y<span class="op">=</span><span class="st">"log_funds_lost"</span>, hue<span class="op">=</span><span class="st">"scam_type_grouped"</span>, data<span class="op">=</span>df, ax<span class="op">=</span>ax[<span class="dv">0</span>]).<span class="bu">set</span>(title<span class="op">=</span><span class="st">'Log Funds Clusters By Scam Type (Y)'</span>)</span>
<span id="cb46-504"><a href="#cb46-504" aria-hidden="true" tabindex="-1"></a>sns.scatterplot(x<span class="op">=</span><span class="st">"log_funds_returned"</span>, y<span class="op">=</span><span class="st">"log_funds_lost"</span>, hue<span class="op">=</span><span class="st">"DBSCAN_final_labels"</span>, data<span class="op">=</span>df[df[<span class="st">'DBSCAN_final_labels'</span>] <span class="op">!=</span> <span class="op">-</span><span class="dv">1</span>], ax<span class="op">=</span>ax[<span class="dv">1</span>]).<span class="bu">set</span>(title<span class="op">=</span><span class="st">'Final DBSCAN Clustering Plot'</span>) <span class="co"># removing label = -1 because it corresponds to noise</span></span>
<span id="cb46-505"><a href="#cb46-505" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb46-506"><a href="#cb46-506" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-509"><a href="#cb46-509" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb46-510"><a href="#cb46-510" aria-hidden="true" tabindex="-1"></a><span class="co"># Number of clusters in labels, ignoring noise if present.</span></span>
<span id="cb46-511"><a href="#cb46-511" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> sklearn.metrics <span class="im">as</span> metrics</span>
<span id="cb46-512"><a href="#cb46-512" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> homogeneity_score, completeness_score, v_measure_score, adjusted_rand_score</span>
<span id="cb46-513"><a href="#cb46-513" aria-hidden="true" tabindex="-1"></a>n_clusters_ <span class="op">=</span> <span class="bu">len</span>(<span class="bu">set</span>(labels_DB)) <span class="op">-</span> (<span class="dv">1</span> <span class="cf">if</span> <span class="op">-</span><span class="dv">1</span> <span class="kw">in</span> labels_DB <span class="cf">else</span> <span class="dv">0</span>)</span>
<span id="cb46-514"><a href="#cb46-514" aria-hidden="true" tabindex="-1"></a>n_noise_ <span class="op">=</span> <span class="bu">list</span>(labels_DB).count(<span class="op">-</span><span class="dv">1</span>)</span>
<span id="cb46-515"><a href="#cb46-515" aria-hidden="true" tabindex="-1"></a>labels_true <span class="op">=</span> y</span>
<span id="cb46-516"><a href="#cb46-516" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-517"><a href="#cb46-517" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Estimated number of clusters: </span><span class="sc">%d</span><span class="st">"</span> <span class="op">%</span> n_clusters_)</span>
<span id="cb46-518"><a href="#cb46-518" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Estimated number of noise points: </span><span class="sc">%d</span><span class="st">"</span> <span class="op">%</span> n_noise_)</span>
<span id="cb46-519"><a href="#cb46-519" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Homogeneity: </span><span class="sc">%0.3f</span><span class="st">"</span> <span class="op">%</span> metrics.homogeneity_score(labels_true, labels_DB))</span>
<span id="cb46-520"><a href="#cb46-520" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Completeness: </span><span class="sc">%0.3f</span><span class="st">"</span> <span class="op">%</span> metrics.completeness_score(labels_true, labels_DB))</span>
<span id="cb46-521"><a href="#cb46-521" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"V-measure: </span><span class="sc">%0.3f</span><span class="st">"</span> <span class="op">%</span> metrics.v_measure_score(labels_true, labels_DB))</span>
<span id="cb46-522"><a href="#cb46-522" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Adjusted Rand Index: </span><span class="sc">%0.3f</span><span class="st">"</span> <span class="op">%</span> metrics.adjusted_rand_score(labels_true, labels_DB))</span>
<span id="cb46-523"><a href="#cb46-523" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(</span>
<span id="cb46-524"><a href="#cb46-524" aria-hidden="true" tabindex="-1"></a>    <span class="st">"Adjusted Mutual Information: </span><span class="sc">%0.3f</span><span class="st">"</span></span>
<span id="cb46-525"><a href="#cb46-525" aria-hidden="true" tabindex="-1"></a>    <span class="op">%</span> metrics.adjusted_mutual_info_score(labels_true, labels_DB)</span>
<span id="cb46-526"><a href="#cb46-526" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb46-527"><a href="#cb46-527" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Silhouette Coefficient: </span><span class="sc">%0.3f</span><span class="st">"</span> <span class="op">%</span> metrics.silhouette_score(X, labels_DB))</span>
<span id="cb46-528"><a href="#cb46-528" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb46-529"><a href="#cb46-529" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-530"><a href="#cb46-530" aria-hidden="true" tabindex="-1"></a><span class="fu"># Agglomerative Clustering (Hierarchical clustering)</span></span>
<span id="cb46-531"><a href="#cb46-531" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-534"><a href="#cb46-534" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb46-535"><a href="#cb46-535" aria-hidden="true" tabindex="-1"></a><span class="co"># Perform Agglomerative Clustering</span></span>
<span id="cb46-536"><a href="#cb46-536" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> scipy.cluster.hierarchy <span class="im">import</span> dendrogram, linkage</span>
<span id="cb46-537"><a href="#cb46-537" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.cluster <span class="im">import</span> AgglomerativeClustering</span>
<span id="cb46-538"><a href="#cb46-538" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-539"><a href="#cb46-539" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> AgglomerativeClustering(n_clusters<span class="op">=</span><span class="dv">3</span>, affinity<span class="op">=</span><span class="st">'euclidean'</span>,linkage<span class="op">=</span><span class="st">'ward'</span>).fit(X)</span>
<span id="cb46-540"><a href="#cb46-540" aria-hidden="true" tabindex="-1"></a>labels <span class="op">=</span> model.labels_</span>
<span id="cb46-541"><a href="#cb46-541" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb46-542"><a href="#cb46-542" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-543"><a href="#cb46-543" aria-hidden="true" tabindex="-1"></a> ### Plot the clusters for Agglomerative Clustering</span>
<span id="cb46-544"><a href="#cb46-544" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-547"><a href="#cb46-547" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb46-548"><a href="#cb46-548" aria-hidden="true" tabindex="-1"></a><span class="co"># create linkage for agglomerative clustering, and the dendrogram for the linkage. </span></span>
<span id="cb46-549"><a href="#cb46-549" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-550"><a href="#cb46-550" aria-hidden="true" tabindex="-1"></a>Z <span class="op">=</span> linkage(X, method<span class="op">=</span><span class="st">'ward'</span>) <span class="co"># linkage computed using euclidean distance  </span></span>
<span id="cb46-551"><a href="#cb46-551" aria-hidden="true" tabindex="-1"></a>dend <span class="op">=</span> dendrogram(Z)</span>
<span id="cb46-552"><a href="#cb46-552" aria-hidden="true" tabindex="-1"></a>plt.axhline(y<span class="op">=</span><span class="dv">115</span>, color<span class="op">=</span><span class="st">'r'</span>, linestyle<span class="op">=</span><span class="st">'--'</span>, label<span class="op">=</span><span class="st">'115'</span>)</span>
<span id="cb46-553"><a href="#cb46-553" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb46-554"><a href="#cb46-554" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-555"><a href="#cb46-555" aria-hidden="true" tabindex="-1"></a>From the above dendrogram, we set a threshold at value 115, which cuts through the dendrogram three times. This means we get three clusters.</span>
<span id="cb46-556"><a href="#cb46-556" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-557"><a href="#cb46-557" aria-hidden="true" tabindex="-1"></a><span class="fu">### Hyper-parameter tuning</span></span>
<span id="cb46-558"><a href="#cb46-558" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-561"><a href="#cb46-561" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb46-562"><a href="#cb46-562" aria-hidden="true" tabindex="-1"></a><span class="co"># THIS WILL ITERATE OVER ONE HYPER-PARAMETER (GRID SEARCH) </span></span>
<span id="cb46-563"><a href="#cb46-563" aria-hidden="true" tabindex="-1"></a><span class="co"># AND RETURN THE CLUSTER RESULT THAT OPTIMIZES THE SILHOUETTE SCORE</span></span>
<span id="cb46-564"><a href="#cb46-564" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> maximize_silhouette(X,algo<span class="op">=</span><span class="st">"birch"</span>,nmax<span class="op">=</span><span class="dv">20</span>,i_plot<span class="op">=</span><span class="va">False</span>):</span>
<span id="cb46-565"><a href="#cb46-565" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-566"><a href="#cb46-566" aria-hidden="true" tabindex="-1"></a>    <span class="co"># PARAM</span></span>
<span id="cb46-567"><a href="#cb46-567" aria-hidden="true" tabindex="-1"></a>    i_print<span class="op">=</span><span class="va">False</span></span>
<span id="cb46-568"><a href="#cb46-568" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-569"><a href="#cb46-569" aria-hidden="true" tabindex="-1"></a>    <span class="co">#FORCE CONTIGUOUS</span></span>
<span id="cb46-570"><a href="#cb46-570" aria-hidden="true" tabindex="-1"></a>    X<span class="op">=</span>np.ascontiguousarray(X) </span>
<span id="cb46-571"><a href="#cb46-571" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-572"><a href="#cb46-572" aria-hidden="true" tabindex="-1"></a>    <span class="co"># LOOP OVER HYPER-PARAM</span></span>
<span id="cb46-573"><a href="#cb46-573" aria-hidden="true" tabindex="-1"></a>    params<span class="op">=</span>[]<span class="op">;</span> sil_scores<span class="op">=</span>[]</span>
<span id="cb46-574"><a href="#cb46-574" aria-hidden="true" tabindex="-1"></a>    sil_max<span class="op">=-</span><span class="dv">10</span></span>
<span id="cb46-575"><a href="#cb46-575" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> param <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">2</span>,nmax<span class="op">+</span><span class="dv">1</span>):</span>
<span id="cb46-576"><a href="#cb46-576" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-577"><a href="#cb46-577" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span>(algo<span class="op">==</span><span class="st">"ag"</span>):</span>
<span id="cb46-578"><a href="#cb46-578" aria-hidden="true" tabindex="-1"></a>            model <span class="op">=</span> sklearn.cluster.AgglomerativeClustering(n_clusters<span class="op">=</span>param, affinity<span class="op">=</span><span class="st">"cosine"</span>, linkage<span class="op">=</span><span class="st">'single'</span>).fit(X)</span>
<span id="cb46-579"><a href="#cb46-579" aria-hidden="true" tabindex="-1"></a>            labels<span class="op">=</span>model.labels_</span>
<span id="cb46-580"><a href="#cb46-580" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-581"><a href="#cb46-581" aria-hidden="true" tabindex="-1"></a>        <span class="cf">try</span>:</span>
<span id="cb46-582"><a href="#cb46-582" aria-hidden="true" tabindex="-1"></a>            sil_scores.append(sklearn.metrics.silhouette_score(X,labels))</span>
<span id="cb46-583"><a href="#cb46-583" aria-hidden="true" tabindex="-1"></a>            params.append(param)</span>
<span id="cb46-584"><a href="#cb46-584" aria-hidden="true" tabindex="-1"></a>        <span class="cf">except</span>:</span>
<span id="cb46-585"><a href="#cb46-585" aria-hidden="true" tabindex="-1"></a>            <span class="cf">continue</span> </span>
<span id="cb46-586"><a href="#cb46-586" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-587"><a href="#cb46-587" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span>(i_print): <span class="bu">print</span>(param,sil_scores[<span class="op">-</span><span class="dv">1</span>])</span>
<span id="cb46-588"><a href="#cb46-588" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb46-589"><a href="#cb46-589" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span>(sil_scores[<span class="op">-</span><span class="dv">1</span>]<span class="op">&gt;</span>sil_max):</span>
<span id="cb46-590"><a href="#cb46-590" aria-hidden="true" tabindex="-1"></a>             opt_param<span class="op">=</span>param</span>
<span id="cb46-591"><a href="#cb46-591" aria-hidden="true" tabindex="-1"></a>             sil_max<span class="op">=</span>sil_scores[<span class="op">-</span><span class="dv">1</span>]</span>
<span id="cb46-592"><a href="#cb46-592" aria-hidden="true" tabindex="-1"></a>             opt_labels<span class="op">=</span>labels</span>
<span id="cb46-593"><a href="#cb46-593" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-594"><a href="#cb46-594" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"OPTIMAL PARAMETER ="</span>,opt_param)</span>
<span id="cb46-595"><a href="#cb46-595" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-596"><a href="#cb46-596" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span>(i_plot):</span>
<span id="cb46-597"><a href="#cb46-597" aria-hidden="true" tabindex="-1"></a>        fig, ax <span class="op">=</span> plt.subplots()</span>
<span id="cb46-598"><a href="#cb46-598" aria-hidden="true" tabindex="-1"></a>        ax.plot(params, sil_scores, <span class="st">"-o"</span>)  </span>
<span id="cb46-599"><a href="#cb46-599" aria-hidden="true" tabindex="-1"></a>        ax.<span class="bu">set</span>(xlabel<span class="op">=</span><span class="st">'N_Clusters'</span>, ylabel<span class="op">=</span><span class="st">'Silhouette Score'</span>)</span>
<span id="cb46-600"><a href="#cb46-600" aria-hidden="true" tabindex="-1"></a>        plt.show()</span>
<span id="cb46-601"><a href="#cb46-601" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-602"><a href="#cb46-602" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> opt_labels</span>
<span id="cb46-603"><a href="#cb46-603" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb46-604"><a href="#cb46-604" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-607"><a href="#cb46-607" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb46-608"><a href="#cb46-608" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> plot(X,color_vector):</span>
<span id="cb46-609"><a href="#cb46-609" aria-hidden="true" tabindex="-1"></a>    fig, [ax1, ax2] <span class="op">=</span> plt.subplots(<span class="dv">1</span>,<span class="dv">2</span>, figsize<span class="op">=</span>(<span class="dv">10</span>,<span class="dv">5</span>))</span>
<span id="cb46-610"><a href="#cb46-610" aria-hidden="true" tabindex="-1"></a>    sns.scatterplot(df[<span class="st">"log_funds_returned"</span>], df[<span class="st">"log_funds_lost"</span>], hue<span class="op">=</span>df[<span class="st">"scam_type_grouped"</span>], ax<span class="op">=</span>ax1)</span>
<span id="cb46-611"><a href="#cb46-611" aria-hidden="true" tabindex="-1"></a>    ax1.<span class="bu">set</span>(xlabel<span class="op">=</span><span class="st">'log_funds_returned'</span>, ylabel<span class="op">=</span><span class="st">'log_funds_lost'</span>,</span>
<span id="cb46-612"><a href="#cb46-612" aria-hidden="true" tabindex="-1"></a>    title<span class="op">=</span><span class="st">'Log Funds Clusters By Scam Type (Y)'</span>)</span>
<span id="cb46-613"><a href="#cb46-613" aria-hidden="true" tabindex="-1"></a>    ax1.grid()</span>
<span id="cb46-614"><a href="#cb46-614" aria-hidden="true" tabindex="-1"></a>    scatter2 <span class="op">=</span> ax2.scatter(X[<span class="st">'log_funds_returned'</span>], X[<span class="st">'log_funds_lost'</span>],c<span class="op">=</span>color_vector, alpha<span class="op">=</span><span class="fl">0.5</span>) </span>
<span id="cb46-615"><a href="#cb46-615" aria-hidden="true" tabindex="-1"></a>    ax2.<span class="bu">set</span>(xlabel<span class="op">=</span><span class="st">'log_funds_returned'</span>, ylabel<span class="op">=</span><span class="st">'log_funds_lost'</span>,</span>
<span id="cb46-616"><a href="#cb46-616" aria-hidden="true" tabindex="-1"></a>    title<span class="op">=</span><span class="st">'Agglomerative Clustering Plot'</span>)</span>
<span id="cb46-617"><a href="#cb46-617" aria-hidden="true" tabindex="-1"></a>    legend2 <span class="op">=</span> ax2.legend(<span class="op">*</span>scatter2.legend_elements(),</span>
<span id="cb46-618"><a href="#cb46-618" aria-hidden="true" tabindex="-1"></a>                    loc<span class="op">=</span><span class="st">"lower right"</span>, title<span class="op">=</span><span class="st">"Clusters"</span>)</span>
<span id="cb46-619"><a href="#cb46-619" aria-hidden="true" tabindex="-1"></a>    ax2.add_artist(legend2)</span>
<span id="cb46-620"><a href="#cb46-620" aria-hidden="true" tabindex="-1"></a>    ax2.grid()</span>
<span id="cb46-621"><a href="#cb46-621" aria-hidden="true" tabindex="-1"></a>    plt.show()</span>
<span id="cb46-622"><a href="#cb46-622" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb46-623"><a href="#cb46-623" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-624"><a href="#cb46-624" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Single-linkage Agglomerative Clustering</span></span>
<span id="cb46-625"><a href="#cb46-625" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-628"><a href="#cb46-628" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb46-629"><a href="#cb46-629" aria-hidden="true" tabindex="-1"></a>opt_labels<span class="op">=</span>maximize_silhouette(X,algo<span class="op">=</span><span class="st">"ag"</span>,nmax<span class="op">=</span><span class="dv">15</span>, i_plot<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb46-630"><a href="#cb46-630" aria-hidden="true" tabindex="-1"></a>plot(X,opt_labels)</span>
<span id="cb46-631"><a href="#cb46-631" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb46-632"><a href="#cb46-632" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-633"><a href="#cb46-633" aria-hidden="true" tabindex="-1"></a>Single-linkage uses the minimum of the distances between all observations of the two sets. For the above model, we also chose the affinity hyper-parameter, a metric used to compute the linkages, as cosine and found that the best number of clusters is two, as shown above in the first plot.</span>
<span id="cb46-634"><a href="#cb46-634" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-635"><a href="#cb46-635" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Complete-linkage Agglomerative Clustering</span></span>
<span id="cb46-636"><a href="#cb46-636" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-639"><a href="#cb46-639" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb46-640"><a href="#cb46-640" aria-hidden="true" tabindex="-1"></a><span class="co"># THIS WILL ITERATE OVER ONE HYPER-PARAMETER (GRID SEARCH) </span></span>
<span id="cb46-641"><a href="#cb46-641" aria-hidden="true" tabindex="-1"></a><span class="co"># AND RETURN THE CLUSTER RESULT THAT OPTIMIZES THE SILHOUETTE SCORE</span></span>
<span id="cb46-642"><a href="#cb46-642" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> maximize_silhouette(X,algo<span class="op">=</span><span class="st">"birch"</span>,nmax<span class="op">=</span><span class="dv">20</span>,i_plot<span class="op">=</span><span class="va">False</span>):</span>
<span id="cb46-643"><a href="#cb46-643" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-644"><a href="#cb46-644" aria-hidden="true" tabindex="-1"></a>    <span class="co"># PARAM</span></span>
<span id="cb46-645"><a href="#cb46-645" aria-hidden="true" tabindex="-1"></a>    i_print<span class="op">=</span><span class="va">False</span></span>
<span id="cb46-646"><a href="#cb46-646" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-647"><a href="#cb46-647" aria-hidden="true" tabindex="-1"></a>    <span class="co">#FORCE CONTIGUOUS</span></span>
<span id="cb46-648"><a href="#cb46-648" aria-hidden="true" tabindex="-1"></a>    X<span class="op">=</span>np.ascontiguousarray(X) </span>
<span id="cb46-649"><a href="#cb46-649" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-650"><a href="#cb46-650" aria-hidden="true" tabindex="-1"></a>    <span class="co"># LOOP OVER HYPER-PARAM</span></span>
<span id="cb46-651"><a href="#cb46-651" aria-hidden="true" tabindex="-1"></a>    params<span class="op">=</span>[]<span class="op">;</span> sil_scores<span class="op">=</span>[]</span>
<span id="cb46-652"><a href="#cb46-652" aria-hidden="true" tabindex="-1"></a>    sil_max<span class="op">=-</span><span class="dv">10</span></span>
<span id="cb46-653"><a href="#cb46-653" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> param <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">2</span>,nmax<span class="op">+</span><span class="dv">1</span>):</span>
<span id="cb46-654"><a href="#cb46-654" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-655"><a href="#cb46-655" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span>(algo<span class="op">==</span><span class="st">"ag"</span>):</span>
<span id="cb46-656"><a href="#cb46-656" aria-hidden="true" tabindex="-1"></a>            model <span class="op">=</span> sklearn.cluster.AgglomerativeClustering(n_clusters<span class="op">=</span>param, affinity<span class="op">=</span><span class="st">"manhattan"</span>, linkage<span class="op">=</span><span class="st">'complete'</span>).fit(X)</span>
<span id="cb46-657"><a href="#cb46-657" aria-hidden="true" tabindex="-1"></a>            labels<span class="op">=</span>model.labels_</span>
<span id="cb46-658"><a href="#cb46-658" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-659"><a href="#cb46-659" aria-hidden="true" tabindex="-1"></a>        <span class="cf">try</span>:</span>
<span id="cb46-660"><a href="#cb46-660" aria-hidden="true" tabindex="-1"></a>            sil_scores.append(sklearn.metrics.silhouette_score(X,labels))</span>
<span id="cb46-661"><a href="#cb46-661" aria-hidden="true" tabindex="-1"></a>            params.append(param)</span>
<span id="cb46-662"><a href="#cb46-662" aria-hidden="true" tabindex="-1"></a>        <span class="cf">except</span>:</span>
<span id="cb46-663"><a href="#cb46-663" aria-hidden="true" tabindex="-1"></a>            <span class="cf">continue</span> </span>
<span id="cb46-664"><a href="#cb46-664" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-665"><a href="#cb46-665" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span>(i_print): <span class="bu">print</span>(param,sil_scores[<span class="op">-</span><span class="dv">1</span>])</span>
<span id="cb46-666"><a href="#cb46-666" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb46-667"><a href="#cb46-667" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span>(sil_scores[<span class="op">-</span><span class="dv">1</span>]<span class="op">&gt;</span>sil_max):</span>
<span id="cb46-668"><a href="#cb46-668" aria-hidden="true" tabindex="-1"></a>             opt_param<span class="op">=</span>param</span>
<span id="cb46-669"><a href="#cb46-669" aria-hidden="true" tabindex="-1"></a>             sil_max<span class="op">=</span>sil_scores[<span class="op">-</span><span class="dv">1</span>]</span>
<span id="cb46-670"><a href="#cb46-670" aria-hidden="true" tabindex="-1"></a>             opt_labels<span class="op">=</span>labels</span>
<span id="cb46-671"><a href="#cb46-671" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-672"><a href="#cb46-672" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"OPTIMAL PARAMETER ="</span>,opt_param)</span>
<span id="cb46-673"><a href="#cb46-673" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-674"><a href="#cb46-674" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span>(i_plot):</span>
<span id="cb46-675"><a href="#cb46-675" aria-hidden="true" tabindex="-1"></a>        fig, ax <span class="op">=</span> plt.subplots()</span>
<span id="cb46-676"><a href="#cb46-676" aria-hidden="true" tabindex="-1"></a>        ax.plot(params, sil_scores, <span class="st">"-o"</span>)  </span>
<span id="cb46-677"><a href="#cb46-677" aria-hidden="true" tabindex="-1"></a>        ax.<span class="bu">set</span>(xlabel<span class="op">=</span><span class="st">'N_Clusters'</span>, ylabel<span class="op">=</span><span class="st">'Silhouette Score'</span>)</span>
<span id="cb46-678"><a href="#cb46-678" aria-hidden="true" tabindex="-1"></a>        plt.show()</span>
<span id="cb46-679"><a href="#cb46-679" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-680"><a href="#cb46-680" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> opt_labels</span>
<span id="cb46-681"><a href="#cb46-681" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb46-682"><a href="#cb46-682" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-685"><a href="#cb46-685" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb46-686"><a href="#cb46-686" aria-hidden="true" tabindex="-1"></a>opt_labels<span class="op">=</span>maximize_silhouette(X,algo<span class="op">=</span><span class="st">"ag"</span>,nmax<span class="op">=</span><span class="dv">15</span>, i_plot<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb46-687"><a href="#cb46-687" aria-hidden="true" tabindex="-1"></a>plot(X,opt_labels)</span>
<span id="cb46-688"><a href="#cb46-688" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb46-689"><a href="#cb46-689" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-690"><a href="#cb46-690" aria-hidden="true" tabindex="-1"></a>Complete-linkage uses the maximum distances between all observations of the two sets. For the above model, we also chose the affinity hyper-parameter, a metric used to compute the linkages, as manhattan and found that the best number of clusters is three, as shown above in the first plot. Therefore, given the plots of two different hyper-parameter tuned models, the single-linkage hierarchical clustering model performs better in clustering, as the clustering predictions better coincide with the existing labels (scam_type_grouped) in the data-set.</span>
<span id="cb46-691"><a href="#cb46-691" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-692"><a href="#cb46-692" aria-hidden="true" tabindex="-1"></a><span class="fu">### Final results for Agglomerative Clustering</span></span>
<span id="cb46-693"><a href="#cb46-693" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-696"><a href="#cb46-696" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb46-697"><a href="#cb46-697" aria-hidden="true" tabindex="-1"></a>final_model <span class="op">=</span> AgglomerativeClustering(n_clusters<span class="op">=</span><span class="dv">2</span>, linkage<span class="op">=</span><span class="st">'single'</span>).fit(X)</span>
<span id="cb46-698"><a href="#cb46-698" aria-hidden="true" tabindex="-1"></a>final_labels <span class="op">=</span> final_model.labels_</span>
<span id="cb46-699"><a href="#cb46-699" aria-hidden="true" tabindex="-1"></a>fig, [ax1, ax2] <span class="op">=</span> plt.subplots(<span class="dv">1</span>,<span class="dv">2</span>, figsize<span class="op">=</span>(<span class="dv">10</span>,<span class="dv">5</span>))</span>
<span id="cb46-700"><a href="#cb46-700" aria-hidden="true" tabindex="-1"></a>sns.scatterplot(df[<span class="st">"log_funds_returned"</span>], df[<span class="st">"log_funds_lost"</span>], hue<span class="op">=</span>df[<span class="st">"scam_type_grouped"</span>], ax<span class="op">=</span>ax1)</span>
<span id="cb46-701"><a href="#cb46-701" aria-hidden="true" tabindex="-1"></a>ax1.<span class="bu">set</span>(xlabel<span class="op">=</span><span class="st">'log_funds_returned'</span>, ylabel<span class="op">=</span><span class="st">'log_funds_lost'</span>,</span>
<span id="cb46-702"><a href="#cb46-702" aria-hidden="true" tabindex="-1"></a>title<span class="op">=</span><span class="st">'Log Funds Clusters By Scam Type (Y)'</span>)</span>
<span id="cb46-703"><a href="#cb46-703" aria-hidden="true" tabindex="-1"></a>ax1.grid()</span>
<span id="cb46-704"><a href="#cb46-704" aria-hidden="true" tabindex="-1"></a>scatter2 <span class="op">=</span> ax2.scatter(X[<span class="st">'log_funds_returned'</span>], X[<span class="st">'log_funds_lost'</span>],c<span class="op">=</span>final_labels, alpha<span class="op">=</span><span class="fl">0.5</span>) </span>
<span id="cb46-705"><a href="#cb46-705" aria-hidden="true" tabindex="-1"></a>ax2.<span class="bu">set</span>(xlabel<span class="op">=</span><span class="st">'log_funds_returned'</span>, ylabel<span class="op">=</span><span class="st">'log_funds_lost'</span>,</span>
<span id="cb46-706"><a href="#cb46-706" aria-hidden="true" tabindex="-1"></a>title<span class="op">=</span><span class="st">'Agglomerative Clustering Plot'</span>)</span>
<span id="cb46-707"><a href="#cb46-707" aria-hidden="true" tabindex="-1"></a>legend2 <span class="op">=</span> ax2.legend(<span class="op">*</span>scatter2.legend_elements(),</span>
<span id="cb46-708"><a href="#cb46-708" aria-hidden="true" tabindex="-1"></a>                loc<span class="op">=</span><span class="st">"lower right"</span>, title<span class="op">=</span><span class="st">"Clusters"</span>)</span>
<span id="cb46-709"><a href="#cb46-709" aria-hidden="true" tabindex="-1"></a>ax2.add_artist(legend2)</span>
<span id="cb46-710"><a href="#cb46-710" aria-hidden="true" tabindex="-1"></a>ax2.grid()</span>
<span id="cb46-711"><a href="#cb46-711" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb46-712"><a href="#cb46-712" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb46-713"><a href="#cb46-713" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-714"><a href="#cb46-714" aria-hidden="true" tabindex="-1"></a><span class="fu"># Results</span></span>
<span id="cb46-715"><a href="#cb46-715" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-716"><a href="#cb46-716" aria-hidden="true" tabindex="-1"></a>We performed clustering using three different models, including K-Means, DBSCAN, and Agglomerative Clustering. The final result of the K-Means model was based off on the outputs of both the silhouette score and the elbow method. The elbow method conveyed that k=4 clusters would be optimum, but the silhouette score for k=4 was 0.35, lower than that of k=5 that had a silhouette score of approximately 0.53. </span>
<span id="cb46-717"><a href="#cb46-717" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-718"><a href="#cb46-718" aria-hidden="true" tabindex="-1"></a>In fact, the final model for K-Means outputs the best clusters out of all other models in relation to the scam_type_grouped (Y) labels in the data set. The labels and clusters coincide the best out of all other models, as one cluster is dedicated to those observations that have a positive log_funds_returned and so are the Exploit scam type labels. Moreover, only K-Means clusters depict the behavior of correctly clustering those points that have 0 log_funds_returned but positive log_funds_lost into two clusters. If we look at the scatterplot for scam_type_grouped labels, we see that Exit Scams have lower log_funds_lost (green cluster) and Exploit scams have higher log_funds_lost (orange cluster) when log_funds_returned is 0. This characteristic is best captured by the final K-Means model, as label 2 (deep purple) clusters are similar to the Exit Scams (green cluster) and label 0 (pink) clusters are similar to the Exploit scams (orange cluster) when log_funds_returned is 0. </span>
<span id="cb46-719"><a href="#cb46-719" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-720"><a href="#cb46-720" aria-hidden="true" tabindex="-1"></a>The DBSCAN model had the best improvement out of all models from its initial model to the final model, which did output three clusters, like K-Means did, but did not capture the relation of the scam_type_grouped (Y) labels. This could be because when one looks at the scatterplot of log_funds_returned vs log_funds_lost, he or she can conclude with the naked eye that there exist two clusters only, points where log_funds_returned is 0 and points where log_funds_returned and log_funds_lost are both positive. Therefore, because these clusters seem linearly separable, they can easily be clustered with a simpler algorithm, like K-Means. Moreover, our log transformed numeric features had almost no outliers present due to the transformation, but DBSCAN detected some noise points around the cluster that had positive values for log_funds_returned and log_funds_lost. Therefore, if our data was more complex, perhaps with non-globular clusters and clusters of different sizes, DBSCAN could have performed better than K-Means.</span>
<span id="cb46-721"><a href="#cb46-721" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-722"><a href="#cb46-722" aria-hidden="true" tabindex="-1"></a>Finally, Agglomerative/Hierarchical clustering did a better job of clustering the feature data (X) than DBSCAN but not as good as K-Means. The initial model was powerful because it outputted three clusters and, upon hyper-parameter tuning, the final model was as good as the initial model. The reason why complete-linkage hyper-parameter model was not chosen as the final model was because it contained overlapping clusters for points that had positive log_funds_returned and log_funds_lost values. The single-linkage model, on the other hand, separated the positive log_funds_returned points from the points that had log_funds_returned = 0, leading to two distinct clusters. However, unlike K-Means, the single-linkage model could not capture cluster associations for points that had log_funds_returned = 0. </span>
<span id="cb46-723"><a href="#cb46-723" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-724"><a href="#cb46-724" aria-hidden="true" tabindex="-1"></a><span class="fu"># Conclusions</span></span>
<span id="cb46-725"><a href="#cb46-725" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-726"><a href="#cb46-726" aria-hidden="true" tabindex="-1"></a>To wrap up, we found that the K-Means model clustered the feature dataset (X) most aptly in relation to the target variable (Y), scam_type_grouped, of the dataset. DBSCAN performed the worst and Hierarchical clustering performed second best. The primary finding from our results was that the right clustering model can help extract the ground truths (or labels) from only the feature dataset. Therefore, although simple and easy to execute, clustering models can be highly powerful in generating accurate insights of the overall data from independent features only. It would have helped my modeling process if I had more numeric features attached to the dataset, which would have probably led to not only more clusters being formed but also clusters of different shapes and sizes/densities. For example, an additional numeric feature not related to funds would have added to the complexity of the dataset and, hence, comparing the clusters with those of the labelled (Y) clusters may have yielded different results. Moreover, we could have visualized the data in 3-D, using the three numeric features, and, as a result, added a new perspective to the dataset.</span>
<span id="cb46-727"><a href="#cb46-727" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-728"><a href="#cb46-728" aria-hidden="true" tabindex="-1"></a><span class="fu"># References</span></span>
<span id="cb46-729"><a href="#cb46-729" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-730"><a href="#cb46-730" aria-hidden="true" tabindex="-1"></a>Santini, Marina. “Advantages*&amp;amp;*Disadvantages*of** k:Means*and*Hierarchical ... - Santini.” Accessed November 8, 2022. http://santini.se/teaching/ml/2016/Lect_10/10c_UnsupervisedMethods.pdf. </span>
<span id="cb46-731"><a href="#cb46-731" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-732"><a href="#cb46-732" aria-hidden="true" tabindex="-1"></a>“Clustering Analysis - Computer Science | Western Michigan University.” Accessed November 10, 2022. https://cs.wmich.edu/alfuqaha/summer14/cs6530/lectures/ClusteringAnalysis.pdf. </span>
<span id="cb46-733"><a href="#cb46-733" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-734"><a href="#cb46-734" aria-hidden="true" tabindex="-1"></a>Ajitesh  Kumar. “Elbow Method vs Silhouette Score - Which Is Better?” Data Analytics, November 28, 2021. https://vitalflux.com/elbow-method-silhouette-score-which-better/#:~:text=The%20elbow%20method%20is%20used,cluster%20or%20across%20different%20clusters. </span>
<span id="cb46-735"><a href="#cb46-735" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-736"><a href="#cb46-736" aria-hidden="true" tabindex="-1"></a>Bock, Tim. “What Is a Dendrogram?” Displayr, September 13, 2022. https://www.displayr.com/what-is-dendrogram/. </span>
<span id="cb46-737"><a href="#cb46-737" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-738"><a href="#cb46-738" aria-hidden="true" tabindex="-1"></a>“Demo of DBSCAN Clustering Algorithm.” scikit. Accessed November 9, 2022. https://scikit-learn.org/stable/auto_examples/cluster/plot_dbscan.html. </span>
<span id="cb46-739"><a href="#cb46-739" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-740"><a href="#cb46-740" aria-hidden="true" tabindex="-1"></a>Hashmi, Farukh. “How to Create Clusters Using DBSCAN in Python.” Thinking Neuron, November 27, 2021. https://thinkingneuron.com/how-to-create-clusters-using-dbscan-in-python/. </span>
</code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div></div></div></div></div>
</div> <!-- /content -->



</body></html>