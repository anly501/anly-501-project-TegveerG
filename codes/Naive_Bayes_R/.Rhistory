# Moreover, no key information, in terms of funds lost or source of the attack, offered about these attacks. We are left with all those rows where funds lost != 0 and where all projects have some information to offer regarding the extent of the scam, helpful in predicting the scam type.
#df2 <- df2 %>% filter(description!="The contract owner could disable the transfer function, which restricted users in selling their tokens." & funds_lost!=0)
#df2 <- df2 %>% filter(description!="A review of the token contract shows low network activity, which didn't lead to significant funds loss for the user in the past. The project is considered abandoned." & funds_lost!=0)
df2 <- df2 %>% filter(funds_lost!=0)
xtabs(~scam_type, data = df2)
# Removing dictionary values from the source column
df2$log_funds_lost <- log(df2$funds_lost)
df2$log_funds_returned <- log(df2$funds_returned + 1) # add +1 because we have zeros in funds_returned and helps avoid negative inf values
# Removing dictionary values from the source column
data_nb <- subset(df2, select = c(project_name, log_funds_lost, log_funds_returned, scamNetworks, month_of_attack, day_of_week_of_attack, day_of_year_of_attack, scam_type))
data_nb %>% group_by(scam_type) %>% summarise(n())
# pooling together scam types into respective types as described above
data_nb <- data_nb %>%
mutate(scam_type_grouped = if_else(scam_type=="Honeypot" | scam_type=="Rugpull" | scam_type=="Abandoned" | project_name=="Kronos Dao", "Exit Scam", "Exploit"))
data_nb <- subset(data_nb, select = -c(project_name, scam_type))
data_nb %>% group_by(scam_type_grouped) %>% summarise(n())
data_nb$log_funds_lost <- as.double(data_nb$log_funds_lost)
data_nb$log_funds_returned <-as.double(data_nb$log_funds_returned)
data_nb$scamNetworks <- as.factor(data_nb$scamNetworks)
data_nb$month_of_attack <- as.factor(data_nb$month_of_attack)
data_nb$day_of_week_of_attack <-as.factor(data_nb$day_of_week_of_attack)
data_nb$day_of_year_of_attack <- as.integer(data_nb$day_of_year_of_attack)
data_nb$scam_type_grouped <-as.factor(data_nb$scam_type_grouped)
data_nb <- data_nb[sample(1:nrow(data_nb)), ] # shuffle rows
set.seed(1234)
ind <- sample(2, nrow(data_nb), replace = T, prob = c(0.8, 0.2))
train <- data_nb[ind == 1,]
test <- data_nb[ind == 2,]
model <- naive_bayes(scam_type_grouped ~ ., data = train)
plot(model)
p <- predict(model, train, type = 'prob')
head(cbind(p, train))
#credit: https://stackoverflow.com/questions/46063234/how-to-produce-a-confusion-matrix-and-find-the-misclassification-rate-of-the-na%C3%AF
library(scales)
ggplotConfusionMatrix <- function(m){
mytitle <- paste("Accuracy", percent_format()(m$overall[1]))
p <-
ggplot(data = as.data.frame(m$table) ,
aes(x = Prediction, y = Reference)) +
geom_tile(aes(fill = log(Freq)),
colour = "white", show.legend = FALSE) +
scale_fill_gradient(low = "white", high = "steelblue") +
geom_text(aes(x = Prediction, y = Reference,
label = Freq, show.legend = FALSE)) +
ggtitle(mytitle) +
scale_x_discrete(limits = rev) +
theme_minimal()
return(p)
}
train_preds <- predict(model, train)
#(tab1 <- table(train_preds, train$scam_type_grouped))
cfm_train <- confusionMatrix(train_preds, train$scam_type_grouped)
ggplotConfusionMatrix(cfm_train)
cfm_train
test_preds <- predict(model, test)
#(tab2 <- table(test_preds, test$scam_type_grouped))
cfm_test <- confusionMatrix(test_preds, test$scam_type_grouped)
ggplotConfusionMatrix(cfm_test)
cfm_test
library(arules)
set.seed(2838)
x = subset(data_nb, select = -c(scam_type_grouped))
Scam_Type = as.factor(data_nb$scam_type_grouped)
Disc_log_funds_lost = discretize(x$log_funds_lost, method = "cluster", breaks = 3)
Disc_log_funds_returned = discretize(x$log_funds_returned, method = "cluster", breaks = 3)
Disc_day_of_year_of_attack = discretize(x$day_of_year_of_attack, method = "cluster", breaks = 3)
Ddf = data.frame(Disc_log_funds_lost, Disc_log_funds_returned, x$scamNetworks, x$month_of_attack, x$day_of_week_of_attack, Disc_day_of_year_of_attack, Scam_Type)
ind <- sample(2, nrow(Ddf), replace = T, prob = c(0.8, 0.2))
train_discretized <- Ddf[ind == 1,]
test_discretized <- Ddf[ind == 2,]
discretized_NB_model <- NaiveBayes(Scam_Type ~ ., data=train_discretized)
cfm_discretized_train <- confusionMatrix(predict(discretized_NB_model, train_discretized)$class, train_discretized$Scam_Type)
ggplotConfusionMatrix(cfm_discretized_train)
cfm_discretized_train
cfm_discretized_test <- confusionMatrix(predict(discretized_NB_model, test_discretized)$class, test_discretized$Scam_Type)
ggplotConfusionMatrix(cfm_discretized_test)
cfm_discretized_train
### Load libraries here
library("selectr")
library("rvest")
library("xml2")
library(rtweet) # for scraping tweets
library(wordcloud2) # for generating really cool looking wordclouds
library(tm) # for text mining
library(tidyverse)
library(twitteR)
library(ROAuth)
library(jsonlite)
consumerKey = '05RsErguXYWBfMpmmZdZlbV6P'
consumerSecret = 'Vr50qRwylJGRr1TtANB6RalqT90dTuq8hlBvoT8WCnYTS5Ni70'
access_Token = '1512312564-pbSid3qWEdigyAalZcII1d2ZF4rQ2WkiYfHf60k'
access_Secret = 'O9XEwsekAJLKkn0HxZ0VhOE7sIJ9VQawNgde4hXhkE0rb'
### Properly parse our API keys
consumerKey=as.character(consumerKey)
consumerSecret=as.character(consumerSecret)
access_Token=as.character(access_Token)
access_Secret=as.character(access_Secret)
##################################################################
### Ensure the right URLs are stored to pull data
requestURL='https://api.twitter.com/oauth/request_token'
accessURL='https://api.twitter.com/oauth/access_token'
authURL='https://api.twitter.com/oauth/authorize'
# setup OAuth and scrape 1,000 tweets relating to crypto and crime
setup_twitter_oauth(consumerKey,consumerSecret,access_Token,access_Secret)
Search1<-twitteR::searchTwitter(searchString="crypto + crime", n=1000, lang = "en", resultType="mixed") # mixed results includes popular and real time results
Raw_Tweets_DF <- twListToDF(Search1)
########## Place Raw Tweets DF in a new file ###################
write.csv(df,"../../data/Raw Data/R_Twitter_API/Crypto_Tweets_R_raw.csv", row.names = FALSE)
########## Place only Raw Tweets Text in a new file ###################
FName1 = "../../data/Raw Data/R_Twitter_API/Crypto_Tweets_R_raw_text.txt"
## Start the file
MyFile1 <- file(FName1)
## Write Tweets to file
cat(unlist(Raw_Tweets_DF$Text), " ", file=MyFile1, sep="\n")
close(MyFile1)
#Raw_Tweets_DF<- twListToDF(Search1)
# cleaning raw tweets dataframe
#Raw_Tweets_DF$text <- gsub("[^[:alnum:][:blank:]?&/\\-]", "",Raw_Tweets_DF$text) # remove alphanumeric characters
#Raw_Tweets_DF$text <- gsub("https\\S*", "",Raw_Tweets_DF$text) # remove hyperlinks
#Raw_Tweets_DF$text <- gsub("&amp", "and",Raw_Tweets_DF$text) # replace ampersand symbol with "and"
# setup OAuth and scrape 1,000 tweets relating to crypto and crime
setup_twitter_oauth(consumerKey,consumerSecret,access_Token,access_Secret)
Search1<-twitteR::searchTwitter(searchString="crypto + crime", n=1000, lang = "en", resultType="mixed") # mixed results includes popular and real time results
Raw_Tweets_DF <- twListToDF(Search1)
########## Place Raw Tweets DF in a new file ###################
write.csv(Raw_Tweets_DF,"../../data/Raw Data/R_Twitter_API/Crypto_Tweets_R_raw.csv", row.names = FALSE)
########## Place only Raw Tweets Text in a new file ###################
FName1 = "../../data/Raw Data/R_Twitter_API/Crypto_Tweets_R_raw_text.txt"
## Start the file
MyFile1 <- file(FName1)
## Write Tweets to file
cat(unlist(Raw_Tweets_DF$Text), " ", file=MyFile1, sep="\n")
close(MyFile1)
#Raw_Tweets_DF<- twListToDF(Search1)
# cleaning raw tweets dataframe
#Raw_Tweets_DF$text <- gsub("[^[:alnum:][:blank:]?&/\\-]", "",Raw_Tweets_DF$text) # remove alphanumeric characters
#Raw_Tweets_DF$text <- gsub("https\\S*", "",Raw_Tweets_DF$text) # remove hyperlinks
#Raw_Tweets_DF$text <- gsub("&amp", "and",Raw_Tweets_DF$text) # replace ampersand symbol with "and"
Raw_Tweets_DF
nrow(Raw_Tweets_DF)
library(tidyverse)
X <- runif(10000, 0, 1)
Y <- runif(10000, 0, 1)
XY = X + Y
plot(X,Y, pch = 46)
N = 10000  # number of initial simulation
set.seed(101)
mydf.0 <- data.frame(X1 = runif(N), X2 = runif(N))
mydf.0 <- mydf.0[rowSums(mydf.0) < 1,]
plot(mydf.0, pch = 46)
N = 10000  # number of initial simulation
set.seed(101)
mydf.0 <- data.frame(X = runif(N), Y = runif(N))
mydf.0 <- mydf.0[rowSums(mydf.0) < 1,]
plot(mydf.0, pch = 46)
hist(mydf.0$X, prob = T)
abline(a = 2, b = -2, col = 2)
hist(mydf.0$X, prob = T)
#abline(a = 2, b = -2, col = 2)
hist(mydf.0$X, prob = T)
abline(a = 1, b = -1, col = 2)
hist(mydf.0$X, prob = T)
#abline(a = 1, b = -1, col = 2)
hist(mydf.0$X, prob = T)
abline(a = 2, b = -1, col = 2)
hist(mydf.0$X, prob = T)
abline(a = 2, b = -2, col = 2)
hist(mydf.0$X)
abline(a = 2, b = -2, col = 2)
library(tidyverse)
X <- runif(10000, 0, 1)
Y <- runif(10000, 0, 1)
plot(X,Y, pch = 46) # plot X & Y
N = 10000  # number of initial simulation
set.seed(101)
mydf.0 <- data.frame(X = runif(N), Y = runif(N))
mydf.0 <- mydf.0[rowSums(mydf.0) < 1,]
plot(mydf.0, pch = 46)
hist(mydf.0$X, prob = T)
abline(a = 2, b = -2, col = 2)
mydf.0 <- data.frame(X = runif(N), Y = runif(N))
hist(mydf.0$X, prob = T)
abline(a = 2, b = -2, col = 2)
mydf.0 <- data.frame(X = runif(N), Y = runif(N))
hist(mydf.0$X, prob = T)
abline(a = 1, col = 2)
mydf.0 <- data.frame(X = runif(N), Y = runif(N))
hist(mydf.0$X, prob = T)
abline(a = 1, b=0, col = 2)
hist(mydf.0$Y, prob = T)
#abline(a = 1, b = -1, col = 2)
hist(mydf.0$Y, prob = T)
abline(a = 1, b = 0, col = 2)
install.packages("dt")
install.packages("DT")
library(DT)
library(tidyverse)
tbl <-
tibble::tribble(
~Links,
'<a href="https://github.com/anly501/anly-501-project-TegveerG/tree/main/codes/Naive_Bayes_py">Python Code</a>',
'<a href="https://github.com/anly501/anly-501-project-TegveerG/tree/main/codes/Naive_Bayes_R">R Code</a>')
tbl %>% DT::datatable(escape = FALSE)
View(tbl)
install.packages("gt")
library(naivebayes)
library(klaR)
library(e1071)
library(caTools)
library(caret)
library(tidyverse)
library(naivebayes)
library(klaR)
library(e1071)
library(caTools)
library(caret)
library(tidyverse)
df <- read_csv('../../data/Clean Data/REKT_Database_Clean_Python.csv')
df <- subset(df, select = -c(...1, token_addresses, network))
# Removing dictionary values from the scam_type column
df$scam_type <- gsub("[^:]*,[^:]*", "",df$scam_type)
df$scam_type <- gsub("'id'::", "",df$scam_type)
df$scam_type <- gsub("\\{|\\}", "",df$scam_type)
df$scam_type <- gsub("'", "",df$scam_type)
df$scam_type <- gsub("type: ", "",df$scam_type)
df$scam_type <- gsub(" ", "",df$scam_type)
# Removing list brackets from the scamNetworks column
df$scamNetworks <- gsub("\\[|\\]", "", df$scamNetworks)
df$scamNetworks <- gsub("'", '', df$scamNetworks)
# imputing the missing values as "Other"
df$scamNetworks[df$scamNetworks==""] <- "Other"
# Split df by scamNetworks feature that has multiple categorical values for respective rows
library(splitstackshape)
df2 <- concat.split.multiple(data = df, split.cols = "scamNetworks", direction = "long")
# Funds returned NA's to 0, funds lost NAs drop
#Let’s calculate the frequency of response variable to see if it is imbalanced. The minimum frequency of #each class is 5 required for analysis.
as.data.frame(xtabs(~scam_type, data = df2))
dmultinom(x=c(1,2,0), prob = c(0.6,0.2,0.2))
prob=c(0.6,0.2,0.2)
y=rmultinom(10000,3,prob)
y=data.frame(t(y))
y1=y[y$X1==1 & y$X2==2,]
Prob=length(y1$X1)/length(y$X1)
hist(y$X1)
Prob
setwd("~/anly-501-project-TegveerG/codes/Naive_Bayes_R")
library(naivebayes)
library(klaR)
library(e1071)
library(caTools)
library(caret)
library(tidyverse)
df <- read_csv('../../data/Clean Data/REKT_Database_Clean_Python.csv')
df <- subset(df, select = -c(...1, token_addresses, network))
# Removing dictionary values from the scam_type column
df$scam_type <- gsub("[^:]*,[^:]*", "",df$scam_type)
df$scam_type <- gsub("'id'::", "",df$scam_type)
df$scam_type <- gsub("\\{|\\}", "",df$scam_type)
df$scam_type <- gsub("'", "",df$scam_type)
df$scam_type <- gsub("type: ", "",df$scam_type)
df$scam_type <- gsub(" ", "",df$scam_type)
# Removing list brackets from the scamNetworks column
df$scamNetworks <- gsub("\\[|\\]", "", df$scamNetworks)
df$scamNetworks <- gsub("'", '', df$scamNetworks)
# imputing the missing values as "Other"
df$scamNetworks[df$scamNetworks==""] <- "Other"
# Split df by scamNetworks feature that has multiple categorical values for respective rows
library(splitstackshape)
df2 <- concat.split.multiple(data = df, split.cols = "scamNetworks", direction = "long")
# Funds returned NA's to 0, funds lost NAs drop
#Let’s calculate the frequency of response variable to see if it is imbalanced. The minimum frequency of #each class is 5 required for analysis.
xtabs(~scam_type, data = df2)
nrow(df2 %>% filter(funds_lost==0))
nrow(df2 %>% filter(funds_returned==0))
# if funds lost is $0, that means the respective observation or record has not much information to offer to us in terms of a Naive Bayes Model. Also, if no funds were lost then no funds can be returned/recovered. Therefore, filtering out the 0 funds_lost observations makes sense if we want balanced classes for our predictor, scam type.
# ~1861 honeypots, 383 rugpulls, and 29 "other" scams filtered out in the process of treating class imbalance
# Moreover, no key information, in terms of funds lost or source of the attack, offered about these attacks. We are left with all those rows where funds lost != 0 and where all projects have some information to offer regarding the extent of the scam, helpful in predicting the scam type.
#df2 <- df2 %>% filter(description!="The contract owner could disable the transfer function, which restricted users in selling their tokens." & funds_lost!=0)
#df2 <- df2 %>% filter(description!="A review of the token contract shows low network activity, which didn't lead to significant funds loss for the user in the past. The project is considered abandoned." & funds_lost!=0)
df2 <- df2 %>% filter(funds_lost!=0)
xtabs(~scam_type, data = df2)
# Removing dictionary values from the source column
df2$log_funds_lost <- log(df2$funds_lost)
df2$log_funds_returned <- log(df2$funds_returned + 1) # add +1 because we have zeros in funds_returned and helps avoid negative inf values
# Removing dictionary values from the source column
df2$log_funds_lost <- log(df2$funds_lost)
df2$log_funds_returned <- log(df2$funds_returned + 1) # add +1 because we have zeros in funds_returned and helps avoid negative inf values
# Removing dictionary values from the source column
data_nb <- subset(df2, select = c(project_name, log_funds_lost, log_funds_returned, scamNetworks, month_of_attack, day_of_week_of_attack, day_of_year_of_attack, scam_type))
data_nb %>% group_by(scam_type) %>% summarise(n())
# pooling together scam types into respective types as described above
data_nb <- data_nb %>%
mutate(scam_type_grouped = if_else(scam_type=="Honeypot" | scam_type=="Rugpull" | scam_type=="Abandoned" | project_name=="Kronos Dao", "Exit Scam", "Exploit"))
data_nb <- subset(data_nb, select = -c(project_name, scam_type))
data_nb %>% group_by(scam_type_grouped) %>% summarise(n())
data_nb$log_funds_lost <- as.double(data_nb$log_funds_lost)
data_nb$log_funds_returned <-as.double(data_nb$log_funds_returned)
data_nb$scamNetworks <- as.factor(data_nb$scamNetworks)
data_nb$month_of_attack <- as.factor(data_nb$month_of_attack)
data_nb$day_of_week_of_attack <-as.factor(data_nb$day_of_week_of_attack)
data_nb$day_of_year_of_attack <- as.integer(data_nb$day_of_year_of_attack)
data_nb$scam_type_grouped <-as.factor(data_nb$scam_type_grouped)
data_nb <- data_nb[sample(1:nrow(data_nb)), ] # shuffle rows
# Exporting the pre-processed dataframe into a CSV file for comparing other classifiers
write.csv(data_nb, "../../data/Clean Data/REKT_Database_Clean_Classification")
# Exporting the pre-processed dataframe into a CSV file for comparing other classifiers
write.csv(data_nb, "../../data/Clean Data/REKT_Database_Clean_Classification.csv")
nrow(df)
library(gt)
library(tidyverse)
df <- tibble(
Codes_and_Data = c("Naive Bayes Python Code", "Clean Tweets", "Clean News", "Naive Bayes R Code", "Clean Record Data"),
link = c("https://github.com/anly501/anly-501-project-TegveerG/tree/main/codes/Naive_Bayes_py", "https://github.com/anly501/anly-501-project-TegveerG/blob/main/data/Clean%20Data/Clean_TokeLemm_Crypto_Tweets.csv", "https://github.com/anly501/anly-501-project-TegveerG/blob/main/data/Clean%20Data/Clean_TokeLemm_News.csv", "https://github.com/anly501/anly-501-project-TegveerG/tree/main/codes/Naive_Bayes_R", "https://github.com/anly501/anly-501-project-TegveerG/blob/main/data/Clean%20Data/REKT_Database_Clean_Python.csv"))
# using html
df %>%
mutate(
Link = glue::glue("[Github]({link})"),
Link = map(Link, gt::md)) %>% select(Codes_and_Data, Link) %>%
gt()
library(gt)
library(tidyverse)
df <- tibble(
Codes_and_Data = c("Naive Bayes Python Code", "Clean Tweets", "Clean News", "Naive Bayes R Code", "Clean Record Data"),
link = c("https://github.com/anly501/anly-501-project-TegveerG/tree/main/codes/Naive_Bayes_py", "https://github.com/anly501/anly-501-project-TegveerG/blob/main/data/Clean%20Data/Clean_TokeLemm_Crypto_Tweets.csv", "https://github.com/anly501/anly-501-project-TegveerG/blob/main/data/Clean%20Data/Clean_TokeLemm_News.csv", "https://github.com/anly501/anly-501-project-TegveerG/tree/main/codes/Naive_Bayes_R", "https://github.com/anly501/anly-501-project-TegveerG/blob/main/data/Clean%20Data/REKT_Database_Clean_Python.csv"))
library(gt)
# using html
df %>%
mutate(
Link = glue::glue("[Github]({link})"),
Link = map(Link, gt::md)) %>% select(Codes_and_Data, Link) %>%
gt()
library(gt)
library(tidyverse)
df <- tibble(
Codes_and_Data = c("Naive Bayes Python Code", "Clean Tweets", "Clean News", "Naive Bayes R Code", "Clean Record Data"),
link = c("https://github.com/anly501/anly-501-project-TegveerG/tree/main/codes/Naive_Bayes_py", "https://github.com/anly501/anly-501-project-TegveerG/blob/main/data/Clean%20Data/Clean_TokeLemm_Crypto_Tweets.csv", "https://github.com/anly501/anly-501-project-TegveerG/blob/main/data/Clean%20Data/Clean_TokeLemm_News.csv", "https://github.com/anly501/anly-501-project-TegveerG/tree/main/codes/Naive_Bayes_R", "https://github.com/anly501/anly-501-project-TegveerG/blob/main/data/Clean%20Data/REKT_Database_Clean_Python.csv"))
# using html
df %>%
mutate(Link = glue::glue("[Github]({link})"),
Link = map(Link, gt::md)) %>%
select(Codes_and_Data, Link) %>%
gt()
library(gt)
library(tidyverse)
df <- tibble(
Codes_and_Data = c("Naive Bayes Python Code", "Clean Tweets", "Clean News", "Naive Bayes R Code", "Clean Record Data"),
link = c("https://github.com/anly501/anly-501-project-TegveerG/tree/main/codes/Naive_Bayes_py", "https://github.com/anly501/anly-501-project-TegveerG/blob/main/data/Clean%20Data/Clean_TokeLemm_Crypto_Tweets.csv", "https://github.com/anly501/anly-501-project-TegveerG/blob/main/data/Clean%20Data/Clean_TokeLemm_News.csv", "https://github.com/anly501/anly-501-project-TegveerG/tree/main/codes/Naive_Bayes_R", "https://github.com/anly501/anly-501-project-TegveerG/blob/main/data/Clean%20Data/REKT_Database_Clean_Python.csv"))
# using html
df %>%
mutate(Link = glue::glue("[Github]({link})"),
Link = map(Link, gt::md)) %>%
select(Codes_and_Data, Link) %>%
gt()
library(gt)
library(tidyverse)
df <- tibble(
Codes_and_Data = c("Naive Bayes Python Code", "Clean Tweets", "Clean News", "Naive Bayes R Code", "Clean Record Data"),
link = c("https://github.com/anly501/anly-501-project-TegveerG/tree/main/codes/Naive_Bayes_py", "https://github.com/anly501/anly-501-project-TegveerG/blob/main/data/Clean%20Data/Clean_TokeLemm_Crypto_Tweets.csv", "https://github.com/anly501/anly-501-project-TegveerG/blob/main/data/Clean%20Data/Clean_TokeLemm_News.csv", "https://github.com/anly501/anly-501-project-TegveerG/tree/main/codes/Naive_Bayes_R", "https://github.com/anly501/anly-501-project-TegveerG/blob/main/data/Clean%20Data/REKT_Database_Clean_Python.csv"))
# using html
df %>%
mutate(Link = glue::glue("[Github]({link})"),
Link = map(Link, gt::md)) %>% select(Codes_and_Data, Link) %>%
gt()
library(naivebayes)
library(klaR)
library(e1071)
library(caTools)
library(caret)
library(tidyverse)
df <- read_csv('../../data/Clean Data/REKT_Database_Clean_Python.csv')
df <- subset(df, select = -c(...1, token_addresses, network))
# Removing dictionary values from the scam_type column
df$scam_type <- gsub("[^:]*,[^:]*", "",df$scam_type)
df$scam_type <- gsub("'id'::", "",df$scam_type)
df$scam_type <- gsub("\\{|\\}", "",df$scam_type)
df$scam_type <- gsub("'", "",df$scam_type)
df$scam_type <- gsub("type: ", "",df$scam_type)
df$scam_type <- gsub(" ", "",df$scam_type)
# Removing list brackets from the scamNetworks column
df$scamNetworks <- gsub("\\[|\\]", "", df$scamNetworks)
df$scamNetworks <- gsub("'", '', df$scamNetworks)
# imputing the missing values as "Other"
df$scamNetworks[df$scamNetworks==""] <- "Other"
# Split df by scamNetworks feature that has multiple categorical values for respective rows
library(splitstackshape)
df2 <- concat.split.multiple(data = df, split.cols = "scamNetworks", direction = "long")
# Funds returned NA's to 0, funds lost NAs drop
#Let’s calculate the frequency of response variable to see if it is imbalanced. The minimum frequency of #each class is 5 required for analysis.
as.data.frame(xtabs(~scam_type, data = df2))
nrow(df2 %>% filter(funds_lost==0))
nrow(df2 %>% filter(funds_returned==0))
df2 <- df2 %>% filter(funds_lost!=0)
as.data.frame(xtabs(~scam_type, data = df2))
# Removing dictionary values from the source column
df2$log_funds_lost <- log(df2$funds_lost)
df2$log_funds_returned <- log(df2$funds_returned + 1) # add +1 because we have zeros in funds_returned and helps avoid negative inf values
# Removing dictionary values from the source column
data_nb <- subset(df2, select = c(project_name, log_funds_lost, log_funds_returned, scamNetworks, month_of_attack, day_of_week_of_attack, day_of_year_of_attack, scam_type))
data_nb %>% group_by(scam_type) %>% summarise(num_occurences=n())
# pooling together scam types into respective types as described above
data_nb <- data_nb %>%
mutate(scam_type_grouped = if_else(scam_type=="Honeypot" | scam_type=="Rugpull" | scam_type=="Abandoned" | project_name=="Kronos Dao", "Exit Scam", "Exploit"))
data_nb <- subset(data_nb, select = -c(project_name, scam_type))
data_nb %>% group_by(scam_type_grouped) %>% summarise(num_occurences=n())
data_nb$log_funds_lost <- as.double(data_nb$log_funds_lost)
data_nb$log_funds_returned <-as.double(data_nb$log_funds_returned)
data_nb$scamNetworks <- as.factor(data_nb$scamNetworks)
data_nb$month_of_attack <- as.factor(data_nb$month_of_attack)
data_nb$day_of_week_of_attack <-as.factor(data_nb$day_of_week_of_attack)
data_nb$day_of_year_of_attack <- as.integer(data_nb$day_of_year_of_attack)
data_nb$scam_type_grouped <-as.factor(data_nb$scam_type_grouped)
data_nb <- data_nb[sample(1:nrow(data_nb)), ] # shuffle rows
set.seed(1234)
ind <- sample(2, nrow(data_nb), replace = T, prob = c(0.8, 0.2))
train <- data_nb[ind == 1,]
test <- data_nb[ind == 2,]
model <- naive_bayes(scam_type_grouped ~ ., data = train)
plot(model)
p <- predict(model, train, type = 'prob')
head(cbind(p, train))
#credit: https://stackoverflow.com/questions/46063234/how-to-produce-a-confusion-matrix-and-find-the-misclassification-rate-of-the-na%C3%AF
library(scales)
ggplotConfusionMatrix <- function(m){
mytitle <- paste("Accuracy", percent_format()(m$overall[1]))
p <-
ggplot(data = as.data.frame(m$table) ,
aes(x = Prediction, y = Reference)) +
geom_tile(aes(fill = log(Freq)),
colour = "white", show.legend = FALSE) +
scale_fill_gradient(low = "white", high = "steelblue") +
geom_text(aes(x = Prediction, y = Reference,
label = Freq, show.legend = FALSE)) +
ggtitle(mytitle) +
scale_x_discrete(limits = rev) +
theme_minimal()
return(p)
}
set.seed(4938)
train_preds <- predict(model, train)
#(tab1 <- table(train_preds, train$scam_type_grouped))
cfm_train <- confusionMatrix(train_preds, train$scam_type_grouped)
ggplotConfusionMatrix(cfm_train)
cfm_train
test_preds <- predict(model, test)
#(tab2 <- table(test_preds, test$scam_type_grouped))
cfm_test <- confusionMatrix(test_preds, test$scam_type_grouped)
ggplotConfusionMatrix(cfm_test)
cfm_test
library(arules)
set.seed(2838)
x = subset(data_nb, select = -c(scam_type_grouped))
Scam_Type = as.factor(data_nb$scam_type_grouped)
Disc_log_funds_lost = discretize(x$log_funds_lost, method = "cluster", breaks = 3)
Disc_log_funds_returned = discretize(x$log_funds_returned, method = "cluster", breaks = 3)
Disc_day_of_year_of_attack = discretize(x$day_of_year_of_attack, method = "cluster", breaks = 3)
Ddf = data.frame(Disc_log_funds_lost, Disc_log_funds_returned, x$scamNetworks, x$month_of_attack, x$day_of_week_of_attack, Disc_day_of_year_of_attack, Scam_Type)
ind <- sample(2, nrow(Ddf), replace = T, prob = c(0.8, 0.2))
train_discretized <- Ddf[ind == 1,]
test_discretized <- Ddf[ind == 2,]
discretized_NB_model <- NaiveBayes(Scam_Type ~ ., data=train_discretized)
cfm_discretized_train <- confusionMatrix(predict(discretized_NB_model, train_discretized)$class, train_discretized$Scam_Type)
ggplotConfusionMatrix(cfm_discretized_train)
cfm_discretized_train
cfm_discretized_test <- confusionMatrix(predict(discretized_NB_model, test_discretized)$class, test_discretized$Scam_Type)
ggplotConfusionMatrix(cfm_discretized_test)
cfm_discretized_test
df <- tibble(
Codes_and_Data = c("Naive Bayes Python Code", "Clean Tweets", "Clean News", "Naive Bayes R Code", "Clean Record Data"),
link = c("https://github.com/anly501/anly-501-project-TegveerG/tree/main/codes/Naive_Bayes_py", "https://github.com/anly501/anly-501-project-TegveerG/blob/main/data/Clean%20Data/Clean_TokeLemm_Crypto_Tweets.csv", "https://github.com/anly501/anly-501-project-TegveerG/blob/main/data/Clean%20Data/Clean_TokeLemm_News.csv", "https://github.com/anly501/anly-501-project-TegveerG/tree/main/codes/Naive_Bayes_R", "https://github.com/anly501/anly-501-project-TegveerG/blob/main/data/Clean%20Data/REKT_Database_Clean_Python.csv"))
df %>%
mutate(Link = glue::glue("[Github]({link})"),
Link = map(Link, gt::md)) %>% select(Codes_and_Data, Link) %>%
gt()
library(gt)
library(tidyverse)
df <- tibble(
Codes_and_Data = c("Naive Bayes Python Code", "Clean Tweets", "Clean News", "Naive Bayes R Code", "Clean Record Data"),
link = c("https://github.com/anly501/anly-501-project-TegveerG/tree/main/codes/Naive_Bayes_py", "https://github.com/anly501/anly-501-project-TegveerG/blob/main/data/Clean%20Data/Clean_TokeLemm_Crypto_Tweets.csv", "https://github.com/anly501/anly-501-project-TegveerG/blob/main/data/Clean%20Data/Clean_TokeLemm_News.csv", "https://github.com/anly501/anly-501-project-TegveerG/tree/main/codes/Naive_Bayes_R", "https://github.com/anly501/anly-501-project-TegveerG/blob/main/data/Clean%20Data/REKT_Database_Clean_Python.csv"))
# using html
df %>%
mutate(
Link = glue::glue("[Github]({link})"),
Link = map(Link, gt::md)) %>% select(Codes_and_Data, Link) %>%
gt()
View(df)
library(gt)
library(tidyverse)
df <- tibble(
Codes_and_Data = c("Naive Bayes Python Code", "Clean Tweets", "Clean News", "Naive Bayes R Code", "Clean Record Data"),
link = c("https://github.com/anly501/anly-501-project-TegveerG/tree/main/codes/Naive_Bayes_py", "https://github.com/anly501/anly-501-project-TegveerG/blob/main/data/Clean%20Data/Clean_TokeLemm_Crypto_Tweets.csv", "https://github.com/anly501/anly-501-project-TegveerG/blob/main/data/Clean%20Data/Clean_TokeLemm_News.csv", "https://github.com/anly501/anly-501-project-TegveerG/tree/main/codes/Naive_Bayes_R", "https://github.com/anly501/anly-501-project-TegveerG/blob/main/data/Clean%20Data/REKT_Database_Clean_Python.csv"))
# using html
df %>%
mutate(
Link = glue::glue("[Github]({link})"),
Link = map(Link, gt::md))
df %>%
select(Codes_and_Data, Link) %>%
gt()
df %>% select(Codes_and_Data, Link)
df %>% select(df$Codes_and_Data, df$Link)
View(df)
library(gt)
library(tidyverse)
df <- tibble(
Codes_and_Data = c("Naive Bayes Python Code", "Clean Tweets", "Clean News", "Naive Bayes R Code", "Clean Record Data"),
link = c("https://github.com/anly501/anly-501-project-TegveerG/tree/main/codes/Naive_Bayes_py", "https://github.com/anly501/anly-501-project-TegveerG/blob/main/data/Clean%20Data/Clean_TokeLemm_Crypto_Tweets.csv", "https://github.com/anly501/anly-501-project-TegveerG/blob/main/data/Clean%20Data/Clean_TokeLemm_News.csv", "https://github.com/anly501/anly-501-project-TegveerG/tree/main/codes/Naive_Bayes_R", "https://github.com/anly501/anly-501-project-TegveerG/blob/main/data/Clean%20Data/REKT_Database_Clean_Python.csv"))
# using html
df %>%
mutate(
Link = glue::glue("[Github]({link})"),
Link = map(Link, gt::md)) %>% select(Codes_and_Data, link) %>%
gt()
